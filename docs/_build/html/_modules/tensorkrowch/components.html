
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>tensorkrowch.components &#8212; TensorKrowch 00.00.01 documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../../_static/tensorkrowch_favicon_light.svg"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/tensorkrowch_logo_light.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contents:
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials.html">
   Tutorials
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/basics.html">
     Basics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../examples.html">
   Experiments
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/jose_mps_mnist.html">
     MPS and Canonical Form - MNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/jose_mps_canonical_mnist.html">
     MPS and Canonical Form initializing also in canonical form - MNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/jose_mps_fashion_mnist.html">
     MPS and Canonical Form - FashionMNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/cirac_snake_sbs_fashion_mnist.html">
     SnakeSBS - FashionMNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/cirac_cnn_snake_sbs_fashion_mnist.html">
     CNN-SnakeSBS - FashionMNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/jose_convmps_fashion_mnist.html">
     ConvMPS - FashionMNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/jose_convtree_fashion_mnist.html">
     ConvTree - FashionMNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/miles_mps_dmrg_mnist.html">
     MPS with DMRG - MNIST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../examples/jose_mps_mnist_binarized.html">
     MPS - MNIST Binarized
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../api.html">
   API Reference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../components.html">
     Components
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../initializers.html">
     Initializers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../embeddings.html">
     Embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../operations.html">
     Operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../models.html">
     Models
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/joserapa98/tensorkrowch"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for tensorkrowch.components</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This script contains:</span>

<span class="sd">    Classes for Nodes and Edges:</span>
<span class="sd">        * Axis</span>
<span class="sd">        * AbstractNode:</span>
<span class="sd">            + Node:</span>
<span class="sd">                - StackNode</span>
<span class="sd">            + ParamNode:</span>
<span class="sd">                - ParamStackNode</span>
<span class="sd">        * Edge:</span>
<span class="sd">            + StackEdge</span>
<span class="sd">            </span>
<span class="sd">    Edge operations:</span>
<span class="sd">        * connect</span>
<span class="sd">        * connect_stack</span>
<span class="sd">        * disconnect</span>
<span class="sd">    </span>
<span class="sd">    Class for successors:        </span>
<span class="sd">        * Successor</span>

<span class="sd">    Class for Tensor Networks:</span>
<span class="sd">        * TensorNetwork</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span><span class="p">,</span> <span class="n">ABC</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span><span class="n">overload</span><span class="p">,</span>
                    <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span>
                    <span class="n">Sequence</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Size</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Parameter</span>

<span class="kn">from</span> <span class="nn">tensorkrowch.utils</span> <span class="kn">import</span> <span class="p">(</span><span class="n">check_name_style</span><span class="p">,</span> <span class="n">enum_repeated_names</span><span class="p">,</span> <span class="n">erase_enum</span><span class="p">,</span>
                                <span class="n">print_list</span><span class="p">,</span> <span class="n">stack_unequal_tensors</span><span class="p">,</span> <span class="n">tab_string</span><span class="p">)</span>


<span class="c1">###############################################################################</span>
<span class="c1">#                                     AXIS                                    #</span>
<span class="c1">###############################################################################</span>
<div class="viewcode-block" id="Axis"><a class="viewcode-back" href="../../components.html#tensorkrowch.Axis">[docs]</a><span class="k">class</span> <span class="nc">Axis</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Axes are the objects that stick edges to nodes. Each instance of the</span>
<span class="sd">    :class:`AbstractNode` class has a list of :math:`N` axes, each corresponding</span>
<span class="sd">    to one edge. Each axis stores information that facilitates accessing that</span>
<span class="sd">    edge, such as its :attr:`name` and :attr:`num` (index). Additionally, an axis</span>
<span class="sd">    keeps track of its :meth:`batch &lt;is_batch&gt;` and :meth:`node1 &lt;is_node1&gt;`</span>
<span class="sd">    attributes.</span>

<span class="sd">    * **batch**: If the axis name contains the word &quot;`batch`&quot;, the edge will be</span>
<span class="sd">      a batch edge, which means that it cannot be connected to other nodes.</span>
<span class="sd">      Instead, it specifies a dimension that allows for batch operations (e.g.,</span>
<span class="sd">      batch contraction). If the name of the axis is changed and no longer contains</span>
<span class="sd">      the word &quot;`batch`&quot;, the corresponding edge will no longer be a batch edge.</span>
<span class="sd">      Furthermore, instances of the :class:`StackNode` and :class:`ParamStackNode`</span>
<span class="sd">      classes always have an axis with name &quot;`stack`&quot; whose edge is a batch edge.</span>

<span class="sd">    * **node1**: When two dangling edges are connected the result is a new</span>
<span class="sd">      edge linking two nodes, say ``nodeA`` and ``nodeB``. If the</span>
<span class="sd">      connection is performed in the following order::</span>

<span class="sd">        new_edge = nodeA[edgeA] ^ nodeB[edgeB]</span>

<span class="sd">      Then ``nodeA`` will be the ``node1`` of ``new_edge`` and ``nodeB``, the</span>
<span class="sd">      ``node2``. Hence, to access one of the nodes from ``new_edge`` one needs</span>
<span class="sd">      to know if it is ``node1`` or ``node2``.</span>
<span class="sd">      </span>
<span class="sd">    |</span>

<span class="sd">    Even though we can create ``Axis`` instances, that will not be usually the</span>
<span class="sd">    case, since axes are automatically created when instantiating a new</span>
<span class="sd">    :class:`node &lt;AbstractNode&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num : int</span>
<span class="sd">        Index of the axis in the node&#39;s axes list.</span>
<span class="sd">    name : str</span>
<span class="sd">        Axis name, should not contain blank spaces or special characters. If it</span>
<span class="sd">        contains the word &quot;`batch`&quot;, the axis will correspond to a batch edge.</span>
<span class="sd">        The word &quot;`stack`&quot; cannot be used in the name, since it is reserved for</span>
<span class="sd">        stacks.</span>
<span class="sd">    node : AbstractNode, optional</span>
<span class="sd">        Node to which the axis belongs.</span>
<span class="sd">    node1 : bool</span>
<span class="sd">        Boolean indicating whether ``node1`` of the edge attached to this axis</span>
<span class="sd">        is the node that contains the axis (``True``). Otherwise, the node is</span>
<span class="sd">        ``node2`` of the edge (``False``).</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    Although Axis will not be usually explicitly instantiated, it can be done</span>
<span class="sd">    like so:</span>

<span class="sd">    &gt;&gt;&gt; axis = tk.Axis(0, &#39;left&#39;)</span>
<span class="sd">    &gt;&gt;&gt; axis</span>
<span class="sd">    Axis( left (0) )</span>

<span class="sd">    &gt;&gt;&gt; axis.is_node1()</span>
<span class="sd">    True</span>

<span class="sd">    &gt;&gt;&gt; axis.is_batch()</span>
<span class="sd">    False</span>

<span class="sd">    Since &quot;`batch`&quot; is not contained in &quot;`left`&quot;, ``axis`` does not correspond</span>
<span class="sd">    to a batch edge, but that can be changed:</span>

<span class="sd">    &gt;&gt;&gt; axis.name = &#39;mybatch&#39;</span>
<span class="sd">    &gt;&gt;&gt; axis.is_batch()</span>
<span class="sd">    True</span>

<span class="sd">    Also, as explained before, knowing if a node is the ``node1`` or ``node2``</span>
<span class="sd">    of an edge enables users to access that node from the edge:</span>

<span class="sd">    &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 4), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; new_edge = nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # nodeA is node1 and nodeB is node2 of new_edge</span>
<span class="sd">    &gt;&gt;&gt; nodeA == new_edge.nodes[1 - nodeA.get_axis(&#39;right&#39;).is_node1()]</span>
<span class="sd">    True</span>

<span class="sd">    &gt;&gt;&gt; nodeB == new_edge.nodes[nodeA.get_axis(&#39;right&#39;).is_node1()]</span>
<span class="sd">    True</span>
<span class="sd">    </span>
<span class="sd">    The ``node1`` attribute is extended to ``resultant`` nodes that inherit</span>
<span class="sd">    edges.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; nodeA = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; nodeB = tk.randn(shape=(3, 4), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; nodeC = tk.randn(shape=(4, 5), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; edge1 = nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">    &gt;&gt;&gt; edge2 = nodeB[&#39;right&#39;] ^ nodeC[&#39;left&#39;]</span>
<span class="sd">    &gt;&gt;&gt; result = nodeA @ nodeB</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # result inherits the edges nodeA[&#39;left&#39;] and edge2</span>
<span class="sd">    &gt;&gt;&gt; result[&#39;left&#39;] == nodeA[&#39;left&#39;]</span>
<span class="sd">    True</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; result[&#39;right&#39;] == edge2</span>
<span class="sd">    True</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; # result is still node1 of edge2, since nodeA was</span>
<span class="sd">    &gt;&gt;&gt; result.is_node1(&#39;right&#39;)</span>
<span class="sd">    True</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">name</span><span class="p">:</span> <span class="n">Text</span><span class="p">,</span>
                 <span class="n">node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;AbstractNode&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">node1</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="c1"># Check types</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`num` should be int type&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`name` should be str type&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`node` should be AbstractNode type&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node1</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`node1` should be bool type&#39;</span><span class="p">)</span>

        <span class="c1"># Check name</span>
        <span class="k">if</span> <span class="s1">&#39;stack&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">StackNode</span><span class="p">,</span> <span class="n">ParamStackNode</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Axis cannot be named &quot;stack&quot; if the node is &#39;</span>
                                 <span class="s1">&#39;not a StackNode or ParamStackNode&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Axis with name &quot;stack&quot; should have index 0&#39;</span><span class="p">)</span>

        <span class="c1"># Set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num</span> <span class="o">=</span> <span class="n">num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_node</span> <span class="o">=</span> <span class="n">node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_node1</span> <span class="o">=</span> <span class="n">node1</span>
        <span class="k">if</span> <span class="p">(</span><span class="s1">&#39;batch&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="s1">&#39;stack&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_batch</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_batch</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># ----------</span>
    <span class="c1"># Properties</span>
    <span class="c1"># ----------</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Index in the node&#39;s axes list.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Axis name, used to access edges by name of the axis. It cannot contain</span>
<span class="sd">        blank spaces or special characters. If it contains the word &quot;`batch`&quot;,</span>
<span class="sd">        the axis will correspond to a batch edge. The word &quot;`stack`&quot; cannot be</span>
<span class="sd">        used in the name, since it is reserved for stacks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="nd">@name</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">Text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set axis name. The name should not contain blank spaces or special</span>
<span class="sd">        characters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`name` should be str type&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">==</span> <span class="s1">&#39;stack&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Name &quot;stack&quot; of stack edge cannot be changed&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;stack&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;Name &quot;stack&quot; is reserved for stack edges of (Param)StackNodes&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="s1">&#39;batch&#39;</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">or</span> <span class="s1">&#39;stack&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_batch</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch</span> <span class="ow">and</span> <span class="p">(</span><span class="s1">&#39;batch&#39;</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">or</span> <span class="s1">&#39;stack&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_batch</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_node</span><span class="o">.</span><span class="n">_change_axis_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">node</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;AbstractNode&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Node to which the axis belongs.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
<div class="viewcode-block" id="Axis.is_node1"><a class="viewcode-back" href="../../components.html#tensorkrowch.Axis.is_node1">[docs]</a>    <span class="k">def</span> <span class="nf">is_node1</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns boolean indicating whether ``node1`` of the edge attached to this</span>
<span class="sd">        axis is the node that contains the axis. Otherwise, the node is ``node2``</span>
<span class="sd">        of the edge.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node1</span></div>

<div class="viewcode-block" id="Axis.is_batch"><a class="viewcode-back" href="../../components.html#tensorkrowch.Axis.is_batch">[docs]</a>    <span class="k">def</span> <span class="nf">is_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns boolean indicating whether the edge in this axis is used as a</span>
<span class="sd">        batch edge.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch</span></div>

    <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">( </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_num</span><span class="si">}</span><span class="s1">) )&#39;</span></div>


<span class="c1">###############################################################################</span>
<span class="c1">#                                    NODES                                    #</span>
<span class="c1">###############################################################################</span>
<span class="n">Ax</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">Axis</span><span class="p">]</span>
<span class="n">Shape</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">Size</span><span class="p">]</span>


<div class="viewcode-block" id="AbstractNode"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode">[docs]</a><span class="k">class</span> <span class="nc">AbstractNode</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Abstract class for all types of nodes. Defines what a node is and most of its</span>
<span class="sd">    properties and methods. Since it is an abstract class, cannot be instantiated. </span>
<span class="sd">      </span>
<span class="sd">    Nodes are the elements that make up a :class:`TensorNetwork`. At its most</span>
<span class="sd">    basic level, a node is a container for a ``torch.Tensor`` that stores other</span>
<span class="sd">    relevant information which enables to build any network and operate nodes</span>
<span class="sd">    to contract it (and train it!). Some of the information that is carried by</span>
<span class="sd">    the nodes includes:</span>
<span class="sd">    </span>
<span class="sd">    * **Shape**: Every node needs a shape to know if connections with other</span>
<span class="sd">      nodes are possible. Even if the tensor is not specified, an empty node</span>
<span class="sd">      needs a shape.</span>
<span class="sd">      </span>
<span class="sd">    * **Tensor**: The key ingredient of the node. Although the node acts as a</span>
<span class="sd">      `container` for the tensor, the node does not `contain` it. Actually,</span>
<span class="sd">      for efficiency purposes, the tensors are stored in a sort of memory that</span>
<span class="sd">      is shared by all the nodes of the :class:`TensorNetwork`. Therefore, all</span>
<span class="sd">      that nodes `contain` is a memory address. Furthermore, some nodes can share</span>
<span class="sd">      the same (or a part of the same) tensor, thus containing the same address.</span>
<span class="sd">      Sometimes, to maintain consistency, when two nodes share a tensor, one</span>
<span class="sd">      stores its memory address, and the other one stores a reference to the</span>
<span class="sd">      former.</span>
<span class="sd">    </span>
<span class="sd">    * **Axes**: A list of :class:`Axes &lt;Axis&gt;` that make it easy to access edges</span>
<span class="sd">      just using a name or an index.</span>
<span class="sd">      </span>
<span class="sd">    * **Edges**: A list of :class:`Edges &lt;Edge&gt;`, one for each dimension of the</span>
<span class="sd">      node. Each edge is attached to the node via an :class:`Axis`. Edges are</span>
<span class="sd">      useful to connect several nodes, creating a :class:`TensorNetwork`.</span>
<span class="sd">      </span>
<span class="sd">    * **Network**: The :class:`TensorNetwork` to which the node belongs. If</span>
<span class="sd">      the network is not specified when creating the node, a new ``TensorNetwork``</span>
<span class="sd">      is created to contain the node. Although the network can be thought of</span>
<span class="sd">      as a graph, it is a ``torch.nn.Module``, so it is much more than that.</span>
<span class="sd">      Actually, the ``TensorNetwork`` can contain different types of nodes,</span>
<span class="sd">      not all of them being part of the graph, but being used for different</span>
<span class="sd">      purposes.</span>
<span class="sd">      </span>
<span class="sd">    * **Successors**: A dictionary with information about the nodes that result</span>
<span class="sd">      from :class:`Operations &lt;Operation&gt;` in which the current node was involved.</span>
<span class="sd">      See :class:`Successor`.</span>
<span class="sd">      </span>
<span class="sd">      </span>
<span class="sd">    Carrying this information with the node is what makes it easy to:</span>
<span class="sd">    </span>
<span class="sd">    * Perform tensor network :class:`Operations &lt;Operation&gt;` such as :func:`contraction</span>
<span class="sd">      &lt;contract_between&gt;` of two neighbouring nodes, without having to worry about</span>
<span class="sd">      tensor&#39;s shapes, order of axes, etc.</span>

<span class="sd">    * Perform more advanced operations such as :func:`stack` or :func:`unbind`</span>
<span class="sd">      saving memory and time.</span>

<span class="sd">    * Keep track of operations in which a node has taken place, so that several</span>
<span class="sd">      steps can be skipped in further training iterations.</span>
<span class="sd">      See :meth:`TensorNetwork.trace`.</span>
<span class="sd">      </span>
<span class="sd">    |</span>
<span class="sd">      </span>
<span class="sd">    Also, there are **4 excluding types** of nodes that will have different</span>
<span class="sd">    roles in the :class:`TensorNetwork`:</span>
<span class="sd">    </span>
<span class="sd">    * **leaf**: These are the nodes that form the :class:`TensorNetwork`</span>
<span class="sd">      (together with the ``data`` nodes). Usually, these will be the `trainable`</span>
<span class="sd">      nodes. These nodes can store their own tensors or use other node&#39;s tensor.</span>
<span class="sd">      </span>
<span class="sd">    * **data**: These are similar to ``leaf`` nodes, but they are never `trainable`,</span>
<span class="sd">      and are used to store the temporary tensors coming from input data. These</span>
<span class="sd">      nodes can store their own tensors or use other node&#39;s tensor.</span>
<span class="sd">      </span>
<span class="sd">    * **virtual**: These nodes are a sort of ancillary, `hidden` nodes that</span>
<span class="sd">      accomplish some useful task (e.g. in uniform tensor networks a virtual</span>
<span class="sd">      node can store the shared tensor, while all the other nodes in the</span>
<span class="sd">      network just have a reference to it). These nodes always store their own</span>
<span class="sd">      tensors.</span>
<span class="sd">      </span>
<span class="sd">    * **resultant**: These are nodes that result from an :class:`Operation`.</span>
<span class="sd">      They are intermediate nodes that (almost always) inherit edges from ``leaf``</span>
<span class="sd">      and ``data`` nodes, the ones that really form the network. These nodes can</span>
<span class="sd">      store their own tensors or use other node&#39;s tensor.</span>
<span class="sd">      </span>
<span class="sd">    See :class:`TensorNetwork` and :meth:`~TensorNetwork.reset` to learn more</span>
<span class="sd">    about the importance of these 4 types of nodes.</span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    </span>
<span class="sd">    Other thing one should take into account are **reserved nodes&#39; names**:</span>
<span class="sd">    </span>
<span class="sd">    * ``&quot;stack_data_memory&quot;``: Name of the ``virtual`` :class:`StackNode` that</span>
<span class="sd">      is created in :meth:`~TensorNetwork.set_data_nodes` to store the whole</span>
<span class="sd">      data tensor from which each ``data`` node might take just one `slice`.</span>
<span class="sd">      There should be at most one ``&quot;stack_data_memory&quot;`` in the network.</span>
<span class="sd">      To learn more about this, see :meth:`~TensorNetwork.set_data_nodes` and</span>
<span class="sd">      :meth:`~TensorNetwork.add_data`.</span>
<span class="sd">    </span>
<span class="sd">    * ``&quot;virtual_stack&quot;``: Name of the ``virtual`` :class:`ParamStackNode` that</span>
<span class="sd">      results from stacking :class:`ParamNodes &lt;ParamNode&gt;` as the first</span>
<span class="sd">      operation in the network contraction. There might be as much</span>
<span class="sd">      ``&quot;virtual_stack&quot;`` nodes as stacks are created from ``ParamNodes``. To</span>
<span class="sd">      learn more about this, see :class:`ParamStackNode`.</span>
<span class="sd">    </span>
<span class="sd">    * ``&quot;virtual_uniform&quot;``: Name of the ``virtual`` :class:`Node` or</span>
<span class="sd">      :class:`ParamNode` that is used in uniform (translationally invariant)</span>
<span class="sd">      tensor networks to store the tensor that will be shared by all ``leaf``</span>
<span class="sd">      nodes. There might be as much ``&quot;virtual_uniform&quot;`` nodes as shared</span>
<span class="sd">      memories are used for the ``leaf`` nodes in the network (usually just one).</span>
<span class="sd">      </span>
<span class="sd">    Although these names can in principle be used for other nodes, this can lead</span>
<span class="sd">    to undesired behaviour.</span>
<span class="sd">      </span>
<span class="sd">    See :meth:`~TensorNetwork.reset` to learn more about the importance of these</span>
<span class="sd">    reserved nodes&#39; names.</span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    </span>
<span class="sd">    Refer to the subclasses of ``AbstractNode`` to see how to instantiate nodes:</span>

<span class="sd">    * :class:`Node`</span>

<span class="sd">    * :class:`ParamNode`</span>

<span class="sd">    * :class:`StackNode`</span>

<span class="sd">    * :class:`ParamStackNode`</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Shape</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">axes_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Text</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">network</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;TensorNetwork&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">virtual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">override_node</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">edges</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">override_edges</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">node1_list</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Check shape and tensor.shape</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;One of `shape` or `tensor` must be provided&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">shape</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;If both `shape` and `tensor` are given, &#39;</span>
                                 <span class="s1">&#39;`tensor`</span><span class="se">\&#39;</span><span class="s1">s shape should be equal to `shape`&#39;</span><span class="p">)</span>

        <span class="c1"># Check shape type</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="n">Size</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s1">&#39;`shape` should be tuple[int], list[int] or torch.Size type&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`shape` elements should be int type&#39;</span><span class="p">)</span>
            <span class="n">aux_shape</span> <span class="o">=</span> <span class="n">Size</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">aux_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
            
        <span class="c1"># Check tensor type</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`tensor` should be torch.Tensor type&#39;</span><span class="p">)</span>

        <span class="c1"># Check axes_names</span>
        <span class="k">if</span> <span class="n">axes_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">Axis</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;axis_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">aux_shape</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes_names</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s1">&#39;`axes_names` should be tuple[str] or list[str] type&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">aux_shape</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;`axes_names` length should match `shape` length&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">axes_names</span> <span class="o">=</span> <span class="n">enum_repeated_names</span><span class="p">(</span><span class="n">axes_names</span><span class="p">)</span>
                <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">Axis</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">node</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes_names</span><span class="p">)]</span>

        <span class="c1"># Check name</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`name` should be str type&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">check_name_style</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;node&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Names cannot contain blank spaces&#39;</span><span class="p">)</span>

        <span class="c1"># Check network</span>
        <span class="k">if</span> <span class="n">network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TensorNetwork</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`network` should be TensorNetwork type&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">network</span> <span class="o">=</span> <span class="n">TensorNetwork</span><span class="p">()</span>

        <span class="c1"># Set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">aux_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span> <span class="o">=</span> <span class="n">axes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_successors</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="c1"># Set node type</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_leaf&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_leaf</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span><span class="n">data</span> <span class="ow">or</span> <span class="n">virtual</span><span class="p">)</span>
            <span class="c1"># else, it is False (check _create_resultant)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_virtual</span> <span class="o">=</span> <span class="n">virtual</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_leaf</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The node can only be one of `leaf`, `data`, &#39;</span>
                             <span class="s1">&#39;`virtual` and `resultant`&#39;</span><span class="p">)</span>

        <span class="c1"># Add edges</span>
        <span class="k">if</span> <span class="n">edges</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_make_edge</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span> <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node1_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;If `edges` are provided, `node1_list` should also be provided&#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node1_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">bool</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`node1_list` should be list[bool] type&#39;</span><span class="p">)</span>
                <span class="n">axis</span><span class="o">.</span><span class="n">_node1</span> <span class="o">=</span> <span class="n">node1_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span> <span class="o">=</span> <span class="n">edges</span><span class="p">[:]</span>
            
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_resultant</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reattach_edges</span><span class="p">(</span><span class="n">override</span><span class="o">=</span><span class="n">override_edges</span><span class="p">)</span>

        <span class="c1"># Add to network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_add_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="n">override_node</span><span class="p">)</span>

        <span class="c1"># Remove from the network edges from virtual nodes</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_remove_edge</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

        <span class="c1"># Set tensor</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">init_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_unrestricted_set_tensor</span><span class="p">(</span>
                    <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_unrestricted_set_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_create_resultant</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private constructor to create resultant nodes. Called from</span>
<span class="sd">        :class:`Operations &lt;Operation&gt;`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Only way to set _leaf to False</span>
        <span class="n">obj</span><span class="o">.</span><span class="n">_leaf</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">obj</span>

    <span class="c1"># ----------</span>
    <span class="c1"># Properties</span>
    <span class="c1"># ----------</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Node&#39;s tensor. It can be a ``torch.Tensor``, ``torch.nn.Parameter`` or</span>
<span class="sd">        ``None`` if the node is empty.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span>
            <span class="k">return</span> <span class="n">result</span>

        <span class="n">address</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span>
        <span class="n">node_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span>
        <span class="n">full</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">address</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">address</span> <span class="o">=</span> <span class="n">node_ref</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">address</span><span class="p">]</span>

        <span class="n">return_result</span> <span class="o">=</span> <span class="n">full</span> <span class="ow">or</span> <span class="p">(</span><span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_auto_unbind</span><span class="p">:</span>
            <span class="n">return_result</span> <span class="o">=</span> <span class="n">return_result</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;unbind&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_result</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="nd">@tensor</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unset_tensor</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Size</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shape of node&#39;s :attr:`tensor`. It is of type ``torch.Size``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length of node&#39;s :attr:`shape`, that is, number of edges of the node.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;``torch.dtype`` of node&#39;s :attr:`tensor`.&quot;&quot;&quot;</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;``torch.device`` of node&#39;s :attr:`tensor`.&quot;&quot;&quot;</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Axis</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of nodes&#39;s :class:`axes &lt;Axis&gt;`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">axes_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Text</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of names of node&#39;s :class:`axes &lt;Axis&gt;`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">axis</span><span class="p">:</span> <span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edges</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of node&#39;s :class:`edges &lt;Edge&gt;`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">network</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;TensorNetwork&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :class:`TensorNetwork` where the node belongs. If the node is moved to</span>
<span class="sd">        another :class:`TensorNetwork`, the entire connected component of the</span>
<span class="sd">        graph where the node is will be moved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span>

    <span class="nd">@network</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="s1">&#39;TensorNetwork&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">move_to_network</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">successors</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="s1">&#39;Successor&#39;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Dictionary with :class:`Operations &lt;Operation&gt;`&#39; names as keys, and the</span>
<span class="sd">        list of :class:`Successors &lt;Successor&gt;` of the node as values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_successors</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Node&#39;s name, used to access the node from the :attr:`tensor network &lt;network&gt;`</span>
<span class="sd">        where it belongs. It cannot contain blank spaces.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="nd">@name</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">Text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`name` should be str type&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">check_name_style</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;node&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`name` cannot contain blank spaces&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_change_node_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># ----------------</span>
    <span class="c1"># Abstract methods</span>
    <span class="c1"># ----------------</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_make_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Axis</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_set_tensor_format</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]:</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">parameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_param</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;AbstractNode&#39;</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">share_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;AbstractNode&#39;</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
<div class="viewcode-block" id="AbstractNode.is_leaf"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.is_leaf">[docs]</a>    <span class="k">def</span> <span class="nf">is_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a boolean indicating if the node is a ``leaf`` node. These are</span>
<span class="sd">        the nodes that form the :class:`TensorNetwork` (together with the</span>
<span class="sd">        ``data`` nodes). Usually, these will be the `trainable` nodes. These</span>
<span class="sd">        nodes can store their own tensors or use other node&#39;s tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leaf</span></div>

<div class="viewcode-block" id="AbstractNode.is_data"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.is_data">[docs]</a>    <span class="k">def</span> <span class="nf">is_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a boolean indicating if the node is a ``data`` node. These nodes</span>
<span class="sd">        are similar to ``leaf`` nodes, but they are never `trainable`, and are</span>
<span class="sd">        used to store the temporary tensors coming from input data. These nodes</span>
<span class="sd">        can store their own tensors or use other node&#39;s tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span></div>

<div class="viewcode-block" id="AbstractNode.is_virtual"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.is_virtual">[docs]</a>    <span class="k">def</span> <span class="nf">is_virtual</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a boolean indicating if the node is a ``virtual`` node. These</span>
<span class="sd">        nodes are a sort of ancillary, `hidden` nodes that accomplish some useful</span>
<span class="sd">        task (e.g. in uniform tensor networks a virtual node can store the shared</span>
<span class="sd">        tensor, while all the other nodes in the network just have a reference</span>
<span class="sd">        to it). These nodes always store their own tensors.</span>
<span class="sd">        </span>
<span class="sd">        If a ``virtual`` node is used as the node storing the shared tensor in</span>
<span class="sd">        a uniform (translationally invariant) :class:`TensorNetwork`, it is</span>
<span class="sd">        recommended to use the string **&quot;virtual_uniform&quot;** in the node&#39;s name</span>
<span class="sd">        (e.g. &quot;virtual_uniform_mps&quot;).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual</span></div>

<div class="viewcode-block" id="AbstractNode.is_resultant"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.is_resultant">[docs]</a>    <span class="k">def</span> <span class="nf">is_resultant</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a boolean indicating if the node is a ``resultant`` node. These</span>
<span class="sd">        are nodes that result from an :class:`Operation`. They are intermediate</span>
<span class="sd">        nodes that (almost always) inherit edges from ``leaf`` and ``data``</span>
<span class="sd">        nodes, the ones that really form the network. These nodes can store</span>
<span class="sd">        their own tensors or use other node&#39;s tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_leaf</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual</span><span class="p">)</span></div>

<div class="viewcode-block" id="AbstractNode.size"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.size">[docs]</a>    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Ax</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Size</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the size of the node&#39;s tensor. If ``axis`` is specified, returns</span>
<span class="sd">        the size of that axis; otherwise returns the shape of the node (same as</span>
<span class="sd">        :attr:`shape`).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str or Axis, optional</span>
<span class="sd">            Axis for which to retrieve the size.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int or torch.Size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span></div>

<div class="viewcode-block" id="AbstractNode.is_node1"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.is_node1">[docs]</a>    <span class="k">def</span> <span class="nf">is_node1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Ax</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns :meth:`node1 &lt;Axis.is_node1&gt;` attribute of axes of the node. If</span>
<span class="sd">        ``axis`` is specified, returns only the ``node1`` of that axis; otherwise</span>
<span class="sd">        returns the ``node1`` of all axes of the node.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str or Axis, optional</span>
<span class="sd">            Axis for which to retrieve the ``node1``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        bool or list[bool]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">ax</span><span class="p">:</span> <span class="n">ax</span><span class="o">.</span><span class="n">_node1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">))</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span><span class="o">.</span><span class="n">_node1</span></div>

<div class="viewcode-block" id="AbstractNode.neighbours"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.neighbours">[docs]</a>    <span class="k">def</span> <span class="nf">neighbours</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Ax</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;AbstractNode&#39;</span><span class="p">],</span>
                                                             <span class="n">List</span><span class="p">[</span><span class="s1">&#39;AbstractNode&#39;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the neighbours of the node, the nodes to which it is connected.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str or Axis, optional</span>
<span class="sd">            Axis for which to retrieve the neighbour.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        AbstractNode or list[AbstractNode]</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.randn(shape=(3, 4), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeC = tk.randn(shape=(4, 5), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; nodeB[&#39;right&#39;] ^ nodeC[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; set(nodeB.neighbours()) == set([nodeA, nodeC])</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeB.neighbours(&#39;right&#39;) == nodeC</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        Nodes ``resultant`` from operations are still connected to original</span>
<span class="sd">        neighbours.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; result = nodeA @ nodeB</span>
<span class="sd">        &gt;&gt;&gt; result.neighbours(&#39;right&#39;) == nodeC</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">node1_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">node2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="n">node1_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">)]]</span>
            <span class="k">return</span> <span class="n">node2</span>

        <span class="n">neighbours</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">():</span>
                <span class="n">node2</span> <span class="o">=</span> <span class="n">edge</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="n">node1_list</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
                <span class="n">neighbours</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node2</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">neighbours</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_change_axis_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Axis</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">Text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Changes the name of an axis. If an axis belongs to a node, we have to</span>
<span class="sd">        take care of repeated names. If the name that is going to be assigned</span>
<span class="sd">        to the axis is already set for another axis, we change  those names by</span>
<span class="sd">        an enumerated version of them.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Axis</span>
<span class="sd">            Axis whose name is going to be changed.</span>
<span class="sd">        name : str</span>
<span class="sd">            New name.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If ``axis`` does not belong to the node.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span><span class="o">.</span><span class="n">_node</span> <span class="o">!=</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Cannot change the name of an axis that does &#39;</span>
                             <span class="s1">&#39;not belong to the node&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">:</span>
            <span class="n">axes_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">[:]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes_names</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">axis_name</span> <span class="o">==</span> <span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">:</span>  <span class="c1"># Axes names are unique</span>
                    <span class="n">axes_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span>
                    <span class="k">break</span>
            <span class="n">new_axes_names</span> <span class="o">=</span> <span class="n">enum_repeated_names</span><span class="p">(</span><span class="n">axes_names</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">axis_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">,</span> <span class="n">new_axes_names</span><span class="p">):</span>
                <span class="n">axis</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">axis_name</span>

    <span class="k">def</span> <span class="nf">_change_axis_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Ax</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Changes axis size, that is, changes size of node&#39;s tensor at a certain</span>
<span class="sd">        axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str or Axis</span>
<span class="sd">            Axis where size is going to be changed.</span>
<span class="sd">        size : int</span>
<span class="sd">            New size.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If new size is not positive.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;New `size` should be greater than zero&#39;</span><span class="p">)</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>

        <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">aux_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
            <span class="n">aux_shape</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">Size</span><span class="p">(</span><span class="n">aux_shape</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]:</span>
                <span class="c1"># If new size is smaller than current, tensor is cropped</span>
                <span class="c1"># starting from the &quot;left&quot;, &quot;top&quot;, &quot;front&quot;, etc. in each dimension</span>
                <span class="n">index</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">axis_num</span><span class="p">:</span>
                        <span class="n">index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="n">dim</span> <span class="o">-</span> <span class="n">size</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
                <span class="n">aux_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
                <span class="n">aux_shape</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">Size</span><span class="p">(</span><span class="n">aux_shape</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_direct_set_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>

            <span class="k">elif</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]:</span>
                <span class="c1"># If new size is greater than current, tensor is expanded with</span>
                <span class="c1"># zeros in the &quot;left&quot;, &quot;top&quot;, &quot;front&quot;, etc. dimension</span>
                <span class="n">pad</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">axis_num</span><span class="p">:</span>
                        <span class="n">pad</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="n">dim</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">pad</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
                <span class="n">pad</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
                <span class="n">aux_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
                <span class="n">aux_shape</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">Size</span><span class="p">(</span><span class="n">aux_shape</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_direct_set_tensor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">pad</span><span class="p">))</span>

<div class="viewcode-block" id="AbstractNode.get_axis"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.get_axis">[docs]</a>    <span class="k">def</span> <span class="nf">get_axis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Ax</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns :class:`Axis` given its ``name`` or ``num``.&quot;&quot;&quot;</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span></div>

<div class="viewcode-block" id="AbstractNode.get_axis_num"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.get_axis_num">[docs]</a>    <span class="k">def</span> <span class="nf">get_axis_num</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Ax</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns axis&#39; ``num`` given the :class:`Axis` or its ``name``.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span>  <span class="c1"># When indexing with -1, -2, ...</span>
            <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="n">ax</span><span class="o">.</span><span class="n">_num</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">ax</span><span class="o">.</span><span class="n">_num</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Node </span><span class="si">{</span><span class="bp">self</span><span class="si">!s}</span><span class="s1"> has no axis with index </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="n">ax</span><span class="o">.</span><span class="n">_name</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">ax</span><span class="o">.</span><span class="n">_num</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Node </span><span class="si">{</span><span class="bp">self</span><span class="si">!s}</span><span class="s1"> has no axis with name </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">Axis</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="n">ax</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">ax</span><span class="o">.</span><span class="n">_num</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Node </span><span class="si">{</span><span class="bp">self</span><span class="si">!s}</span><span class="s1"> has no axis </span><span class="si">{</span><span class="n">axis</span><span class="si">!r}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`axis` should be int, str or Axis type&#39;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_add_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">edge</span><span class="p">:</span> <span class="s1">&#39;Edge&#39;</span><span class="p">,</span>
                  <span class="n">axis</span><span class="p">:</span> <span class="n">Ax</span><span class="p">,</span>
                  <span class="n">node1</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds an edge to the specified axis of the node.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        edge : Edge</span>
<span class="sd">            Edge that will be added.</span>
<span class="sd">        axis : int, str or Axis</span>
<span class="sd">            Axes where the edge will be attached.</span>
<span class="sd">        node1 : bool, optional</span>
<span class="sd">            Boolean indicating whether the node is the ``node1`` (``True``) or</span>
<span class="sd">            ``node2`` (``False``) of the edge.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span><span class="o">.</span><span class="n">_node1</span> <span class="o">=</span> <span class="n">node1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">edge</span>

<div class="viewcode-block" id="AbstractNode.get_edge"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.get_edge">[docs]</a>    <span class="k">def</span> <span class="nf">get_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Ax</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns :class:`Edge` given the :class:`Axis` (or its ``name``</span>
<span class="sd">        or ``num``) where it is attached to the node.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">[</span><span class="n">axis_num</span><span class="p">]</span></div>

<div class="viewcode-block" id="AbstractNode.in_which_axis"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.in_which_axis">[docs]</a>    <span class="k">def</span> <span class="nf">in_which_axis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">edge</span><span class="p">:</span> <span class="s1">&#39;Edge&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Axis</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns :class:`Axis` given the :class:`Edge` that is attached</span>
<span class="sd">        to the node through it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ed</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">ed</span> <span class="o">==</span> <span class="n">edge</span><span class="p">:</span>
                <span class="n">lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Edge </span><span class="si">{</span><span class="n">edge</span><span class="si">}</span><span class="s1"> not in node </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lst</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Case of trace edges (attached to the node in two axes)</span>
            <span class="k">return</span> <span class="n">lst</span></div>

<div class="viewcode-block" id="AbstractNode.reattach_edges"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.reattach_edges">[docs]</a>    <span class="k">def</span> <span class="nf">reattach_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">override</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Substitutes current edges by copies of them that are attached to the node.</span>
<span class="sd">        It can happen that an edge is not attached to the node if it is the result</span>
<span class="sd">        of an :class:`Operation` and, hence, it inherits edges from the operands.</span>
<span class="sd">        In that case, the new copied edges will be attached to the resultant node,</span>
<span class="sd">        replacing each previous ``node1`` or ``node2`` with it (according to the</span>
<span class="sd">        ``node1`` attribute of each axis).</span>

<span class="sd">        Used for in-place operations like :func:`permute_` or :func:`split_` and</span>
<span class="sd">        to (de)parameterize nodes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        override: bool</span>
<span class="sd">            Boolean indicating if the new, reattached edges should also replace</span>
<span class="sd">            the corresponding edges in the node&#39;s neighbours (``True``). Otherwise,</span>
<span class="sd">            the neighbours&#39; edges will be pointing to the original nodes from which</span>
<span class="sd">            the current node inherits its edges (``False``).</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.randn(shape=(3, 4), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeC = tk.randn(shape=(4, 5), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; nodeB[&#39;right&#39;] ^ nodeC[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; result = nodeA @ nodeB</span>
<span class="sd">        </span>
<span class="sd">        Node ``result`` inherits its ``right`` edge from ``nodeB``.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; result[&#39;right&#39;] == nodeB[&#39;right&#39;]</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        However, ``nodeB[&#39;right&#39;]`` still connects ``nodeB`` and ``nodeC``.</span>
<span class="sd">        There is no reference to ``result``.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; result in result[&#39;right&#39;].nodes</span>
<span class="sd">        False</span>
<span class="sd">        </span>
<span class="sd">        One can reattach its edges so that ``result``&#39;s edges do have references</span>
<span class="sd">        to it.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; result.reattach_edges()</span>
<span class="sd">        &gt;&gt;&gt; result in result[&#39;right&#39;].nodes</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        If ``override`` is ``True``, ``nodeB[&#39;right&#39;]`` would be replaced by the</span>
<span class="sd">        new ``result[&#39;right&#39;]``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="n">node1</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">())):</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">edge</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">node1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">node</span> <span class="o">!=</span> <span class="bp">self</span><span class="p">:</span>
                <span class="c1"># New edges are always a copy, so that the original</span>
                <span class="c1"># nodes have different edges from the current node</span>
                <span class="n">new_edge</span> <span class="o">=</span> <span class="n">edge</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_edge</span>

                <span class="n">new_edge</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">node1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span>
                <span class="n">new_edge</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">node1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

                <span class="c1"># Case of trace edges (attached to the node in two axes)</span>
                <span class="n">neighbour</span> <span class="o">=</span> <span class="n">new_edge</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="n">node1</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">neighbour</span> <span class="o">==</span> <span class="n">node</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">other_edge</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">):</span>
                        <span class="k">if</span> <span class="p">(</span><span class="n">other_edge</span> <span class="o">==</span> <span class="n">edge</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">):</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_edge</span>
                            <span class="n">new_edge</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="n">node1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span>
                            <span class="n">new_edge</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="n">node1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">override</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">new_edge</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">neighbour</span> <span class="o">!=</span> <span class="n">node</span><span class="p">):</span>
                        <span class="n">neighbour</span><span class="o">.</span><span class="n">_add_edge</span><span class="p">(</span>
                            <span class="n">new_edge</span><span class="p">,</span> <span class="n">new_edge</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="n">node1</span><span class="p">],</span> <span class="ow">not</span> <span class="n">node1</span><span class="p">)</span></div>

<div class="viewcode-block" id="AbstractNode.disconnect"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.disconnect">[docs]</a>    <span class="k">def</span> <span class="nf">disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Ax</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disconnects all edges of the node if they were connected to other nodes.</span>
<span class="sd">        If ``axis`` is sepcified, only the corresponding edge is disconnected.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str or Axis, optional</span>
<span class="sd">            Axis whose edge will be disconnected.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 4), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeC = tk.Node(shape=(4, 5), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; nodeB[&#39;right&#39;] ^ nodeC[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; set(nodeB.neighbours()) == set([nodeA, nodeC])</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeB.disconnect()</span>
<span class="sd">        &gt;&gt;&gt; nodeB.neighbours() == []</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">edges</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">[</span><span class="n">axis</span><span class="p">]]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">edges</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span>

        <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_attached_to</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">():</span>
                    <span class="n">edge</span> <span class="o">|</span> <span class="n">edge</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_make_copy_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
                          <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns copy tensor (ones in the &quot;diagonal&quot;, zeros elsewhere).&quot;&quot;&quot;</span>
        <span class="n">copy_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">copy_tensor</span><span class="p">[(</span><span class="n">i</span><span class="p">,)</span> <span class="o">*</span> <span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="k">return</span> <span class="n">copy_tensor</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_make_rand_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
                          <span class="n">low</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
                          <span class="n">high</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
                          <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns tensor whose entries are drawn from the uniform distribution.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`low` should be float type&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">high</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`high` should be float type&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">low</span> <span class="o">&gt;=</span> <span class="n">high</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`low` should be strictly smaller than `high`&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span> <span class="o">+</span> <span class="n">low</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_make_randn_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
                           <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
                           <span class="n">std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
                           <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns tensor whose entries are drawn from the normal distribution.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`mean` should be float type&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">std</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`std` should be float type&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">std</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`std` should be positive&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">std</span> <span class="o">+</span> <span class="n">mean</span>

<div class="viewcode-block" id="AbstractNode.make_tensor"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.make_tensor">[docs]</a>    <span class="k">def</span> <span class="nf">make_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Shape</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">init_method</span><span class="p">:</span> <span class="n">Text</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a tensor that can be put in the node, and is initialized according</span>
<span class="sd">        to ``init_method``. By default, it has the same shape as the node.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        shape : list[int], tuple[int] or torch.Size, optional</span>
<span class="sd">            Shape of the tensor. If ``None``, node&#39;s shape will be used.</span>
<span class="sd">        init_method : {&quot;zeros&quot;, &quot;ones&quot;, &quot;copy&quot;, &quot;rand&quot;, &quot;randn&quot;}, optional</span>
<span class="sd">            Initialization method.</span>
<span class="sd">        device : torch.device, optional</span>
<span class="sd">            Device where to initialize the tensor.</span>
<span class="sd">        kwargs : float</span>
<span class="sd">            Keyword arguments for the different initialization methods:</span>

<span class="sd">            * ``low``, ``high`` for uniform initialization. See</span>
<span class="sd">              `torch.rand() &lt;https://pytorch.org/docs/stable/generated/torch.rand.html&gt;`_</span>

<span class="sd">            * ``mean``, ``std`` for normal initialization. See</span>
<span class="sd">              `torch.randn() &lt;https://pytorch.org/docs/stable/generated/torch.randn.html&gt;`_</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If ``init_method`` is not one of &quot;zeros&quot;, &quot;ones&quot;, &quot;copy&quot;, &quot;rand&quot;, &quot;randn&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>
        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s1">&#39;ones&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s1">&#39;copy&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_copy_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s1">&#39;rand&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_rand_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s1">&#39;randn&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_randn_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Choose a valid `init_method`: &quot;zeros&quot;, &#39;</span>
                             <span class="s1">&#39;&quot;ones&quot;, &quot;copy&quot;, &quot;rand&quot;, &quot;randn&quot;&#39;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_compatible_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks if tensor&#39;s shape is &quot;compatible&quot; with the node&#39;s shape, meaning</span>
<span class="sd">        that the sizes in all axes must match except for the batch axes, where</span>
<span class="sd">        sizes can be different.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
                <span class="n">edge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_edge</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_batch</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dim</span> <span class="o">!=</span> <span class="n">edge</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
                    <span class="k">return</span> <span class="kc">False</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_crop_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Crops the tensor in case its shape is not compatible with the node&#39;s shape.</span>
<span class="sd">        That is, if the tensor has a size that is greater than the corresponding</span>
<span class="sd">        size of the node for a certain axis, the tensor is cropped in that axis</span>
<span class="sd">        (provided that the axis is not a batch axis). If that size is smaller in</span>
<span class="sd">        the tensor than in the node, raises a ``ValueError``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tensor : torch.Tensor</span>
<span class="sd">            Tensor to be cropped.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
                <span class="n">edge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_edge</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_batch</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span><span class="n">dim</span> <span class="o">==</span> <span class="n">edge</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
                    <span class="n">index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">dim</span> <span class="o">&gt;</span> <span class="n">edge</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
                    <span class="n">index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="n">dim</span> <span class="o">-</span> <span class="n">edge</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">dim</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Cannot crop tensor if its size at axis </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span>
                                     <span class="s1">&#39; is smaller than node</span><span class="se">\&#39;</span><span class="s1">s size&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`tensor` should have the same number of&#39;</span>
                             <span class="s1">&#39; dimensions as node</span><span class="se">\&#39;</span><span class="s1">s tensor (same rank)&#39;</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">_direct_set_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                           <span class="n">check_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets a new node&#39;s tensor without checking extra conditions. It just</span>
<span class="sd">        can crop the tensor in case it is specified with ``check_shape``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">check_shape</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compatible_shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_crop_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">correct_format_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_tensor_format</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_save_in_network</span><span class="p">(</span><span class="n">correct_format_tensor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">_unrestricted_set_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                 <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">init_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                                 <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets a new node&#39;s tensor or creates one with :meth:`make_tensor` and sets</span>
<span class="sd">        it. Before setting it, it is casted to the correct type, so that a</span>
<span class="sd">        ``torch.Tensor`` can be turned into a ``torch.nn.Parameter`` when setting</span>
<span class="sd">        it in :class:`ParamNodes &lt;ParamNode&gt;`. This is not restricted, can be</span>
<span class="sd">        used in any node, even in ``resultant`` nodes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tensor : torch.Tensor, optional</span>
<span class="sd">            Tensor to be set in the node. If ``None``, and ``init_method`` is</span>
<span class="sd">            provided, the tensor is created with :meth:`make_tensor`. Otherwise,</span>
<span class="sd">            a ``None`` is set as node&#39;s tensor.</span>
<span class="sd">        init_method : {&quot;zeros&quot;, &quot;ones&quot;, &quot;copy&quot;, &quot;rand&quot;, &quot;randn&quot;}, optional</span>
<span class="sd">            Initialization method.</span>
<span class="sd">        device : torch.device, optional</span>
<span class="sd">            Device where to initialize the tensor.</span>
<span class="sd">        kwargs : float</span>
<span class="sd">            Keyword arguments for the different initialization methods. See</span>
<span class="sd">            :meth:`make_tensor`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`tensor` should be torch.Tensor type&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`device` was specified but is being ignored. &#39;</span>
                              <span class="s1">&#39;Provide a tensor that is already in the required&#39;</span>
                              <span class="s1">&#39; device&#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compatible_shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
                <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_crop_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">correct_format_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_tensor_format</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">init_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">node_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">node_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
                <span class="n">device</span> <span class="o">=</span> <span class="n">node_tensor</span><span class="o">.</span><span class="n">device</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_tensor</span><span class="p">(</span>
                <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">correct_format_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_tensor_format</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">correct_format_tensor</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_save_in_network</span><span class="p">(</span><span class="n">correct_format_tensor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>

<div class="viewcode-block" id="AbstractNode.set_tensor"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.set_tensor">[docs]</a>    <span class="k">def</span> <span class="nf">set_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                   <span class="n">init_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                   <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets new node&#39;s tensor or creates one with :meth:`make_tensor` and sets</span>
<span class="sd">        it. Before setting it, it is casted to the correct type: ``torch.Tensor``</span>
<span class="sd">        for :class:`Node` and ``torch.nn.Parameter`` for :class:`ParamNode`.</span>
<span class="sd">        </span>
<span class="sd">        When a tensor is **set** in the node, it means the node stores it, that</span>
<span class="sd">        is, the node has its own memory address for its tensor, rather than a</span>
<span class="sd">        reference to other node&#39;s tensor. Because of this, ``set_tensor`` cannot</span>
<span class="sd">        be applied for nodes that have a reference to other node&#39;s tensor, since</span>
<span class="sd">        that tensor would be changed also in the referenced node. To overcome</span>
<span class="sd">        this issue, see :meth:`reset_tensor_address`.</span>
<span class="sd">        </span>
<span class="sd">        This can only be used for **non** ``resultant``nodes that store their</span>
<span class="sd">        own tensors. For ``resultant`` nodes, tensors are set automatically when</span>
<span class="sd">        computing :class:`Operations &lt;Operation&gt;`.</span>
<span class="sd">        </span>
<span class="sd">        Although this can also be used for ``data`` nodes, input data will be</span>
<span class="sd">        usually automatically set into nodes when calling the :meth:`TensorNetwork.forward`</span>
<span class="sd">        method of :class:`TensorNetwork` with a data tensor or a sequence of</span>
<span class="sd">        tensors. This method calls :meth:`TensorNetwork.add_data`, which can</span>
<span class="sd">        also be used to set data tensors into the ``data`` nodes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tensor : torch.Tensor, optional</span>
<span class="sd">            Tensor to be set in the node. If ``None``, and ``init_method`` is</span>
<span class="sd">            provided, the tensor is created with :meth:`make_tensor`. Otherwise,</span>
<span class="sd">            a ``None`` is set as node&#39;s tensor.</span>
<span class="sd">        init_method : {&quot;zeros&quot;, &quot;ones&quot;, &quot;copy&quot;, &quot;rand&quot;, &quot;randn&quot;}, optional</span>
<span class="sd">            Initialization method.</span>
<span class="sd">        device : torch.device, optional</span>
<span class="sd">            Device where to initialize the tensor.</span>
<span class="sd">        kwargs : float</span>
<span class="sd">            Keyword arguments for the different initialization methods. See</span>
<span class="sd">            :meth:`make_tensor`.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If the node is a ``resultant`` node or if it does not store its own</span>
<span class="sd">            tensor.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; node = tk.Node(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Call set_tensor without arguments uses the</span>
<span class="sd">        &gt;&gt;&gt; # default init_method (&quot;zeros&quot;)</span>
<span class="sd">        &gt;&gt;&gt; node.set_tensor()</span>
<span class="sd">        &gt;&gt;&gt; torch.equal(node.tensor, torch.zeros(node.shape))</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node.set_tensor(init_method=&#39;randn&#39;, mean=1., std=2., device=&#39;cuda&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.equal(node.tensor, torch.zeros(node.shape, device=&#39;cuda&#39;))</span>
<span class="sd">        False</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node.device</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=0)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.randn(2, 3)</span>
<span class="sd">        &gt;&gt;&gt; node.set_tensor(tensor)</span>
<span class="sd">        &gt;&gt;&gt; torch.equal(node.tensor, tensor)</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If node stores its own tensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_resultant</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_unrestricted_set_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span>
                                          <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
                                          <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                          <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Node</span><span class="se">\&#39;</span><span class="s1">s tensor can only be changed if it is not&#39;</span>
                             <span class="s1">&#39; resultant and stores its own tensor&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="AbstractNode.unset_tensor"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.unset_tensor">[docs]</a>    <span class="k">def</span> <span class="nf">unset_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replaces node&#39;s tensor with ``None``. This can only be used for **non**</span>
<span class="sd">        ``resultant`` nodes that store their own tensors.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; node = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; node.tensor is None</span>
<span class="sd">        False</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node.unset_tensor()</span>
<span class="sd">        &gt;&gt;&gt; node.tensor is None</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If node stores its own tensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_resultant</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_in_network</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Node</span><span class="se">\&#39;</span><span class="s1">s tensor can only be changed if it is not&#39;</span>
                             <span class="s1">&#39; resultant and stores its own tensor&#39;</span><span class="p">)</span></div>
            
<div class="viewcode-block" id="AbstractNode.set_tensor_from"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.set_tensor_from">[docs]</a>    <span class="k">def</span> <span class="nf">set_tensor_from</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s1">&#39;AbstractNode&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets node&#39;s tensor as the tensor used by ``other`` node. That is, when</span>
<span class="sd">        setting the tensor this way, the current node will store a reference to</span>
<span class="sd">        the ``other`` node&#39;s tensor, instead of having its own tensor.</span>
<span class="sd">        </span>
<span class="sd">        The node and ``other`` should be both the same type (:class:`Node` or</span>
<span class="sd">        :class:`ParamNode`). Also, they should be in the same :class:`TensorNetwork`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other : Node or ParamNode</span>
<span class="sd">            Node whose tensor is to be set in current node.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If ``other`` is a different type than the current node, or if it is</span>
<span class="sd">            in a different network.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn(shape=(2, 3),</span>
<span class="sd">        ...                  name=&#39;nodeA&#39;,</span>
<span class="sd">        ...                  axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.empty(shape=(2, 3),</span>
<span class="sd">        ...                  name=&#39;nodeB&#39;,</span>
<span class="sd">        ...                  axes_names=(&#39;left&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                  network=nodeA.network)</span>
<span class="sd">        &gt;&gt;&gt; nodeB.set_tensor_from(nodeA)</span>
<span class="sd">        &gt;&gt;&gt; print(nodeB.tensor_address())</span>
<span class="sd">        nodeA</span>
<span class="sd">        </span>
<span class="sd">        Since ``nodeB`` has a reference to ``nodeA``&#39;s tensor, if this one is</span>
<span class="sd">        changed, ``nodeB`` will reproduce all the changes.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeA.tensor = torch.randn(nodeA.shape)</span>
<span class="sd">        &gt;&gt;&gt; torch.equal(nodeA.tensor, nodeB.tensor)</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Both nodes should be the same type&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">other</span><span class="o">.</span><span class="n">_network</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Both nodes should be in the same network&#39;</span><span class="p">)</span>
        
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]]</span>
        
        <span class="k">if</span> <span class="n">other</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">other</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">_tensor_info</span></div>
            
<div class="viewcode-block" id="AbstractNode.tensor_address"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.tensor_address">[docs]</a>    <span class="k">def</span> <span class="nf">tensor_address</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns address of the node&#39;s tensor in the network&#39;s memory.&quot;&quot;&quot;</span>
        <span class="n">address</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">address</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">node_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span>
            <span class="n">address</span> <span class="o">=</span> <span class="n">node_ref</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">address</span></div>
            
<div class="viewcode-block" id="AbstractNode.reset_tensor_address"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.reset_tensor_address">[docs]</a>    <span class="k">def</span> <span class="nf">reset_tensor_address</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resets memory address of node&#39;s tensor to reference the node itself.</span>
<span class="sd">        Thus the node will store its own tensor, instead of having a reference</span>
<span class="sd">        to other node&#39;s tensor.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn(shape=(2, 3),</span>
<span class="sd">        ...                  name=&#39;nodeA&#39;,</span>
<span class="sd">        ...                  axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.empty(shape=(2, 3),</span>
<span class="sd">        ...                  name=&#39;nodeB&#39;,</span>
<span class="sd">        ...                  axes_names=(&#39;left&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                  network=nodeA.network)</span>
<span class="sd">        &gt;&gt;&gt; nodeB.set_tensor_from(nodeA)</span>
<span class="sd">        &gt;&gt;&gt; print(nodeB.tensor_address())</span>
<span class="sd">        nodeA</span>
<span class="sd">        </span>
<span class="sd">        Now one cannot set in ``nodeB`` a different tensor from the one in</span>
<span class="sd">        ``nodeA``, unless tensor address is reset in ``nodeB``.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeB.reset_tensor_address()</span>
<span class="sd">        &gt;&gt;&gt; nodeB.tensor = torch.randn(nodeB.shape)</span>
<span class="sd">        &gt;&gt;&gt; torch.equal(nodeA.tensor, nodeB.tensor)</span>
<span class="sd">        False</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span> <span class="s1">&#39;param_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>
                    <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span> <span class="s1">&#39;param_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>
                    
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Set tensor and save it in ``memory_nodes``</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_unrestricted_set_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="kc">None</span></div>

    <span class="k">def</span> <span class="nf">_save_in_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Saves new node&#39;s tensor in the network&#39;s memory.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span>
                <span class="s1">&#39;param_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">],</span> <span class="n">tensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_record_in_inverse_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Records information of the node in network&#39;s ``inverse memory``. This</span>
<span class="sd">        memory is a dictionary that, for each node used in an :class:`Operation`,</span>
<span class="sd">        keeps track of:</span>

<span class="sd">        * The total amount of times that the node&#39;s tensor is accessed to compute</span>
<span class="sd">          operations (calculated when contracting the network for the first time,</span>
<span class="sd">          in ``tracing`` mode).</span>

<span class="sd">        * The number of accesses to the node&#39;s tensor in the current contraction.</span>

<span class="sd">        * Whether this node&#39;s tensor can be erased after using it for all the</span>
<span class="sd">          operations in which it is involved.</span>

<span class="sd">        When contracting the :class:`TensorNetwork`, if the node&#39;s tensor has been</span>
<span class="sd">        accessed the total amount of times it has to be accessed, and it can be</span>
<span class="sd">        erased, then its tensor is indeed replaced by None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span>
        <span class="n">address</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">address</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">node_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span>
            <span class="n">address</span> <span class="o">=</span> <span class="n">node_ref</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span>
            <span class="n">check_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_ref</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">check_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">]</span>

        <span class="c1"># When tracing network, node is recorded in inverse memory</span>
        <span class="k">if</span> <span class="n">net</span><span class="o">.</span><span class="n">_tracing</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">address</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">[</span><span class="n">address</span><span class="p">][</span><span class="s1">&#39;erase&#39;</span><span class="p">]:</span>
                    <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">[</span><span class="n">address</span><span class="p">][</span><span class="s1">&#39;accessed&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Node can only be erased if both itself and the node from which</span>
                <span class="c1"># it is taking the tensor information (node_ref) are resultant or</span>
                <span class="c1"># data nodes (including virtual node that stores stack data tensor)</span>
                <span class="n">erase</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">check_nodes</span><span class="p">:</span>
                    <span class="n">erase</span> <span class="o">&amp;=</span> <span class="n">node</span><span class="o">.</span><span class="n">is_resultant</span><span class="p">()</span> <span class="ow">or</span> <span class="n">node</span><span class="o">.</span><span class="n">_data</span> <span class="ow">or</span> \
                        <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">_virtual</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span> <span class="o">==</span> <span class="s1">&#39;stack_data_memory&#39;</span><span class="p">)</span>

                <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">[</span><span class="n">address</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s1">&#39;accessed&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="s1">&#39;re-accessed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="s1">&#39;erase&#39;</span><span class="p">:</span> <span class="n">erase</span><span class="p">}</span>

        <span class="c1"># When contracting network, we keep track of the number of accesses</span>
        <span class="c1"># to &quot;erasable&quot; nodes</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">address</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">:</span>
                <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">[</span><span class="n">address</span><span class="p">][</span><span class="s1">&#39;re-accessed&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">aux_dict</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">[</span><span class="n">address</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">aux_dict</span><span class="p">[</span><span class="s1">&#39;accessed&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">aux_dict</span><span class="p">[</span><span class="s1">&#39;re-accessed&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">aux_dict</span><span class="p">[</span><span class="s1">&#39;erase&#39;</span><span class="p">]:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">address</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="n">net</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">[</span><span class="n">address</span><span class="p">][</span><span class="s1">&#39;re-accessed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<div class="viewcode-block" id="AbstractNode.move_to_network"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.move_to_network">[docs]</a>    <span class="k">def</span> <span class="nf">move_to_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">network</span><span class="p">:</span> <span class="s1">&#39;TensorNetwork&#39;</span><span class="p">,</span>
                        <span class="n">visited</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s1">&#39;AbstractNode&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Moves node to another network. All other nodes connected to it, or</span>
<span class="sd">        to a node connected to it, etc. are also moved to the new network.</span>
<span class="sd">        </span>
<span class="sd">        If a node does not store its own tensor, and is moved to other network,</span>
<span class="sd">        it will recover the &quot;ownership&quot; of its tensor.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        network : TensorNetwork</span>
<span class="sd">            Tensor Network to which the nodes will be moved.</span>
<span class="sd">        visited : list[AbstractNode], optional</span>
<span class="sd">            List indicating the nodes that have been already moved to the new</span>
<span class="sd">            network, used by this DFS-like algorithm.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; net = tk.TensorNetwork()</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 3),</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                 network=net)</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 4),</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                 network=net)</span>
<span class="sd">        &gt;&gt;&gt; nodeC = tk.Node(shape=(5, 5),</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                 network=net)</span>
<span class="sd">        &gt;&gt;&gt; nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        </span>
<span class="sd">        If ``nodeA`` is moved to other network, ``nodeB`` will also move, but</span>
<span class="sd">        ``nodeC`` will not.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; net2 = tk.TensorNetwork()</span>
<span class="sd">        &gt;&gt;&gt; nodeA.network = net2</span>
<span class="sd">        &gt;&gt;&gt; nodeA.network == nodeB.network</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeA.network != nodeC.network</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">network</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">visited</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">visited</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_remove_node</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                <span class="n">network</span><span class="o">.</span><span class="n">_add_node</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                <span class="n">visited</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">neighbour</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neighbours</span><span class="p">():</span>
                    <span class="n">neighbour</span><span class="o">.</span><span class="n">move_to_network</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">network</span><span class="p">,</span> <span class="n">visited</span><span class="o">=</span><span class="n">visited</span><span class="p">)</span></div>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">slice</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]:</span>
        <span class="k">pass</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Ax</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">slice</span><span class="p">,</span> <span class="n">Ax</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">],</span>
                                                          <span class="s1">&#39;Edge&#39;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">slice</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_edge</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="c1"># -----------------</span>
    <span class="c1"># Tensor operations</span>
    <span class="c1"># -----------------</span>
<div class="viewcode-block" id="AbstractNode.sum"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.sum">[docs]</a>    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Ax</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Ax</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the sum of all elements in the node&#39;s tensor. If an ``axis`` is</span>
<span class="sd">        specified, the sum is over that axis. If ``axis`` is a sequence of axes,</span>
<span class="sd">        reduce over all of them.</span>

<span class="sd">        This is not a node :class:`Operation`, hence it returns a ``torch.Tensor``</span>
<span class="sd">        instead of a :class:`Node`.</span>

<span class="sd">        See also `torch.sum() &lt;https://pytorch.org/docs/stable/generated/torch.sum.html&gt;`_.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str, Axis or list[int, str or Axis], optional</span>
<span class="sd">            Axis or sequence of axes over which to reduce.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; node = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; node.tensor</span>
<span class="sd">        tensor([[-0.2799, -0.4383, -0.8387],</span>
<span class="sd">                [ 1.6225, -0.3370, -1.2316]])</span>
<span class="sd">            </span>
<span class="sd">        &gt;&gt;&gt; node.sum()</span>
<span class="sd">        tensor(-1.5029)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node.sum(&#39;left&#39;)</span>
<span class="sd">        tensor([ 1.3427, -0.7752, -2.0704])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                    <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">ax</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axis_num</span><span class="p">)</span></div>

<div class="viewcode-block" id="AbstractNode.mean"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.mean">[docs]</a>    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Ax</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Ax</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the mean of all elements in the node&#39;s tensor. If an ``axis`` is</span>
<span class="sd">        specified, the mean is over that axis. If ``axis`` is a sequence of axes,</span>
<span class="sd">        reduce over all of them.</span>

<span class="sd">        This is not a node :class:`Operation`, hence it returns a ``torch.Tensor``</span>
<span class="sd">        instead of a :class:`Node`.</span>

<span class="sd">        See also `torch.mean() &lt;https://pytorch.org/docs/stable/generated/torch.mean.html&gt;`_.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str, Axis or list[int, str or Axis], optional</span>
<span class="sd">            Axis or sequence of axes over which to reduce.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; node = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; node.tensor</span>
<span class="sd">        tensor([[ 1.4005, -0.0521, -1.2091],</span>
<span class="sd">                [ 1.9844,  0.3513, -0.5920]])</span>
<span class="sd">            </span>
<span class="sd">        &gt;&gt;&gt; node.mean()</span>
<span class="sd">        tensor(0.3139)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node.mean(&#39;left&#39;)</span>
<span class="sd">        tensor([ 1.6925,  0.1496, -0.9006])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                    <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">ax</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axis_num</span><span class="p">)</span></div>

<div class="viewcode-block" id="AbstractNode.std"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.std">[docs]</a>    <span class="k">def</span> <span class="nf">std</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Ax</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Ax</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the std of all elements in the node&#39;s tensor. If an ``axis`` is</span>
<span class="sd">        specified, the std is over that axis. If ``axis`` is a sequence of axes,</span>
<span class="sd">        reduce over all of them.</span>

<span class="sd">        This is not a node :class:`Operation`, hence it returns a ``torch.Tensor``</span>
<span class="sd">        instead of a :class:`Node`.</span>

<span class="sd">        See also `torch.std() &lt;https://pytorch.org/docs/stable/generated/torch.std.html&gt;`_.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, str, Axis or list[int, str or Axis], optional</span>
<span class="sd">            Axis or sequence of axes over which to reduce.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; node = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; node.tensor</span>
<span class="sd">        tensor([[ 0.2111, -0.9551, -0.7812],</span>
<span class="sd">                [ 0.2254,  0.3381, -0.2461]])</span>
<span class="sd">            </span>
<span class="sd">        &gt;&gt;&gt; node.std()</span>
<span class="sd">        tensor(0.5567)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node.std(&#39;left&#39;)</span>
<span class="sd">        tensor([0.0101, 0.9145, 0.3784])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                    <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">ax</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axis_num</span><span class="p">)</span></div>

<div class="viewcode-block" id="AbstractNode.norm"><a class="viewcode-back" href="../../components.html#tensorkrowch.AbstractNode.norm">[docs]</a>    <span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">p</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
             <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Ax</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the norm of all elements in the node&#39;s tensor. If an ``axis`` is</span>
<span class="sd">        specified, the norm is over that axis. If ``axis`` is a sequence of axes,</span>
<span class="sd">        reduce over all of them.</span>

<span class="sd">        This is not a node :class:`Operation`, hence it returns a ``torch.Tensor``</span>
<span class="sd">        instead of a :class:`Node`.</span>

<span class="sd">        See also `torch.norm() &lt;https://pytorch.org/docs/stable/generated/torch.norm.html&gt;`_.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : int, float</span>
<span class="sd">            The order of the norm.</span>
<span class="sd">        axis : int, str, Axis or list[int, str or Axis], optional</span>
<span class="sd">            Axis or sequence of axes over which to reduce.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; node = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; node.tensor</span>
<span class="sd">        tensor([[ 1.5570,  1.8441, -0.0743],</span>
<span class="sd">                [ 0.4572,  0.7592,  0.6356]])</span>
<span class="sd">            </span>
<span class="sd">        &gt;&gt;&gt; node.norm()</span>
<span class="sd">        tensor(2.6495)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node.norm(axis=&#39;left&#39;)</span>
<span class="sd">        tensor([1.6227, 1.9942, 0.6399])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">axis_num</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                    <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">ax</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">axis_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_axis_num</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">axis_num</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">(</span><span class="se">\n</span><span class="s1"> &#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">name: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">tab_string</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">axes:</span><span class="se">\n</span><span class="si">{</span><span class="n">tab_string</span><span class="p">(</span><span class="n">print_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">edges:</span><span class="se">\n</span><span class="si">{</span><span class="n">tab_string</span><span class="p">(</span><span class="n">print_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">)&#39;</span></div>


<div class="viewcode-block" id="Node"><a class="viewcode-back" href="../../components.html#tensorkrowch.Node">[docs]</a><span class="k">class</span> <span class="nc">Node</span><span class="p">(</span><span class="n">AbstractNode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for non-trainable nodes. Should be subclassed by any class of nodes</span>
<span class="sd">    that are not intended to be trained (e.g. :class:`StackNode`).</span>

<span class="sd">    Can be used for fixed nodes of the :class:`TensorNetwork`, or intermediate</span>
<span class="sd">    nodes that are resultant from an :class:`Operation` between nodes.</span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    </span>
<span class="sd">    All **4 types of nodes** (``leaf``, ``data``, ``virtual`` and ``resultant``)</span>
<span class="sd">    can be ``Node``. In fact, ``data`` and ``resultant`` nodes can **only** be</span>
<span class="sd">    of class ``Node``, since they are not intended to be trainable. To learn</span>
<span class="sd">    more about these **4 types of nodes**, see :class:`AbstractNode`.</span>
<span class="sd">    </span>
<span class="sd">    |</span>

<span class="sd">    For a complete list of properties and methods, see also :class:`AbstractNode`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    shape : list[int], tuple[int], torch.Size, optional</span>
<span class="sd">        Node&#39;s shape, that is, the shape of its tensor. If ``shape`` and</span>
<span class="sd">        ``init_method`` are provided, a tensor will be made for the node. Otherwise,</span>
<span class="sd">        ``tensor`` would be required.</span>
<span class="sd">    axes_names : list[str], tuple[str], optional</span>
<span class="sd">        Sequence of names for each of the node&#39;s axes. Names are used to access</span>
<span class="sd">        the edge that is attached to the node in a certain axis. Hence they should</span>
<span class="sd">        be all distinct. They cannot contain blank spaces or special characters.</span>
<span class="sd">        By default, axes names will be ``&quot;axis_0&quot;``, ..., ``&quot;axis_n&quot;``, being</span>
<span class="sd">        ``n`` the nummber of axes. If an axis&#39; name contains the word ``&quot;batch&quot;``,</span>
<span class="sd">        it will define a batch edge. The word ``&quot;stack&quot;`` cannot be used, since</span>
<span class="sd">        it is reserved for the stack edge of :class:`StackNode`.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        Node&#39;s name, used to access the node from de :class:`TensorNetwork` where</span>
<span class="sd">        it belongs. It cannot contain blank spaces. By default, it is the name</span>
<span class="sd">        of the class (e.g. ``&quot;node&quot;``, ``&quot;paramnode&quot;``).</span>
<span class="sd">    network : TensorNetwork, optional</span>
<span class="sd">        Tensor network where the node should belong. If ``None``, a new tensor</span>
<span class="sd">        network will be created to contain the node.</span>
<span class="sd">    data : bool</span>
<span class="sd">        Boolean indicating if the node is a ``data`` node.</span>
<span class="sd">    virtual : bool</span>
<span class="sd">        Boolean indicating if the node is a ``virtual`` node.</span>
<span class="sd">    override_node : bool</span>
<span class="sd">        Boolean indicating whether the node should override (``True``) another</span>
<span class="sd">        node in the network that has the same name (e.g. if a node is parameterized,</span>
<span class="sd">        it would be required that a new :class:`ParamNode` replaces the</span>
<span class="sd">        non-parameterized node in the network).</span>
<span class="sd">    tensor : torch.Tensor, optional</span>
<span class="sd">        Tensor that is to be stored in the node. If ``None``, ``shape`` and</span>
<span class="sd">        ``init_method`` will be required.</span>
<span class="sd">    edges : list[Edge], optional</span>
<span class="sd">        List of edges that are to be attached to the node. This can be used in</span>
<span class="sd">        case the node inherits the edges from other node(s), like results from</span>
<span class="sd">        :class:`Operations &lt;Operation&gt;`.</span>
<span class="sd">    override_edges : bool</span>
<span class="sd">        Boolean indicating whether the provided ``edges`` should be overriden</span>
<span class="sd">        (``True``) when reattached (e.g. if a node is parameterized, it would</span>
<span class="sd">        be required that the new :class:`ParamNode`&#39;s edges are indeed connected</span>
<span class="sd">        to it, instead of to the original non-parameterized node).</span>
<span class="sd">    node1_list : list[bool], optional</span>
<span class="sd">        If ``edges`` are provided, the list of ``node1`` attributes of each edge</span>
<span class="sd">        should also be provided.</span>
<span class="sd">    init_method : {&quot;zeros&quot;, &quot;ones&quot;, &quot;copy&quot;, &quot;rand&quot;, &quot;randn&quot;}, optional</span>
<span class="sd">        Initialization method.</span>
<span class="sd">    device : torch.device, optional</span>
<span class="sd">        Device where to initialize the tensor if ``init_method`` is provided.</span>
<span class="sd">    kwargs : float</span>
<span class="sd">        Keyword arguments for the different initialization methods. See</span>
<span class="sd">        :meth:`AbstractNode.make_tensor`.</span>
<span class="sd">        </span>
<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; node = tk.Node(shape=(2, 5, 2),</span>
<span class="sd">    ...                axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">    ...                name=&#39;my_node&#39;,</span>
<span class="sd">    ...                init_method=&#39;randn&#39;,</span>
<span class="sd">    ...                mean=0.,</span>
<span class="sd">    ...                std=1.)</span>
<span class="sd">    &gt;&gt;&gt; node</span>
<span class="sd">    Node(</span>
<span class="sd">     	name: my_node</span>
<span class="sd">    	tensor:</span>
<span class="sd">                tensor([[[-1.2517, -1.8147],</span>
<span class="sd">                         [-0.7997, -0.0440],</span>
<span class="sd">                         [-0.2808,  0.3508],</span>
<span class="sd">                         [-1.2380,  0.8859],</span>
<span class="sd">                         [-0.3585,  0.8815]],</span>
<span class="sd">                        [[-0.2898, -2.2775],</span>
<span class="sd">                         [ 1.2856, -0.3222],</span>
<span class="sd">                         [-0.8911, -0.4216],</span>
<span class="sd">                         [ 0.0086,  0.2449],</span>
<span class="sd">                         [-2.1998, -1.6295]]])</span>
<span class="sd">    	axes:</span>
<span class="sd">                [left</span>
<span class="sd">                 input</span>
<span class="sd">                 right]</span>
<span class="sd">    	edges:</span>
<span class="sd">                [my_node[left] &lt;-&gt; None</span>
<span class="sd">                 my_node[input] &lt;-&gt; None</span>
<span class="sd">                 my_node[right] &lt;-&gt; None])</span>
<span class="sd">    </span>
<span class="sd">    Also, one can use one of the :ref:`Initializers` to simplify:</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; node = tk.randn((2, 5, 2))</span>
<span class="sd">    &gt;&gt;&gt; node</span>
<span class="sd">    Node(</span>
<span class="sd">     	name: node</span>
<span class="sd">    	tensor:</span>
<span class="sd">                tensor([[[ 0.6545, -0.0445],</span>
<span class="sd">                         [-0.9265, -0.2730],</span>
<span class="sd">                         [-0.5069, -0.6524],</span>
<span class="sd">                         [-0.8227, -1.1211],</span>
<span class="sd">                         [ 0.2390,  0.9432]],</span>
<span class="sd">                        [[ 0.8633,  0.4402],</span>
<span class="sd">                         [-0.6982,  0.4461],</span>
<span class="sd">                         [-0.0633, -0.9320],</span>
<span class="sd">                         [ 1.6023,  0.5406],</span>
<span class="sd">                         [ 0.3489, -0.3088]]])</span>
<span class="sd">    	axes:</span>
<span class="sd">                [axis_0</span>
<span class="sd">                 axis_1</span>
<span class="sd">                 axis_2]</span>
<span class="sd">    	edges:</span>
<span class="sd">                [node[axis_0] &lt;-&gt; None</span>
<span class="sd">                 node[axis_1] &lt;-&gt; None</span>
<span class="sd">                 node[axis_2] &lt;-&gt; None])</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
    <span class="k">def</span> <span class="nf">_make_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Axis</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Makes :class:`Edges &lt;Edge&gt;` that will be attached to each axis.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_tensor_format</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a ``torch.Tensor`` if input tensor is given as ``torch.nn.Parameter``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor</span>

<div class="viewcode-block" id="Node.parameterize"><a class="viewcode-back" href="../../components.html#tensorkrowch.Node.parameterize">[docs]</a>    <span class="k">def</span> <span class="nf">parameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_param</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="s1">&#39;Node&#39;</span><span class="p">,</span> <span class="s1">&#39;ParamNode&#39;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replaces the node with a parameterized version of it, that is, turns a</span>
<span class="sd">        fixed :class:`Node` into a trainable :class:`ParamNode`.</span>

<span class="sd">        Since the node is **replaced**, it will be completely removed from the</span>
<span class="sd">        network, and its neighbours will point to the new parameterized node.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        set_param : bool</span>
<span class="sd">            Boolean indicating whether the node should be parameterized (``True``).</span>
<span class="sd">            Otherwise (``False``), the non-parameterized node itself will be</span>
<span class="sd">            returned.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Node or ParamNode</span>
<span class="sd">            The original node or a parameterized version of it.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.randn((3, 4))</span>
<span class="sd">        &gt;&gt;&gt; nodeA[1] ^ nodeB[0]</span>
<span class="sd">        &gt;&gt;&gt; paramnodeA = nodeA.parameterize()</span>
<span class="sd">        &gt;&gt;&gt; nodeB.neighbours() == [paramnodeA]</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; isinstance(paramnodeA.tensor, torch.nn.Parameter)</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        ``nodeA`` still exists and has an edge pointing to ``nodeB``, but the</span>
<span class="sd">        latter does not &quot;see&quot; the former. It should be deleted.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; del nodeA</span>
<span class="sd">        </span>
<span class="sd">        To overcome this issue, one should override ``nodeA``:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeA = nodeA.parameterize()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">set_param</span><span class="p">:</span>
            <span class="n">new_node</span> <span class="o">=</span> <span class="n">ParamNode</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                 <span class="n">axes_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                                 <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
                                 <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                                 <span class="n">override_node</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">tensor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                 <span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                                 <span class="n">override_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">new_node</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Node.copy"><a class="viewcode-back" href="../../components.html#tensorkrowch.Node.copy">[docs]</a>    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">share_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Node&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a copy of the node. That is, returns a node whose tensor is a copy</span>
<span class="sd">        of the original, whose edges are directly inherited (these are not copies,</span>
<span class="sd">        but the exact same edges) and whose name is extended with the suffix</span>
<span class="sd">        ``&quot;_copy&quot;``.</span>
<span class="sd">        </span>
<span class="sd">        To create a copy that has its own (non-inherited) edges, one can use</span>
<span class="sd">        :meth:`reattach_edges` afterwards.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        share_tensor : bool</span>
<span class="sd">            Boolean indicating whether the copied node should store its own</span>
<span class="sd">            copy of the tensor (``False``) or share it with the original node</span>
<span class="sd">            (``True``) storing a reference to it.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Node</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; node = tk.randn(shape=(2, 3), name=&#39;node&#39;)</span>
<span class="sd">        &gt;&gt;&gt; copy = node.copy()</span>
<span class="sd">        &gt;&gt;&gt; node.tensor_address() != copy.tensor_address()</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; torch.equal(node.tensor, copy.tensor)</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        If tensor is shared:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; copy = node.copy(True)</span>
<span class="sd">        &gt;&gt;&gt; node.tensor_address() == copy.tensor_address()</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; torch.equal(node.tensor, copy.tensor)</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">share_tensor</span><span class="p">:</span>
            <span class="n">new_node</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
                        <span class="n">axes_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="s1">&#39;_copy&#39;</span><span class="p">,</span>
                        <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                        <span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                        <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">())</span>
            <span class="n">new_node</span><span class="o">.</span><span class="n">set_tensor_from</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  
            <span class="n">new_node</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
                            <span class="n">axes_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="s1">&#39;_copy&#39;</span><span class="p">,</span>
                            <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                            <span class="n">tensor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                            <span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                            <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">new_node</span></div></div>


<div class="viewcode-block" id="ParamNode"><a class="viewcode-back" href="../../components.html#tensorkrowch.ParamNode">[docs]</a><span class="k">class</span> <span class="nc">ParamNode</span><span class="p">(</span><span class="n">AbstractNode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for trainable nodes. Should be subclassed by any class of nodes that</span>
<span class="sd">    are intended to be trained (e.g. :class:`ParamStackNode`).</span>

<span class="sd">    Should be used as the initial nodes conforming the :class:`TensorNetwork`,</span>
<span class="sd">    if it is going to be trained. When operating these initial nodes, the resultant</span>
<span class="sd">    nodes will be non-parameterized (e.g. :class:`Node`, :class:`StackNode`).</span>

<span class="sd">    The main difference with :class:`Nodes &lt;Node&gt;` is that ``ParamNodes`` have</span>
<span class="sd">    ``torch.nn.Parameter`` tensors instead of ``torch.Tensor``. Therefore, a</span>
<span class="sd">    ``ParamNode`` is a sort of `parameter` that is attached to the</span>
<span class="sd">    :class:`TensorNetwork` (which is itself a ``torch.nn.Module``). That is,</span>
<span class="sd">    the **list of parameters of the tensor network** module contains the tensors</span>
<span class="sd">    of all ``ParamNodes``.</span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    </span>
<span class="sd">    ``ParamNodes`` can only be ``leaf`` and ``virtual`` (e.g. a ``virtual`` node</span>
<span class="sd">    used in a uniform :class:`TensorNetwork` to store the tensor that is shared</span>
<span class="sd">    by all the trainable nodes must also be a ``ParamNode``, since it stores</span>
<span class="sd">    a ``torch.nn.Parameter``.</span>
<span class="sd">    </span>
<span class="sd">    |</span>

<span class="sd">    For a complete list of properties and methods, see also :class:`AbstractNode`.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    shape : list[int], tuple[int], torch.Size, optional</span>
<span class="sd">        Node&#39;s shape, that is, the shape of its tensor. If ``shape`` and</span>
<span class="sd">        ``init_method`` are provided, a tensor will be made for the node. Otherwise,</span>
<span class="sd">        ``tensor`` would be required.</span>
<span class="sd">    axes_names : list[str], tuple[str], optional</span>
<span class="sd">        Sequence of names for each of the node&#39;s axes. Names are used to access</span>
<span class="sd">        the edge that is attached to the node in a certain axis. Hence they should</span>
<span class="sd">        be all distinct. They cannot contain blank spaces or special characters.</span>
<span class="sd">        By default, axes names will be ``&quot;axis_0&quot;``, ..., ``&quot;axis_n&quot;``, being</span>
<span class="sd">        ``n`` the nummber of axes. If an axis&#39; name contains the word ``&quot;batch&quot;``,</span>
<span class="sd">        it will define a batch edge. The word ``&quot;stack&quot;`` cannot be used, since</span>
<span class="sd">        it is reserved for the stack edge of :class:`StackNode`.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        Node&#39;s name, used to access the node from de :class:`TensorNetwork` where</span>
<span class="sd">        it belongs. It cannot contain blank spaces. By default, it is the name</span>
<span class="sd">        of the class (e.g. ``&quot;node&quot;``, ``&quot;paramnode&quot;``).</span>
<span class="sd">    network : TensorNetwork, optional</span>
<span class="sd">        Tensor network where the node should belong. If ``None``, a new tensor</span>
<span class="sd">        network will be created to contain the node.</span>
<span class="sd">    virtual : bool</span>
<span class="sd">        Boolean indicating if the node is a ``virtual`` node.</span>
<span class="sd">    override_node : bool</span>
<span class="sd">        Boolean indicating whether the node should override (``True``) another</span>
<span class="sd">        node in the network that has the same name (e.g. if a node is parameterized,</span>
<span class="sd">        it would be required that a new :class:`ParamNode` replaces the</span>
<span class="sd">        non-parameterized node in the network).</span>
<span class="sd">    tensor : torch.Tensor, optional</span>
<span class="sd">        Tensor that is to be stored in the node. If ``None``, ``shape`` and</span>
<span class="sd">        ``init_method`` will be required.</span>
<span class="sd">    edges : list[Edge], optional</span>
<span class="sd">        List of edges that are to be attached to the node. This can be used in</span>
<span class="sd">        case the node inherits the edges from other node(s), like results from</span>
<span class="sd">        :class:`Operations &lt;Operation&gt;`.</span>
<span class="sd">    override_edges : bool</span>
<span class="sd">        Boolean indicating whether the provided ``edges`` should be overriden</span>
<span class="sd">        (``True``) when reattached (e.g. if a node is parameterized, it would</span>
<span class="sd">        be required that the new :class:`ParamNode`&#39;s edges are indeed connected</span>
<span class="sd">        to it, instead of to the original non-parameterized node).</span>
<span class="sd">    node1_list : list[bool], optional</span>
<span class="sd">        If ``edges`` are provided, the list of ``node1`` attributes of each edge</span>
<span class="sd">        should also be provided.</span>
<span class="sd">    init_method : {&quot;zeros&quot;, &quot;ones&quot;, &quot;copy&quot;, &quot;rand&quot;, &quot;randn&quot;}, optional</span>
<span class="sd">        Initialization method.</span>
<span class="sd">    device : torch.device, optional</span>
<span class="sd">        Device where to initialize the tensor if ``init_method`` is provided.</span>
<span class="sd">    kwargs : float</span>
<span class="sd">        Keyword arguments for the different initialization methods. See</span>
<span class="sd">        :meth:`AbstractNode.make_tensor`.</span>
<span class="sd">        </span>
<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; node = tk.ParamNode(shape=(2, 5, 2),</span>
<span class="sd">    ...                     axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">    ...                     name=&#39;my_paramnode&#39;,</span>
<span class="sd">    ...                     init_method=&#39;randn&#39;,</span>
<span class="sd">    ...                     mean=0.,</span>
<span class="sd">    ...                     std=1.)</span>
<span class="sd">    &gt;&gt;&gt; node</span>
<span class="sd">    ParamNode(</span>
<span class="sd">     	name: my_paramnode</span>
<span class="sd">    	tensor:</span>
<span class="sd">                Parameter containing:</span>
<span class="sd">                tensor([[[ 1.8090, -0.1371],</span>
<span class="sd">                         [-0.0501, -1.0371],</span>
<span class="sd">                         [ 1.4588, -0.8361],</span>
<span class="sd">                         [-0.4974, -1.9957],</span>
<span class="sd">                         [ 0.3760, -1.0412]],</span>
<span class="sd">                        [[ 0.3393, -0.2503],</span>
<span class="sd">                         [ 1.7752, -0.0188],</span>
<span class="sd">                         [-0.9561, -0.0806],</span>
<span class="sd">                         [-1.0465, -0.5731],</span>
<span class="sd">                         [ 1.5021,  0.4181]]], requires_grad=True)</span>
<span class="sd">    	axes:</span>
<span class="sd">                [left</span>
<span class="sd">                 input</span>
<span class="sd">                 right]</span>
<span class="sd">    	edges:</span>
<span class="sd">                [my_paramnode[left] &lt;-&gt; None</span>
<span class="sd">                 my_paramnode[input] &lt;-&gt; None</span>
<span class="sd">                 my_paramnode[right] &lt;-&gt; None])</span>
<span class="sd">    </span>
<span class="sd">    Also, one can use one of the :ref:`Initializers` to simplify:</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; node = tk.randn((2, 5, 2),</span>
<span class="sd">    ...                 param_node=True)</span>
<span class="sd">    &gt;&gt;&gt; node</span>
<span class="sd">    ParamNode(</span>
<span class="sd">     	name: paramnode</span>
<span class="sd">    	tensor:</span>
<span class="sd">                Parameter containing:</span>
<span class="sd">                tensor([[[-0.8442,  1.4184],</span>
<span class="sd">                         [ 0.4431, -1.4385],</span>
<span class="sd">                         [-0.5161, -0.6492],</span>
<span class="sd">                         [ 0.2095,  0.5760],</span>
<span class="sd">                         [-0.9925, -1.5797]],</span>
<span class="sd">                        [[-0.8649, -0.5401],</span>
<span class="sd">                         [-0.1091,  1.1654],</span>
<span class="sd">                         [-0.3821, -0.2477],</span>
<span class="sd">                         [-0.7688, -2.4731],</span>
<span class="sd">                         [-0.0234,  0.9618]]], requires_grad=True)</span>
<span class="sd">    	axes:</span>
<span class="sd">                [axis_0</span>
<span class="sd">                 axis_1</span>
<span class="sd">                 axis_2]</span>
<span class="sd">    	edges:</span>
<span class="sd">                [paramnode[axis_0] &lt;-&gt; None</span>
<span class="sd">                 paramnode[axis_1] &lt;-&gt; None</span>
<span class="sd">                 paramnode[axis_2] &lt;-&gt; None])</span>
<span class="sd">       </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Shape</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">axes_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Text</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">network</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;TensorNetwork&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">virtual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">override_node</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">edges</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">override_edges</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">node1_list</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
                         <span class="n">axes_names</span><span class="o">=</span><span class="n">axes_names</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                         <span class="n">network</span><span class="o">=</span><span class="n">network</span><span class="p">,</span>
                         <span class="n">data</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">virtual</span><span class="o">=</span><span class="n">virtual</span><span class="p">,</span>
                         <span class="n">override_node</span><span class="o">=</span><span class="n">override_node</span><span class="p">,</span>
                         <span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span>
                         <span class="n">edges</span><span class="o">=</span><span class="n">edges</span><span class="p">,</span>
                         <span class="n">override_edges</span><span class="o">=</span><span class="n">override_edges</span><span class="p">,</span>
                         <span class="n">node1_list</span><span class="o">=</span><span class="n">node1_list</span><span class="p">,</span>
                         <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
                         <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                         <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># ----------</span>
    <span class="c1"># Properties</span>
    <span class="c1"># ----------</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns gradient of the param-node&#39;s tensor.</span>

<span class="sd">        See also `torch.Tensor.grad</span>
<span class="sd">        &lt;https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html&gt;`_</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor or None</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; paramnode = tk.randn((2, 3), param_node=True)</span>
<span class="sd">        &gt;&gt;&gt; paramnode.tensor</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[-0.3340,  0.6811, -0.2866],</span>
<span class="sd">                [ 1.3371,  1.4761,  0.6551]], requires_grad=True)</span>
<span class="sd">            </span>
<span class="sd">        &gt;&gt;&gt; paramnode.sum().backward()</span>
<span class="sd">        &gt;&gt;&gt; paramnode.grad</span>
<span class="sd">        tensor([[1., 1., 1.],</span>
<span class="sd">                [1., 1., 1.]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">aux_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">aux_node</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span>
                <span class="n">aux_node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]]</span>

        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">aux_grad</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">grad</span>
        <span class="k">if</span> <span class="n">aux_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">aux_grad</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">aux_grad</span>
            <span class="k">return</span> <span class="n">aux_grad</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]]</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_create_resultant</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private constructor to create resultant nodes. Called from</span>
<span class="sd">        :class:`Operations &lt;Operation&gt;`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;ParamNodes can not be resultant nodes&#39;</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_make_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Axis</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Makes ``Edges`` that will be attached to each axis.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_tensor_format</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Parameter</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a nn.Parameter if input tensor is just torch.Tensor.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor</span>
        <span class="k">return</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

<div class="viewcode-block" id="ParamNode.parameterize"><a class="viewcode-back" href="../../components.html#tensorkrowch.ParamNode.parameterize">[docs]</a>    <span class="k">def</span> <span class="nf">parameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_param</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="s1">&#39;Node&#39;</span><span class="p">,</span> <span class="s1">&#39;ParamNode&#39;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replaces the param-node with a de-parameterized version of it, that is,</span>
<span class="sd">        turns a :class:`ParamNode` into a non-trainable, fixed :class:`Node`.</span>

<span class="sd">        Since the param-node is **replaced**, it will be completely removed from</span>
<span class="sd">        the network, and its neighbours will point to the new node.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        set_param : bool</span>
<span class="sd">            Boolean indicating whether the node should stay parameterized</span>
<span class="sd">            (``True``), thus returning the param-node itself. Otherwise (``False``),</span>
<span class="sd">            the param-node will be de-parameterized.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ParamNode or Node</span>
<span class="sd">            The original node or a de-parameterized version of it.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; paramnodeA = tk.randn((2, 3), param_node=True)</span>
<span class="sd">        &gt;&gt;&gt; paramnodeB = tk.randn((3, 4), param_node=True)</span>
<span class="sd">        &gt;&gt;&gt; paramnodeA[1] ^ paramnodeB[0]</span>
<span class="sd">        &gt;&gt;&gt; nodeA = paramnodeA.parameterize(False)</span>
<span class="sd">        &gt;&gt;&gt; paramnodeB.neighbours() == [nodeA]</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; isinstance(nodeA.tensor, torch.nn.Parameter)</span>
<span class="sd">        False</span>
<span class="sd">        </span>
<span class="sd">        ``paramnodeA`` still exists and has an edge pointing to ``paramnodeB``,</span>
<span class="sd">        but the latter does not &quot;see&quot; the former. It should be deleted.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; del paramnodeA</span>
<span class="sd">        </span>
<span class="sd">        To overcome this issue, one should override ``paramnodeA``:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; paramnodeA = paramnodeA.parameterize()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">set_param</span><span class="p">:</span>
            <span class="n">new_node</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                            <span class="n">axes_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
                            <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                            <span class="n">override_node</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">tensor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                            <span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                            <span class="n">override_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">new_node</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ParamNode.copy"><a class="viewcode-back" href="../../components.html#tensorkrowch.ParamNode.copy">[docs]</a>    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">share_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;ParamNode&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a copy of the param-node. That is, returns a param-node whose</span>
<span class="sd">        tensor is a copy of the original, whose edges are directly inherited</span>
<span class="sd">        (these are not copies, but the exact same edges) and whose name is</span>
<span class="sd">        extended with the suffix ``&quot;_copy&quot;``.</span>
<span class="sd">        </span>
<span class="sd">        To create a copy that has its own (non-inherited) edges, one can use</span>
<span class="sd">        :meth:`reattach_edges` afterwards.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        share_tensor : bool</span>
<span class="sd">            Boolean indicating whether the copied param-node should store its</span>
<span class="sd">            own copy of the tensor (``False``) or share it with the original</span>
<span class="sd">            param-node (``True``) storing a reference to it.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ParamNode</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; paramnode = tk.randn(shape=(2, 3), name=&#39;node&#39;, param_node=True)</span>
<span class="sd">        &gt;&gt;&gt; copy = paramnode.copy()</span>
<span class="sd">        &gt;&gt;&gt; paramnode.tensor_address() != copy.tensor_address()</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; torch.equal(paramnode.tensor, copy.tensor)</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        If tensor is shared:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; copy = paramnode.copy(True)</span>
<span class="sd">        &gt;&gt;&gt; paramnode.tensor_address() == copy.tensor_address()</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; torch.equal(paramnode.tensor, copy.tensor)</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">share_tensor</span><span class="p">:</span>
            <span class="n">new_node</span> <span class="o">=</span> <span class="n">ParamNode</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
                        <span class="n">axes_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="s1">&#39;_copy&#39;</span><span class="p">,</span>
                        <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                        <span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                        <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">())</span>
            <span class="n">new_node</span><span class="o">.</span><span class="n">set_tensor_from</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  
            <span class="n">new_node</span> <span class="o">=</span> <span class="n">ParamNode</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
                            <span class="n">axes_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="s1">&#39;_copy&#39;</span><span class="p">,</span>
                            <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                            <span class="n">tensor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                            <span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                            <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_node1</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">new_node</span></div></div>


<span class="c1">###############################################################################</span>
<span class="c1">#                                 STACK NODES                                 #</span>
<span class="c1">###############################################################################</span>
<div class="viewcode-block" id="StackNode"><a class="viewcode-back" href="../../components.html#tensorkrowch.StackNode">[docs]</a><span class="k">class</span> <span class="nc">StackNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for stacked nodes. ``StackNodes`` are nodes that store the information</span>
<span class="sd">    of a list of nodes that are stacked via :func:`stack`, although they can also</span>
<span class="sd">    be instantiated directly. To do so, there are two options:</span>

<span class="sd">    * Provide a sequence of nodes: if ``nodes`` are provided, their tensors will</span>
<span class="sd">      be stacked and stored in the ``StackNode``. It is necessary that all nodes</span>
<span class="sd">      are of the same class (:class:`Node` or :class:`ParamNode`), have the same</span>
<span class="sd">      rank (although dimension of each leg can be different for different nodes;</span>
<span class="sd">      in which case smaller tensors are extended with 0&#39;s to match the dimensions</span>
<span class="sd">      of the largest tensor in the stack), same axes names (to ensure only the</span>
<span class="sd">      &quot;same kind&quot; of nodes are stacked), belong to the same network and have edges</span>
<span class="sd">      with the same type in each axis (:class:`Edge` or :class:`ParamEdge`).</span>

<span class="sd">    * Provide a stacked tensor: if the stacked ``tensor`` is provided, it is also</span>
<span class="sd">      necessary to specify the ``axes_names``, ``network``, ``edges`` and</span>
<span class="sd">      ``node1_list``.</span>
<span class="sd">      </span>
<span class="sd">    |</span>

<span class="sd">    ``StackNodes`` have an additional axis for the new `stack` dimension, which</span>
<span class="sd">    is a batch edge. This way, some contractions can be computed in parallel by</span>
<span class="sd">    first stacking two sequences of nodes (connected pair-wise), performing the</span>
<span class="sd">    batch contraction and finally unbinding the ``StackNodes`` to retrieve just</span>
<span class="sd">    one sequence of nodes.</span>

<span class="sd">    For the rest of the axes, a list of the edges corresponding to all nodes in</span>
<span class="sd">    the stack is stored, so that, when :func:`unbinding &lt;unbind&gt;` the stack, it</span>
<span class="sd">    can be inferred to which nodes the unbinded nodes have to be connected.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    nodes : list[AbstractNode] or tuple[AbstractNode], optional</span>
<span class="sd">        Sequence of nodes that are to be stacked. They should all be of the same</span>
<span class="sd">        class (:class:`Node` or :class:`ParamNode`), have the same rank, same</span>
<span class="sd">        axes names and belong to the same network. They do not need to have equal</span>
<span class="sd">        shapes.</span>
<span class="sd">    axes_names : list[str], tuple[str], optional</span>
<span class="sd">        Sequence of names for each of the node&#39;s axes. Names are used to access</span>
<span class="sd">        the edge that is attached to the node in a certain axis. Hence they should</span>
<span class="sd">        be all distinct. Necessary if ``nodes`` are not provided.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        Node&#39;s name, used to access the node from de :class:`TensorNetwork` where</span>
<span class="sd">        it belongs. It cannot contain blank spaces.</span>
<span class="sd">    network : TensorNetwork, optional</span>
<span class="sd">        Tensor network where the node should belong. Necessary if ``nodes`` are</span>
<span class="sd">        not provided.</span>
<span class="sd">    override_node : bool, optional</span>
<span class="sd">        Boolean indicating whether the node should override (``True``) another</span>
<span class="sd">        node in the network that has the same name (e.g. if a node is parameterized,</span>
<span class="sd">        it would be required that a new :class:`ParamNode` replaces the</span>
<span class="sd">        non-parameterized node in the network).</span>
<span class="sd">    tensor : torch.Tensor, optional</span>
<span class="sd">        Tensor that is to be stored in the node. Necessary if ``nodes`` are not</span>
<span class="sd">        provided.</span>
<span class="sd">    edges : list[Edge], optional</span>
<span class="sd">        List of edges that are to be attached to the node. Necessary if ``nodes``</span>
<span class="sd">        are not provided.</span>
<span class="sd">    node1_list : list[bool], optional</span>
<span class="sd">        If ``edges`` are provided, the list of ``node1`` attributes of each edge</span>
<span class="sd">        should also be provided. Necessary if ``nodes`` are not provided.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; net = tk.TensorNetwork()</span>
<span class="sd">    &gt;&gt;&gt; nodes = [tk.randn(shape=(2, 4, 2),</span>
<span class="sd">    ...                   axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">    ...                   network=net)</span>
<span class="sd">    ...          for _ in range(10)]</span>
<span class="sd">    &gt;&gt;&gt; data = [tk.randn(shape=(4,),</span>
<span class="sd">    ...                  axes_names=(&#39;feature&#39;,),</span>
<span class="sd">    ...                  network=net)</span>
<span class="sd">    ...         for _ in range(10)]</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; for i in range(10):</span>
<span class="sd">    ...     nodes[i][&#39;input&#39;] ^ data[i][&#39;feature&#39;]</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; stack_nodes = tk.stack(nodes)</span>
<span class="sd">    &gt;&gt;&gt; stack_data = tk.stack(data)</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # It is necessary to re-connect stacks</span>
<span class="sd">    &gt;&gt;&gt; stack_nodes[&#39;input&#39;] ^ stack_data[&#39;feature&#39;]</span>
<span class="sd">    &gt;&gt;&gt; result = tk.unbind(stack_nodes @ stack_data)</span>
<span class="sd">    &gt;&gt;&gt; print(result[0].name)</span>
<span class="sd">    unbind_0</span>

<span class="sd">    &gt;&gt;&gt; print(result[0].axes)</span>
<span class="sd">    [Axis( left (0) ), Axis( right (1) )]</span>

<span class="sd">    &gt;&gt;&gt; print(result[0].shape)</span>
<span class="sd">    torch.Size([2, 2])</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">nodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">AbstractNode</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">axes_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Text</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">network</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;TensorNetwork&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">override_node</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">edges</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">node1_list</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="k">if</span> <span class="n">nodes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`nodes` should be a list or tuple of nodes&#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">StackNode</span><span class="p">,</span> <span class="n">ParamStackNode</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s1">&#39;Cannot create a stack using (Param)StackNode</span><span class="se">\&#39;</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;If `nodes` are provided, `tensor` must not be given&#39;</span><span class="p">)</span>

            <span class="c1"># Check all nodes share properties</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nodes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Cannot stack nodes of different types. Nodes &#39;</span>
                                    <span class="s1">&#39;must be either all Node or all ParamNode type&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s1">&#39;Cannot stack nodes with different number of edges&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axes_names</span> <span class="o">!=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axes_names</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s1">&#39;Stacked nodes must have the same name for each axis&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_network</span> <span class="o">!=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">_network</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s1">&#39;Stacked nodes must all be in the same network&#39;</span><span class="p">)</span>

            <span class="n">edges_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># Each axis has a list of edges</span>
            <span class="n">node1_lists_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_axes</span><span class="p">:</span>
                    <span class="n">edge</span> <span class="o">=</span> <span class="n">node</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">axis</span><span class="o">.</span><span class="n">_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">edges_dict</span><span class="p">:</span>
                        <span class="n">edges_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">edge</span><span class="p">]</span>
                        <span class="n">node1_lists_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_node1</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">edges_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
                        <span class="n">node1_lists_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axis</span><span class="o">.</span><span class="n">_node1</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_edges_dict</span> <span class="o">=</span> <span class="n">edges_dict</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_node1_lists_dict</span> <span class="o">=</span> <span class="n">node1_lists_dict</span>

            <span class="n">tensor</span> <span class="o">=</span> <span class="n">stack_unequal_tensors</span><span class="p">([</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">])</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axes_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;stack&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                             <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                             <span class="n">network</span><span class="o">=</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                             <span class="n">override_node</span><span class="o">=</span><span class="n">override_node</span><span class="p">,</span>
                             <span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Case stacked tensor is provided, and there is no need of having</span>
            <span class="c1"># to stack the nodes&#39; tensors</span>
            <span class="k">if</span> <span class="n">axes_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;If `nodes` are not provided, `axes_names` must be given&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">network</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;If `nodes` are not provided, `network` must be given&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;If `nodes` are not provided, `tensor` must be given&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">edges</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;If `nodes` are not provided, `edges` must be given&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">node1_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;If `nodes` are not provided, `node1_list` must be given&#39;</span><span class="p">)</span>

            <span class="n">edges_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="n">node1_lists_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes_names</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">edges</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="n">edges_dict</span><span class="p">[</span><span class="n">axis_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">edge</span><span class="o">.</span><span class="n">_edges</span>
                <span class="n">node1_lists_dict</span><span class="p">[</span><span class="n">axis_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">edge</span><span class="o">.</span><span class="n">_node1_list</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_edges_dict</span> <span class="o">=</span> <span class="n">edges_dict</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_node1_lists_dict</span> <span class="o">=</span> <span class="n">node1_lists_dict</span>

            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axes_names</span><span class="o">=</span><span class="n">axes_names</span><span class="p">,</span>
                             <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                             <span class="n">network</span><span class="o">=</span><span class="n">network</span><span class="p">,</span>
                             <span class="n">override_node</span><span class="o">=</span><span class="n">override_node</span><span class="p">,</span>
                             <span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span>
                             <span class="n">edges</span><span class="o">=</span><span class="n">edges</span><span class="p">,</span>
                             <span class="n">node1_list</span><span class="o">=</span><span class="n">node1_list</span><span class="p">)</span>

    <span class="c1"># ----------</span>
    <span class="c1"># Properties</span>
    <span class="c1"># ----------</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edges_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns dictionary where the keys are the axes. For each axis, the value</span>
<span class="sd">        is the list of all the edges (one from each node) that correspond to</span>
<span class="sd">        that axis.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges_dict</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">node1_lists_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns dictionary where the keys are the axes. For each axis, the value</span>
<span class="sd">        is the list with the ``node1`` attribute of that axis for all nodes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node1_lists_dict</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
    <span class="k">def</span> <span class="nf">_make_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Axis</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Makes ``StackEdges``that will be attached to each axis. Also makes an</span>
<span class="sd">        ``Edge`` for the stack dimension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span><span class="o">.</span><span class="n">_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Stack axis</span>
            <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">StackEdge</span><span class="p">(</span><span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">],</span>
                             <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_node1_lists_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">],</span>
                             <span class="n">node1</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="ParamStackNode"><a class="viewcode-back" href="../../components.html#tensorkrowch.ParamStackNode">[docs]</a><span class="k">class</span> <span class="nc">ParamStackNode</span><span class="p">(</span><span class="n">ParamNode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for parametric stacked nodes. They are essentially the same as</span>
<span class="sd">    :class:`StackNodes &lt;StackNode&gt;` but they are :class:`ParamNodes &lt;ParamNode&gt;`.</span>
<span class="sd">    </span>
<span class="sd">    They are used to optimize memory usage and save some time when the first</span>
<span class="sd">    operation that occurs to param-nodes in a contraction (that might be</span>
<span class="sd">    computed several times during training) is :func:`stack`. If this is the case,</span>
<span class="sd">    the param-nodes no longer store their own tensors, but rather they make</span>
<span class="sd">    reference to a slide of a greater ``ParamStackNode`` (if ``auto_stack``</span>
<span class="sd">    attribute of the :class:`TensorNetwork` is set to ``True``). Hence, that</span>
<span class="sd">    first :func:`stack` is never actually computed.</span>
<span class="sd">    </span>
<span class="sd">    The ``ParamStackNode`` that results from this process uses the reserved name</span>
<span class="sd">    ``&quot;virtual_stack&quot;``, as explained :class:`here &lt;AbstractNode&gt;`. This node</span>
<span class="sd">    stores the tensor from which all the stacked :class:`ParamNodes &lt;ParamNode&gt;`</span>
<span class="sd">    just take one `slice`.</span>
<span class="sd">    </span>
<span class="sd">    This behaviour occurs when stacking param-nodes via :func:`stack`, not when</span>
<span class="sd">    instantiating ``ParamStackNode`` manually.</span>
<span class="sd">    </span>
<span class="sd">    |</span>

<span class="sd">    ``ParamStackNodes`` can only be instantiated by providing a sequence of nodes.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    nodes : list[AbstractNode] or tuple[AbstractNode]</span>
<span class="sd">        Sequence of nodes that are to be stacked. They should all be of the same</span>
<span class="sd">        class (:class:`Node` or :class:`ParamNode`), have the same rank, same</span>
<span class="sd">        axes names and belong to the same network. They do not need to have equal</span>
<span class="sd">        shapes.</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        Node&#39;s name, used to access the node from de :class:`TensorNetwork` where</span>
<span class="sd">        it belongs. It cannot contain blank spaces.</span>
<span class="sd">    virtual : bool, optional</span>
<span class="sd">        Boolean indicating if the node is a ``virtual`` node. Since it will be</span>
<span class="sd">        used mainly for the case described :class:`here &lt;ParamStackNode&gt;`, the</span>
<span class="sd">        node will be ``virtual``, it will not be an `effective` part of the</span>
<span class="sd">        tensor network.</span>
<span class="sd">    override_node : bool, optional</span>
<span class="sd">        Boolean indicating whether the node should override (``True``) another</span>
<span class="sd">        node in the network that has the same name (e.g. if a node is parameterized,</span>
<span class="sd">        it would be required that a new :class:`ParamNode` replaces the</span>
<span class="sd">        non-parameterized node in the network).</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; net = tk.TensorNetwork()</span>
<span class="sd">    &gt;&gt;&gt; net.auto_stack = True</span>
<span class="sd">    &gt;&gt;&gt; nodes = [tk.randn(shape=(2, 4, 2),</span>
<span class="sd">    ...                   axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">    ...                   network=net,</span>
<span class="sd">    ...                   param_node=True)</span>
<span class="sd">    ...          for _ in range(10)]</span>
<span class="sd">    &gt;&gt;&gt; data = [tk.randn(shape=(4,),</span>
<span class="sd">    ...                  axes_names=(&#39;feature&#39;,),</span>
<span class="sd">    ...                  network=net)</span>
<span class="sd">    ...         for _ in range(10)]</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; for i in range(10):</span>
<span class="sd">    ...     nodes[i][&#39;input&#39;] ^ data[i][&#39;feature&#39;]</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; stack_nodes = tk.stack(nodes)</span>
<span class="sd">    &gt;&gt;&gt; stack_nodes.name = &#39;my_stack&#39;</span>
<span class="sd">    &gt;&gt;&gt; print(nodes[0].tensor_address())</span>
<span class="sd">    my_stack</span>

<span class="sd">    &gt;&gt;&gt; stack_data = tk.stack(data)</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # It is necessary to re-connect stacks</span>
<span class="sd">    &gt;&gt;&gt; stack_nodes[&#39;input&#39;] ^ stack_data[&#39;feature&#39;]</span>
<span class="sd">    &gt;&gt;&gt; result = tk.unbind(stack_nodes @ stack_data)</span>
<span class="sd">    &gt;&gt;&gt; print(result[0].name)</span>
<span class="sd">    unbind_0</span>

<span class="sd">    &gt;&gt;&gt; print(result[0].axes)</span>
<span class="sd">    [Axis( left (0) ), Axis( right (1) )]</span>

<span class="sd">    &gt;&gt;&gt; print(result[0].shape)</span>
<span class="sd">    torch.Size([2, 2])</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">nodes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AbstractNode</span><span class="p">],</span>
                 <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">virtual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">override_node</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`nodes` should be a list or tuple of nodes&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">(</span><span class="n">StackNode</span><span class="p">,</span> <span class="n">ParamStackNode</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s1">&#39;Cannot create a stack using (Param)StackNode</span><span class="se">\&#39;</span><span class="s1">s&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nodes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Cannot stack nodes of different types. Nodes &#39;</span>
                                <span class="s1">&#39;must be either all Node or all ParamNode type&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Cannot stack nodes with different number of edges&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axes_names</span> <span class="o">!=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axes_names</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Stacked nodes must have the same name for each axis&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_network</span> <span class="o">!=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">_network</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Stacked nodes must all be in the same network&#39;</span><span class="p">)</span>

        <span class="n">edges_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">node1_lists_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_axes</span><span class="p">:</span>
                <span class="n">edge</span> <span class="o">=</span> <span class="n">node</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">axis</span><span class="o">.</span><span class="n">_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">edges_dict</span><span class="p">:</span>
                    <span class="n">edges_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">edge</span><span class="p">]</span>
                    <span class="n">node1_lists_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_node1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">edges_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
                    <span class="n">node1_lists_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axis</span><span class="o">.</span><span class="n">_node1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_edges_dict</span> <span class="o">=</span> <span class="n">edges_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_node1_lists_dict</span> <span class="o">=</span> <span class="n">node1_lists_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">nodes</span>

        <span class="n">tensor</span> <span class="o">=</span> <span class="n">stack_unequal_tensors</span><span class="p">([</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axes_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;stack&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axes_names</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                         <span class="n">network</span><span class="o">=</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span>
                         <span class="n">virtual</span><span class="o">=</span><span class="n">virtual</span><span class="p">,</span>
                         <span class="n">override_node</span><span class="o">=</span><span class="n">override_node</span><span class="p">,</span>
                         <span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">)</span>

    <span class="c1"># ----------</span>
    <span class="c1"># Properties</span>
    <span class="c1"># ----------</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edges_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns dictionary where the keys are the axes. For each axis, the value</span>
<span class="sd">        is the list of all the edges (one from each node) that correspond to</span>
<span class="sd">        that axis.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges_dict</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">node1_lists_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns dictionary where the keys are the axes. For each axis, the value</span>
<span class="sd">        is the list with the ``node1`` attribute of that axis for all nodes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node1_lists_dict</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
    <span class="k">def</span> <span class="nf">_make_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Axis</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Makes ``StackEdges``that will be attached to each axis. Also makes an</span>
<span class="sd">        ``Edge`` for the stack dimension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span><span class="o">.</span><span class="n">num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Stack axis</span>
            <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">StackEdge</span><span class="p">(</span><span class="n">edges</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">],</span>
                             <span class="n">node1_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_node1_lists_dict</span><span class="p">[</span><span class="n">axis</span><span class="o">.</span><span class="n">_name</span><span class="p">],</span>
                             <span class="n">node1</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<span class="c1">###############################################################################</span>
<span class="c1">#                                    EDGES                                    #</span>
<span class="c1">###############################################################################</span>
<div class="viewcode-block" id="Edge"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge">[docs]</a><span class="k">class</span> <span class="nc">Edge</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for edges. Should be subclassed by any new class of edges.</span>

<span class="sd">    An edge is nothing more than an object that wraps references to the nodes it</span>
<span class="sd">    connects. Thus it stores information like the nodes it connects, the</span>
<span class="sd">    corresponding nodes&#39; axes it is attached to, whether it is dangling or</span>
<span class="sd">    batch, its size, etc.</span>

<span class="sd">    Above all, its importance lies in that edges enable to connect nodes, forming</span>
<span class="sd">    any possible graph, and to perform easily :class:`Operations &lt;Operation&gt;` like</span>
<span class="sd">    contracting and splitting nodes.</span>
<span class="sd">    </span>
<span class="sd">    |</span>

<span class="sd">    Furthermore, edges have specific operations like :meth:`contract_` or</span>
<span class="sd">    :meth:`svd_` (and its variations) that allow in-place modification of the</span>
<span class="sd">    :class:`TensorNetwork`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    node1 : AbstractNode</span>
<span class="sd">        First node to which the edge is connected.</span>
<span class="sd">    axis1: int, str or Axis</span>
<span class="sd">        Axis of ``node1`` where the edge is attached.</span>
<span class="sd">    node2 : AbstractNode, optional</span>
<span class="sd">        Second node to which the edge is connected. If ``None,`` the edge will</span>
<span class="sd">        be dangling.</span>
<span class="sd">    axis2 : int, str, Axis, optional</span>
<span class="sd">        Axis of ``node2`` where the edge is attached.</span>
<span class="sd">    </span>
<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; nodeA = tk.randn((2, 3))</span>
<span class="sd">    &gt;&gt;&gt; nodeB = tk.randn((3, 4))</span>
<span class="sd">    &gt;&gt;&gt; nodeA[1] ^ nodeB[0]</span>
<span class="sd">    &gt;&gt;&gt; nodeA[0]</span>
<span class="sd">    Edge( node_0[axis_0] &lt;-&gt; None )  (Dangling Edge)</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; nodeA[1]</span>
<span class="sd">    Edge( node_0[axis_1] &lt;-&gt; node_1[axis_0] )</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; nodeB[1]</span>
<span class="sd">    Edge( node_1[axis_1] &lt;-&gt; None )  (Dangling Edge)</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">node1</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span>
                 <span class="n">axis1</span><span class="p">:</span> <span class="n">Ax</span><span class="p">,</span>
                 <span class="n">node2</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AbstractNode</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">axis2</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Ax</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># check node1 and axis1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node1</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`node1` should be AbstractNode type&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis1</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Axis</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`axis1` should be int, str or Axis type&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis1</span><span class="p">,</span> <span class="n">Axis</span><span class="p">):</span>
            <span class="n">axis1</span> <span class="o">=</span> <span class="n">node1</span><span class="o">.</span><span class="n">get_axis</span><span class="p">(</span><span class="n">axis1</span><span class="p">)</span>

        <span class="c1"># check node2 and axis2</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">node2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="n">axis2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;`node2` and `axis2` must both be None or both not be None&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">node2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node2</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`node2` should be AbstractNode type&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis2</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Axis</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`axis2` should be int, str or Axis type&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis2</span><span class="p">,</span> <span class="n">Axis</span><span class="p">):</span>
                <span class="n">axis2</span> <span class="o">=</span> <span class="n">node2</span><span class="o">.</span><span class="n">get_axis</span><span class="p">(</span><span class="n">axis2</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">node1</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">axis1</span><span class="o">.</span><span class="n">_num</span><span class="p">]</span> <span class="o">!=</span> <span class="n">node2</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">axis2</span><span class="o">.</span><span class="n">_num</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Sizes of `axis1` and `axis2` should match&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">node2</span> <span class="o">==</span> <span class="n">node1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">axis2</span> <span class="o">==</span> <span class="n">axis1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Cannot connect the same axis of the same node to itself&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">node1</span><span class="p">,</span> <span class="n">node2</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">]</span>

    <span class="c1"># ----------</span>
    <span class="c1"># Properties</span>
    <span class="c1"># ----------</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">node1</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractNode</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns ``node1`` of the edge.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">node2</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractNode</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns ``node2`` of the edge. If the edge is dangling, it is ``None``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">AbstractNode</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a list with ``node1`` and ``node2``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">axis1</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Axis</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns axis where the edge is attached to ``node1``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">axis2</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Axis</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns axis where the edge is attached to ``node2``. If the edge is</span>
<span class="sd">        dangling, it is ``None``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Axis</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of axes where the edge is attached to ``node1`` and</span>
<span class="sd">        ``node2``, respectively.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns edge&#39;s name. It is formed with the corresponding nodes&#39; and axes&#39;</span>
<span class="sd">        names.</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 3),</span>
<span class="sd">        ...                 name=&#39;nodeA&#39;,</span>
<span class="sd">        ...                 axes_names=[&#39;left&#39;, &#39;right&#39;])</span>
<span class="sd">        &gt;&gt;&gt; edge = nodeA[&#39;right&#39;]</span>
<span class="sd">        &gt;&gt;&gt; print(edge.name)</span>
<span class="sd">        nodeA[right] &lt;-&gt; None</span>

<span class="sd">        &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 4),</span>
<span class="sd">        ...                 name=&#39;nodeB&#39;,</span>
<span class="sd">        ...                 axes_names=[&#39;left&#39;, &#39;right&#39;])</span>
<span class="sd">        &gt;&gt;&gt; new_edge = nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; print(new_edge.name)</span>
<span class="sd">        nodeA[right] &lt;-&gt; nodeB[left]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">():</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">node1</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">[</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">axis1</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">] &lt;-&gt; None&#39;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">node1</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">[</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">axis1</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">] &lt;-&gt; &#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">node2</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">[</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">axis2</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">]&#39;</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
<div class="viewcode-block" id="Edge.is_dangling"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.is_dangling">[docs]</a>    <span class="k">def</span> <span class="nf">is_dangling</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns boolean indicating whether the edge is a dangling edge.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node2</span> <span class="ow">is</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="Edge.is_batch"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.is_batch">[docs]</a>    <span class="k">def</span> <span class="nf">is_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns boolean indicating whether the edge is a batch edge.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis1</span><span class="o">.</span><span class="n">is_batch</span><span class="p">()</span></div>

<div class="viewcode-block" id="Edge.is_attached_to"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.is_attached_to">[docs]</a>    <span class="k">def</span> <span class="nf">is_attached_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns boolean indicating whether the edge is attached to ``node``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node1</span> <span class="o">==</span> <span class="n">node</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node2</span> <span class="o">==</span> <span class="n">node</span><span class="p">)</span></div>

<div class="viewcode-block" id="Edge.size"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.size">[docs]</a>    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns edge&#39;s size. Equivalent to node&#39;s shape in that axis.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_num</span><span class="p">]</span></div>

<div class="viewcode-block" id="Edge.change_size"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.change_size">[docs]</a>    <span class="k">def</span> <span class="nf">change_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Changes size of the edge, thus changing the size of tensors of ``node1``</span>
<span class="sd">        and ``node2`` at the corresponding axes. If new size is smaller, the</span>
<span class="sd">        tensor will be cropped; if larger, the tensor will be expanded with zeros.</span>
<span class="sd">        In both cases, the process (cropping/expanding) occurs at the &quot;left&quot;,</span>
<span class="sd">        &quot;top&quot;, &quot;front&quot;, etc. of each dimension.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        size : int</span>
<span class="sd">            New size of the edge.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.ones((3, 4))</span>
<span class="sd">        &gt;&gt;&gt; edge = nodeA[1] ^ nodeB[0]</span>
<span class="sd">        &gt;&gt;&gt; edge.size()</span>
<span class="sd">        3</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; edge.change_size(4)</span>
<span class="sd">        &gt;&gt;&gt; nodeA.tensor</span>
<span class="sd">        tensor([[0., 1., 1., 1.],</span>
<span class="sd">                [0., 1., 1., 1.]])</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeB.tensor</span>
<span class="sd">        tensor([[0., 0., 0., 0.],</span>
<span class="sd">                [1., 1., 1., 1.],</span>
<span class="sd">                [1., 1., 1., 1.],</span>
<span class="sd">                [1., 1., 1., 1.]])</span>
<span class="sd">                </span>
<span class="sd">        &gt;&gt;&gt; edge.size()</span>
<span class="sd">        4</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; edge.change_size(2)</span>
<span class="sd">        &gt;&gt;&gt; nodeA.tensor</span>
<span class="sd">        tensor([[1., 1.],</span>
<span class="sd">                [1., 1.]])</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeB.tensor</span>
<span class="sd">        tensor([[1., 1., 1., 1.],</span>
<span class="sd">                [1., 1., 1., 1.]])</span>
<span class="sd">                </span>
<span class="sd">        &gt;&gt;&gt; edge.size()</span>
<span class="sd">        2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`size` should be int type&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">node2</span><span class="o">.</span><span class="n">_change_axis_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis2</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node1</span><span class="o">.</span><span class="n">_change_axis_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis1</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>

<div class="viewcode-block" id="Edge.copy"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.copy">[docs]</a>    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a copy of the edge, that is, a new edge referencing the same</span>
<span class="sd">        nodes at the same axes.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.randn((3, 4))</span>
<span class="sd">        &gt;&gt;&gt; edge = nodeA[1] ^ nodeB[0]</span>
<span class="sd">        &gt;&gt;&gt; copy = edge.copy()</span>
<span class="sd">        &gt;&gt;&gt; copy != edge</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; copy.is_attached_to(nodeA)</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; copy.is_attached_to(nodeB)</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_edge</span> <span class="o">=</span> <span class="n">Edge</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">node1</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis1</span><span class="p">,</span>
                        <span class="n">node2</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">node2</span><span class="p">,</span> <span class="n">axis2</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_edge</span></div>

<div class="viewcode-block" id="Edge.connect"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.connect">[docs]</a>    <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s1">&#39;Edge&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Connects dangling edge to another dangling edge. It is necessary that</span>
<span class="sd">        both edges have the same size so that contractions along that edge can</span>
<span class="sd">        be computed.</span>
<span class="sd">        </span>
<span class="sd">        Note that this connectes edges from ``leaf`` (or ``data``, ``virtual``)</span>
<span class="sd">        nodes, but never from ``resultant`` nodes. If one tries to connect</span>
<span class="sd">        one of the inherited edges of a ``resultant`` node, the new connected</span>
<span class="sd">        edge will be attached to the original ``leaf` nodes from which the</span>
<span class="sd">        ``resultant`` node inherited its edges. Hence, the ``resultant`` node</span>
<span class="sd">        will not &quot;see&quot; the connection until the :class:`TensorNetwork` is</span>
<span class="sd">        :meth:`~TensorNetwork.reset`.</span>
<span class="sd">        </span>
<span class="sd">        If the nodes that are being connected come from different networks, the</span>
<span class="sd">        ``node2`` (and its connected component) will be moved to ``node1``&#39;s</span>
<span class="sd">        network. See also :meth:`~AbstractNode.move_to_network`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other : Edge</span>
<span class="sd">            The other edge to which current edge will be connected.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Edge</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        To connect two edges, the overloaded operator ``^`` can also be used.</span>

<span class="sd">        &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 3),</span>
<span class="sd">        ...                 name=&#39;nodeA&#39;,</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 4),</span>
<span class="sd">        ...                 name=&#39;nodeB&#39;,</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; new_edge = nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]  # Same as .connect()</span>
<span class="sd">        &gt;&gt;&gt; print(new_edge.name)</span>
<span class="sd">        nodeA[right] &lt;-&gt; nodeB[left]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>

<div class="viewcode-block" id="Edge.disconnect"><a class="viewcode-back" href="../../components.html#tensorkrowch.Edge.disconnect">[docs]</a>    <span class="k">def</span> <span class="nf">disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">,</span> <span class="s1">&#39;Edge&#39;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disconnects connected edge, that is, the connected edge is splitted into</span>
<span class="sd">        two dangling edges, one for each node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tuple[Edge, Edge]</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        To disconnect an edge, the overloaded operator ``|`` can also be used.</span>

<span class="sd">        &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 3),</span>
<span class="sd">        ...                 name=&#39;nodeA&#39;,</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 4),</span>
<span class="sd">        ...                 name=&#39;nodeB&#39;,</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">        &gt;&gt;&gt; new_edge = nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        &gt;&gt;&gt; new_edgeA, new_edgeB = new_edge | new_edge  # Same as .disconnect()</span>
<span class="sd">        &gt;&gt;&gt; print(new_edgeA.name)</span>
<span class="sd">        nodeA[right] &lt;-&gt; None</span>

<span class="sd">        &gt;&gt;&gt; print(new_edgeB.name)</span>
<span class="sd">        nodeB[left] &lt;-&gt; None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__xor__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s1">&#39;Edge&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Edge&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__or__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s1">&#39;Edge&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s1">&#39;Edge&#39;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">other</span> <span class="o">==</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">disconnect</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;Cannot disconnect one edge from another, different one. &#39;</span>
                <span class="s1">&#39;Edge should be disconnected from itself&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batch</span><span class="p">():</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">( </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1"> )  (Batch Edge)&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">():</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">( </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1"> )  (Dangling Edge)&#39;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">( </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1"> )&#39;</span></div>


<span class="c1">###############################################################################</span>
<span class="c1">#                                 STACK EDGES                                 #</span>
<span class="c1">###############################################################################</span>
<span class="n">AbstractStackNode</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">StackNode</span><span class="p">,</span> <span class="n">ParamStackNode</span><span class="p">]</span>


<div class="viewcode-block" id="StackEdge"><a class="viewcode-back" href="../../components.html#tensorkrowch.StackEdge">[docs]</a><span class="k">class</span> <span class="nc">StackEdge</span><span class="p">(</span><span class="n">Edge</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for stacked edges. They are just like :class:`Edges &lt;Edge&gt;` but used</span>
<span class="sd">    when stacking a collection of nodes into a :class:`StackNode`. When doing</span>
<span class="sd">    this, all edges of the stacked nodes must be kept, since they have the</span>
<span class="sd">    information regarding the nodes&#39; neighbours, which will be used when :func:</span>
<span class="sd">    `unbinding &lt;unbind&gt;` the stack.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    edges : list[Edge]</span>
<span class="sd">        List of edges (one from each node that is being stacked) that are</span>
<span class="sd">        attached to the equivalent of ``axis1`` in each node.</span>
<span class="sd">    node1_list : list[bool]</span>
<span class="sd">        List of ``axis1`` attributes (one from each node that is being stacked)</span>
<span class="sd">        of the equivalent of ``axis1`` in each node.</span>
<span class="sd">    node1 : StackNode or ParamStackNode</span>
<span class="sd">        First node to which the edge is connected.</span>
<span class="sd">    axis1: int, str or Axis</span>
<span class="sd">        Axis of ``node1`` where the edge is attached.</span>
<span class="sd">    node2 : StackNode or ParamStackNode, optional</span>
<span class="sd">        Second node to which the edge is connected. If ``None``, the edge will</span>
<span class="sd">        be dangling.</span>
<span class="sd">    axis2 : int, str, Axis, optional</span>
<span class="sd">        Axis of ``node2`` where the edge is attached.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">edges</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Edge</span><span class="p">],</span>
                 <span class="n">node1_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                 <span class="n">node1</span><span class="p">:</span> <span class="n">AbstractStackNode</span><span class="p">,</span>
                 <span class="n">axis1</span><span class="p">:</span> <span class="n">Axis</span><span class="p">,</span>
                 <span class="n">node2</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AbstractStackNode</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">axis2</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Axis</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span> <span class="o">=</span> <span class="n">edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_node1_list</span> <span class="o">=</span> <span class="n">node1_list</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="n">node1</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis1</span><span class="p">,</span>
                         <span class="n">node2</span><span class="o">=</span><span class="n">node2</span><span class="p">,</span> <span class="n">axis2</span><span class="o">=</span><span class="n">axis2</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edges</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Edge</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns list of stacked edges corresponding to this axis.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">node1_list</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns list of ``node1``&#39;s corresponding to this axis.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node1_list</span>

<div class="viewcode-block" id="StackEdge.connect"><a class="viewcode-back" href="../../components.html#tensorkrowch.StackEdge.connect">[docs]</a>    <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s1">&#39;StackEdge&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;StackEdge&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Same as :meth:`~Edge.connect` but it is first verified that all stacked</span>
<span class="sd">        :meth:`edges` corresponding to both ``StackEdges`` are the same.</span>
<span class="sd">        </span>
<span class="sd">        That is, this is a redundant operation to **re-connect** a list of edges</span>
<span class="sd">        that should be already connected. However, this is mandatory, since when</span>
<span class="sd">        stacking two sequences of nodes independently it cannot be inferred that</span>
<span class="sd">        the resultant ``StackNodes`` had to be connected.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other : StackEdge</span>
<span class="sd">            The other edge to which current edge will be connected.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        StackEdge</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        To connect two stack-edges, the overloaded operator ``^`` can also be</span>
<span class="sd">        used.</span>

<span class="sd">        &gt;&gt;&gt; net = tk.TensorNetwork()</span>
<span class="sd">        &gt;&gt;&gt; nodes = [tk.randn(shape=(2, 4, 2),</span>
<span class="sd">        ...                   axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                   network=net)</span>
<span class="sd">        ...          for _ in range(10)]</span>
<span class="sd">        &gt;&gt;&gt; data = [tk.randn(shape=(4,),</span>
<span class="sd">        ...                  axes_names=(&#39;feature&#39;,),</span>
<span class="sd">        ...                  network=net)</span>
<span class="sd">        ...         for _ in range(10)]</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; for i in range(10):</span>
<span class="sd">        ...     nodes[i][&#39;input&#39;] ^ data[i][&#39;feature&#39;]</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; stack_nodes = tk.stack(nodes)</span>
<span class="sd">        &gt;&gt;&gt; stack_data = tk.stack(data)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # It is necessary to re-connect stacks to be able to contract</span>
<span class="sd">        &gt;&gt;&gt; new_edge = stack_nodes[&#39;input&#39;] ^ stack_data[&#39;feature&#39;]</span>
<span class="sd">        &gt;&gt;&gt; print(new_edge.name)</span>
<span class="sd">        stack_0[input] &lt;-&gt; stack_1[feature]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">connect_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div></div>


<span class="c1">###############################################################################</span>
<span class="c1">#                               EDGE OPERATIONS                               #</span>
<span class="c1">###############################################################################</span>
<div class="viewcode-block" id="connect"><a class="viewcode-back" href="../../operations.html#tensorkrowch.connect">[docs]</a><span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="n">edge1</span><span class="p">:</span> <span class="n">Edge</span><span class="p">,</span> <span class="n">edge2</span><span class="p">:</span> <span class="n">Edge</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Edge</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Connects two dangling edges. It is necessary that both edges have the same</span>
<span class="sd">    size so that contractions along that edge can be computed.</span>
<span class="sd">    </span>
<span class="sd">    Note that this connectes edges from ``leaf`` (or ``data``, ``virtual``)</span>
<span class="sd">    nodes, but never from ``resultant`` nodes. If one tries to connect one of</span>
<span class="sd">    the inherited edges of a ``resultant`` node, the new connected edge will be</span>
<span class="sd">    attached to the original ``leaf` nodes from which the ``resultant`` node</span>
<span class="sd">    inherited its edges. Hence, the ``resultant`` node will not &quot;see&quot; the</span>
<span class="sd">    connection until the :class:`TensorNetwork` is :meth:`~TensorNetwork.reset`.</span>
<span class="sd">    </span>
<span class="sd">    If the nodes that are being connected come from different networks, the</span>
<span class="sd">    ``node2`` (and its connected component) will be movec to ``node1``&#39;s network.</span>
<span class="sd">    See also :meth:`~AbstractNode.move_to_network`.</span>

<span class="sd">    This operation is the same as :meth:`Edge.connect`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    edge1 : Edge</span>
<span class="sd">        The first edge that will be connected. Its node will become the ``node1``</span>
<span class="sd">        of the resultant edge.</span>
<span class="sd">    edge2 : Edge</span>
<span class="sd">        The second edge that will be connected. Its node will become the ``node2``</span>
<span class="sd">        of the resultant edge.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Edge</span>
<span class="sd">    </span>
<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 3),</span>
<span class="sd">    ...                 name=&#39;nodeA&#39;,</span>
<span class="sd">    ...                 axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 4),</span>
<span class="sd">    ...                 name=&#39;nodeB&#39;,</span>
<span class="sd">    ...                 axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; new_edge = tk.connect(nodeA[&#39;right&#39;], nodeB[&#39;left&#39;])</span>
<span class="sd">    &gt;&gt;&gt; print(new_edge.name)</span>
<span class="sd">    nodeA[right] &lt;-&gt; nodeB[left]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Case edge is already connected</span>
    <span class="k">if</span> <span class="n">edge1</span> <span class="o">==</span> <span class="n">edge2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">edge1</span>

    <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="p">[</span><span class="n">edge1</span><span class="p">,</span> <span class="n">edge2</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Edge </span><span class="si">{</span><span class="n">edge</span><span class="si">!s}</span><span class="s1"> is not a dangling edge. &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;This edge points to nodes: </span><span class="si">{</span><span class="n">edge</span><span class="o">.</span><span class="n">node1</span><span class="si">!s}</span><span class="s1"> and &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">edge</span><span class="o">.</span><span class="n">node2</span><span class="si">!s}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_batch</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Edge </span><span class="si">{</span><span class="n">edge</span><span class="si">!s}</span><span class="s1"> is a batch edge. Batch edges &#39;</span>
                             <span class="s1">&#39;cannot be connected&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">edge1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">edge2</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Cannot connect edges of unequal size. &#39;</span>
                         <span class="sa">f</span><span class="s1">&#39;Size of edge </span><span class="si">{</span><span class="n">edge1</span><span class="si">!s}</span><span class="s1">: </span><span class="si">{</span><span class="n">edge1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s1">. &#39;</span>
                         <span class="sa">f</span><span class="s1">&#39;Size of edge </span><span class="si">{</span><span class="n">edge2</span><span class="si">!s}</span><span class="s1">: </span><span class="si">{</span><span class="n">edge2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">node1</span><span class="p">,</span> <span class="n">axis1</span> <span class="o">=</span> <span class="n">edge1</span><span class="o">.</span><span class="n">node1</span><span class="p">,</span> <span class="n">edge1</span><span class="o">.</span><span class="n">axis1</span>
    <span class="n">node2</span><span class="p">,</span> <span class="n">axis2</span> <span class="o">=</span> <span class="n">edge2</span><span class="o">.</span><span class="n">node1</span><span class="p">,</span> <span class="n">edge2</span><span class="o">.</span><span class="n">axis1</span>
    <span class="n">net1</span><span class="p">,</span> <span class="n">net2</span> <span class="o">=</span> <span class="n">node1</span><span class="o">.</span><span class="n">_network</span><span class="p">,</span> <span class="n">node2</span><span class="o">.</span><span class="n">_network</span>

    <span class="k">if</span> <span class="n">net1</span> <span class="o">!=</span> <span class="n">net2</span><span class="p">:</span>
        <span class="n">node2</span><span class="o">.</span><span class="n">move_to_network</span><span class="p">(</span><span class="n">net1</span><span class="p">)</span>
    <span class="n">net1</span><span class="o">.</span><span class="n">_remove_edge</span><span class="p">(</span><span class="n">edge1</span><span class="p">)</span>
    <span class="n">net1</span><span class="o">.</span><span class="n">_remove_edge</span><span class="p">(</span><span class="n">edge2</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge1</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">):</span>
        <span class="n">new_edge</span> <span class="o">=</span> <span class="n">StackEdge</span><span class="p">(</span><span class="n">edges</span><span class="o">=</span><span class="n">edge1</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                             <span class="n">node1_list</span><span class="o">=</span><span class="n">edge1</span><span class="o">.</span><span class="n">_node1_list</span><span class="p">,</span>
                             <span class="n">node1</span><span class="o">=</span><span class="n">node1</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis1</span><span class="p">,</span>
                             <span class="n">node2</span><span class="o">=</span><span class="n">node2</span><span class="p">,</span> <span class="n">axis2</span><span class="o">=</span><span class="n">axis2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_edge</span> <span class="o">=</span> <span class="n">Edge</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="n">node1</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis1</span><span class="p">,</span>
                        <span class="n">node2</span><span class="o">=</span><span class="n">node2</span><span class="p">,</span> <span class="n">axis2</span><span class="o">=</span><span class="n">axis2</span><span class="p">)</span>

    <span class="n">node1</span><span class="o">.</span><span class="n">_add_edge</span><span class="p">(</span><span class="n">new_edge</span><span class="p">,</span> <span class="n">axis1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">node2</span><span class="o">.</span><span class="n">_add_edge</span><span class="p">(</span><span class="n">new_edge</span><span class="p">,</span> <span class="n">axis2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_edge</span></div>


<div class="viewcode-block" id="connect_stack"><a class="viewcode-back" href="../../operations.html#tensorkrowch.connect_stack">[docs]</a><span class="k">def</span> <span class="nf">connect_stack</span><span class="p">(</span><span class="n">edge1</span><span class="p">:</span> <span class="n">StackEdge</span><span class="p">,</span> <span class="n">edge2</span><span class="p">:</span> <span class="n">StackEdge</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">StackEdge</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as :func:`connect` but it is first verified that all stacked edges</span>
<span class="sd">    corresponding to both ``StackEdges`` are the same. That is, this is a</span>
<span class="sd">    redundant operation to **re-connect** a list of edges that should be already</span>
<span class="sd">    connected. However, this is mandatory, since when stacking two sequences of</span>
<span class="sd">    nodes independently it cannot be inferred that the resultant ``StackNodes``</span>
<span class="sd">    had to be connected.</span>

<span class="sd">    This operation is the same as :meth:`StackEdge.connect`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    edge1 : StackEdge</span>
<span class="sd">        The first edge that will be connected. Its node will become the ``node1``</span>
<span class="sd">        of the resultant edge.</span>
<span class="sd">    edge2 : StackEdge</span>
<span class="sd">        The second edge that will be connected. Its node will become the ``node2``</span>
<span class="sd">        of the resultant edge.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge1</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge2</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Both edges should be StackEdge</span><span class="se">\&#39;</span><span class="s1">s&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">edge1</span><span class="o">.</span><span class="n">_edges</span> <span class="o">!=</span> <span class="n">edge2</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Cannot connect stack edges whose lists of edges are &#39;</span>
                         <span class="s1">&#39;not the same. They will be the same when both lists &#39;</span>
                         <span class="s1">&#39;contain edges connecting the nodes that formed the &#39;</span>
                         <span class="s1">&#39;stack nodes.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">connect</span><span class="p">(</span><span class="n">edge1</span><span class="o">=</span><span class="n">edge1</span><span class="p">,</span> <span class="n">edge2</span><span class="o">=</span><span class="n">edge2</span><span class="p">)</span></div>


<div class="viewcode-block" id="disconnect"><a class="viewcode-back" href="../../operations.html#tensorkrowch.disconnect">[docs]</a><span class="k">def</span> <span class="nf">disconnect</span><span class="p">(</span><span class="n">edge</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Edge</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Edge</span><span class="p">,</span> <span class="n">Edge</span><span class="p">],</span>
                                                      <span class="n">Tuple</span><span class="p">[</span><span class="n">StackEdge</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Disconnects connected edge, that is, the connected edge is splitted into</span>
<span class="sd">    two dangling edges, one for each node.</span>

<span class="sd">    This operation is the same as :meth:`Edge.disconnect`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    edge : Edge or StackEdge</span>
<span class="sd">        Edge that is going to be disconnected (splitted in two).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tuple[Edge, Edge] or tuple[StackEdge, StackEdge]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Cannot disconnect a dangling edge&#39;</span><span class="p">)</span>

    <span class="c1"># This is to avoid disconnecting an edge when we are trying to disconnect a</span>
    <span class="c1"># copy of that edge, in which case `copy_edge` might connect `node1` and</span>
    <span class="c1"># `node2`, but none of them &quot;sees&quot; `copy_edge`, since they have `edge`</span>
    <span class="c1"># connecting them</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">edge</span><span class="o">.</span><span class="n">_axes</span><span class="p">,</span> <span class="n">edge</span><span class="o">.</span><span class="n">_nodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
            <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
            <span class="n">axes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>

    <span class="n">new_edges</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">nodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">):</span>
            <span class="n">new_edge</span> <span class="o">=</span> <span class="n">StackEdge</span><span class="p">(</span><span class="n">edges</span><span class="o">=</span><span class="n">edge</span><span class="o">.</span><span class="n">_edges</span><span class="p">,</span>
                                 <span class="n">node1_list</span><span class="o">=</span><span class="n">edge</span><span class="o">.</span><span class="n">_node1_list</span><span class="p">,</span>
                                 <span class="n">node1</span><span class="o">=</span><span class="n">node</span><span class="p">,</span>
                                 <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
            <span class="n">new_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_edge</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_edge</span> <span class="o">=</span> <span class="n">Edge</span><span class="p">(</span><span class="n">node1</span><span class="o">=</span><span class="n">node</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
            <span class="n">new_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_edge</span><span class="p">)</span>

            <span class="n">net</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_network</span>
            <span class="n">net</span><span class="o">.</span><span class="n">_add_edge</span><span class="p">(</span><span class="n">new_edge</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">new_edge</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">new_edges</span><span class="p">):</span>
        <span class="n">node</span><span class="o">.</span><span class="n">_add_edge</span><span class="p">(</span><span class="n">new_edge</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_edges</span><span class="p">)</span></div>


<span class="c1">###############################################################################</span>
<span class="c1">#                                   SUCCESSOR                                 #</span>
<span class="c1">###############################################################################</span>
<div class="viewcode-block" id="Successor"><a class="viewcode-back" href="../../components.html#tensorkrowch.Successor">[docs]</a><span class="k">class</span> <span class="nc">Successor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for successors. This is a sort of cache memory for :class:`Operations</span>
<span class="sd">    &lt;Operation&gt;` that have been already computed.</span>

<span class="sd">    For instance, when contracting two nodes, the result gives a new node that</span>
<span class="sd">    stores the tensor resultant from contracting both nodes&#39;s tensors. However,</span>
<span class="sd">    when training a :class:`TensorNetwork`, the tensors inside the nodes will</span>
<span class="sd">    change every epoch, but there is actually no need to create a new resultant</span>
<span class="sd">    node every time. Instead, it is more efficient to keep track of which node</span>
<span class="sd">    arose as the result of an operation, and simply change its tensor.</span>

<span class="sd">    Hence, a ``Successor`` is instantiated providing the arguments of the operation</span>
<span class="sd">    that gave rise to a resultant node, a reference to the resultant node itself,</span>
<span class="sd">    and some hints that might help accelerating the computations the next time</span>
<span class="sd">    the operation is performed.</span>
<span class="sd">    </span>
<span class="sd">    |</span>

<span class="sd">    These three properties can be accessed via ``successor.kwargs``,</span>
<span class="sd">    ``successor.child`` and ``successor.hints``.</span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    </span>
<span class="sd">    See the different :class:`operations &lt;Operation&gt;` to learn which resultant</span>
<span class="sd">    node keeps the ``Successor`` information.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    kwargs : dict[str, any]</span>
<span class="sd">        Dictionary with keyword arguments used to call an operation.</span>
<span class="sd">    child : AbstractNode or list[AbstractNode]</span>
<span class="sd">        The node or list of nodes that result from an operation.</span>
<span class="sd">    hints : dict[str, any], optional</span>
<span class="sd">        A dictionary of hints created the first time an operation is computed in</span>
<span class="sd">        order to save some computation in the next calls of the operation.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    When contracting two nodes, a ``Successor`` is created and added to the list</span>
<span class="sd">    of successors of the first node (left operand).</span>

<span class="sd">    &gt;&gt;&gt; nodeA = tk.randn(shape=(2, 3), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; nodeB = tk.randn(shape=(3, 4), axes_names=(&#39;left&#39;, &#39;right&#39;))</span>
<span class="sd">    &gt;&gt;&gt; nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # Contract nodes</span>
<span class="sd">    &gt;&gt;&gt; result = nodeA @ nodeB</span>
<span class="sd">    &gt;&gt;&gt; print(result.name)</span>
<span class="sd">    contract_edges</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; nodeA.successors[&#39;contract_edges&#39;][0].child == result</span>
<span class="sd">    True</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
                 <span class="n">child</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AbstractNode</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">AbstractNode</span><span class="p">]],</span>
                 <span class="n">hints</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">child</span> <span class="o">=</span> <span class="n">child</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hints</span> <span class="o">=</span> <span class="n">hints</span></div>


<span class="c1">###############################################################################</span>
<span class="c1">#                                TENSOR NETWORK                               #</span>
<span class="c1">###############################################################################</span>
<div class="viewcode-block" id="TensorNetwork"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork">[docs]</a><span class="k">class</span> <span class="nc">TensorNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for arbitrary Tensor Networks. Subclass of **PyTorch**</span>
<span class="sd">    ``torch.nn.Module``.</span>

<span class="sd">    Tensor Networks are the central objects of **TensorKrowch**. Basically,</span>
<span class="sd">    a tensor network is a graph with vertices (:class:`Nodes &lt;AbstractNode&gt;`)</span>
<span class="sd">    connected by :class:`Edges &lt;Edge&gt;`. In these models, nodes&#39; tensors will be</span>
<span class="sd">    trained so that the contraction of the whole network approximates a certain</span>
<span class="sd">    function. Hence, ``TensorNetwork``&#39;s are the **trainable objects** of</span>
<span class="sd">    **TensorKrowch**, very much like ``torch.nn.Module``&#39;s are the **trainable</span>
<span class="sd">    objects** of **PyTorch**.</span>
<span class="sd">    </span>
<span class="sd">    |</span>

<span class="sd">    Recall that the common way of defining models out of ``torch.nn.Module`` is</span>
<span class="sd">    by defining a subclass where the ``__init__`` and ``forward`` methods are</span>
<span class="sd">    overriden:</span>

<span class="sd">    * ``__init__``: Defines the model itself (its layers, attributes, etc.).</span>
<span class="sd">    </span>
<span class="sd">    * ``forward``: Defines the way the model operates, that is, how the different</span>
<span class="sd">      parts of the model might combine to get an output from a particular input.</span>


<span class="sd">    With ``TensorNetwork``, the workflow is similar, though there are other</span>
<span class="sd">    methods that should be overriden:</span>
<span class="sd">    </span>
<span class="sd">    * ``__init__``: Defines the graph of the tensor network and initializes the</span>
<span class="sd">      tensors of the nodes. See :class:`AbstractNode` and :class:`Edge` to learn</span>
<span class="sd">      how to create nodes and connect them.</span>
<span class="sd">      </span>
<span class="sd">    * ``set_data_nodes`` (optional): Creates the data nodes where the data</span>
<span class="sd">      tensor(s) will be placed. Usually, it will just select the edges to which</span>
<span class="sd">      the data nodes should be connected, and call the parent method. See</span>
<span class="sd">      :meth:`set_data_nodes` to learn good practices to override it. See also</span>
<span class="sd">      :meth:`add_data`.</span>
<span class="sd">      </span>
<span class="sd">    * ``add_data`` (optional): Adds new data tensors that will be stored in</span>
<span class="sd">      ``data`` nodes. Usually it will not be necessary to override this method,</span>
<span class="sd">      but if one wants to customize how data is set into the ``data`` nodes,</span>
<span class="sd">      :meth:`add_data` can be overriden.</span>
<span class="sd">      </span>
<span class="sd">    * ``contract``: Defines the contraction algorithm of the whole tensor network,</span>
<span class="sd">      thus returning a single node. Very much like ``forward`` this is the main</span>
<span class="sd">      method that describes how the components of the network are combined.</span>
<span class="sd">      Hence, in ``TensorNetwork`` the :meth:`forward` method shall not be</span>
<span class="sd">      overriden, since it will just call :meth:`set_data_nodes`, if needed,</span>
<span class="sd">      and :meth:`contract`.</span>
<span class="sd">      </span>
<span class="sd">    |</span>

<span class="sd">    Although one can define how the network is going to be contracted, there a</span>
<span class="sd">    couple of modes that can change how this contraction behaves at a lower level:</span>

<span class="sd">    * ``auto_stack`` (``False`` by default): This mode indicates whether the</span>
<span class="sd">      operation :func:`stack` can take control of the memory management of the</span>
<span class="sd">      network to skip some steps in future computations. If ``auto_stack`` is</span>
<span class="sd">      set to ``True`` and a collection of :class:`ParamNodes &lt;ParamNode&gt;` are</span>
<span class="sd">      :func:`stacked &lt;stack&gt;` (as the first operation in which these nodes are</span>
<span class="sd">      involved), then those nodes will no longer store their own tensors,</span>
<span class="sd">      but rather a ``virtual`` :class:`ParamStackNode` will store the stacked</span>
<span class="sd">      tensor, avoiding the computation of that first :func:`stack` in every</span>
<span class="sd">      contraction. This behaviour is not possible if ``auto_stack`` is set to</span>
<span class="sd">      ``False``, in which case all nodes will always store their own tensors.</span>
<span class="sd">      </span>
<span class="sd">      Setting ``auto_stack`` to ``True`` will be faster for both **inference**</span>
<span class="sd">      and **training**. However, while experimenting with ``TensorNetwork``&#39;s</span>
<span class="sd">      one might want that all nodes store their own tensors to avoid problems.</span>

<span class="sd">    * ``auto_unbind`` (``False`` by default): This mode indicates whether the</span>
<span class="sd">      operation :func:`unbind` has to actually `unbind` the stacked tensor or</span>
<span class="sd">      just generate a collection of references. That is, if ``auto_unbind`` is</span>
<span class="sd">      set to ``False``, :func:`unbind` creates a collection of nodes, each of</span>
<span class="sd">      them storing the corresponding slice of the stacked tensor. If</span>
<span class="sd">      ``auto_unbind`` is set to ``True``, :func:`unbind` just creates the nodes</span>
<span class="sd">      and gives each of them an index to reference the stacked tensor, so that</span>
<span class="sd">      each node&#39;s tensor would be retrieved by indexing the stack. This avoids</span>
<span class="sd">      performing the operation, since these indices will be the same in</span>
<span class="sd">      subsequent iterations.</span>
<span class="sd">      </span>
<span class="sd">      Setting ``auto_unbind`` to ``True`` will be faster for **inference**, but</span>
<span class="sd">      slower for **training**.</span>

<span class="sd">    Once the training algorithm starts, these modes should not be changed (very</span>
<span class="sd">    often at least), since changing them entails first :meth:`resetting &lt;reset&gt;`</span>
<span class="sd">    the whole network, which is a costly method.</span>
<span class="sd">    </span>
<span class="sd">    |</span>

<span class="sd">    When the ``TensorNetwork`` is defined, it has a bunch of ``leaf``, ``data``</span>
<span class="sd">    and ``virtual`` nodes that make up the network structure, each of them</span>
<span class="sd">    storing its own tensor. However, when the network is contracted, several</span>
<span class="sd">    ``resultant`` nodes become new members of the network, even modifying its</span>
<span class="sd">    memory (depending on the ``auto_stack`` and ``auto_unbind`` modes).</span>
<span class="sd">    </span>
<span class="sd">    Therefore, if one wants to :meth:`reset` the network to its initial state</span>
<span class="sd">    after performing some operations, all the ``resultant`` nodes should be</span>
<span class="sd">    deleted, and all the tensors should return to its nodes (each node stores</span>
<span class="sd">    its own tensor). This is exactly what :meth:`reset` does. Besides, since</span>
<span class="sd">    ``auto_stack`` and ``auto_unbind`` can change how the tensors are stored,</span>
<span class="sd">    if one wants to change these modes, the network should be first reset (this</span>
<span class="sd">    is already done automatically when changing the modes).</span>
<span class="sd">    </span>
<span class="sd">    See :class:`AbstractNode` to learn about the **4 excluding types** of nodes,</span>
<span class="sd">    and :meth:`reset` to learn about how these nodes are treated differently.</span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    </span>
<span class="sd">    There are also some special nodes that one should take into account. These</span>
<span class="sd">    are specified by name. See :class:`AbstractNode` to learn about</span>
<span class="sd">    **reserved nodes&#39; names**, and :meth:`reset` to learn about how these</span>
<span class="sd">    nodes are treated differently.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        Network&#39;s name. By default, it is the name of the class</span>
<span class="sd">        (e.g. ``&quot;tensornetwork&quot;``).</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    |</span>
<span class="sd">    </span>
<span class="sd">    # TODO: add reference to example in other file</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">operations</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># References to the Operations defined for nodes</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Text</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

        <span class="c1"># Types of nodes of the TN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leaf_nodes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="c1"># Repeated nodes to keep track of enumeration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="c1"># Edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Memories</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>    <span class="c1"># address -&gt; memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inverse_memory</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># address -&gt; nodes using that memory</span>

        <span class="c1"># TN modes</span>
        <span class="c1"># Auto-management of memory mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_auto_stack</span> <span class="o">=</span> <span class="kc">False</span>   <span class="c1"># train -&gt; True / eval -&gt; True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_auto_unbind</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># train -&gt; False / eval -&gt; True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tracing</span> <span class="o">=</span> <span class="kc">False</span>      <span class="c1"># Tracing mode (True while calling .trace())</span>

        <span class="c1"># Lis of operations used to contract the TN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_seq_ops</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># ----------</span>
    <span class="c1"># Properties</span>
    <span class="c1"># ----------</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns dictionary with all the nodes belonging to the network (``leaf``,</span>
<span class="sd">        ``data``, ``virtual`` and ``resultant``).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_nodes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">all_nodes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_leaf_nodes</span><span class="p">)</span>
        <span class="n">all_nodes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="p">)</span>
        <span class="n">all_nodes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="p">)</span>
        <span class="n">all_nodes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_nodes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">nodes_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Text</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns list of names of all the nodes belonging to the network (``leaf``,</span>
<span class="sd">        ``data``, ``virtual`` and ``resultant``).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_nodes_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">al_nodes_names</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_leaf_nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">al_nodes_names</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">al_nodes_names</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">al_nodes_names</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">all_nodes_names</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">leaf_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns dictionary of ``leaf`` nodes of the network.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leaf_nodes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns dictionary of ``data`` nodes of the network.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">virtual_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns dictionary of ``virtual`` nodes of the network.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">resultant_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns dictionary of ``resultant`` nodes of the network.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edges</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Edge</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns list of dangling, non-batch edges of the network. Dangling</span>
<span class="sd">        edges from ``virtual`` nodes are not included.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">auto_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns boolean indicating whether ``auto_stack`` mode is active. By</span>
<span class="sd">        default it is ``False``.</span>
<span class="sd">        </span>
<span class="sd">        This mode indicates whether the operation :func:`stack` can take control</span>
<span class="sd">        of the memory management of the network to skip some steps in future</span>
<span class="sd">        computations. If ``auto_stack`` is set to ``True`` and a collection of</span>
<span class="sd">        :class:`ParamNodes &lt;ParamNode&gt;` are :func:`stacked &lt;stack&gt;` (as the</span>
<span class="sd">        first operation in which these nodes are involved), then those nodes</span>
<span class="sd">        will no longer store their own tensors, but rather a ``virtual``</span>
<span class="sd">        :class:`ParamStackNode` will store the stacked tensor, avoiding the</span>
<span class="sd">        computation of that first :func:`stack` in every contraction. This</span>
<span class="sd">        behaviour is not possible if ``auto_stack`` is set to ``False``, in</span>
<span class="sd">        which case all nodes will always store their own tensors.</span>
<span class="sd">        </span>
<span class="sd">        Setting ``auto_stack`` to ``True`` will be faster for both **inference**</span>
<span class="sd">        and **training**. However, while experimenting with ``TensorNetwork``&#39;s</span>
<span class="sd">        one might want that all nodes store their own tensors to avoid problems.</span>
<span class="sd">        </span>
<span class="sd">        Be aware that changing ``auto_stack`` mode entails :meth:`resetting &lt;reset&gt;`</span>
<span class="sd">        the network, which will modify its nodes. This has to be done manually</span>
<span class="sd">        in order to avoid undesired behaviour.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_stack</span>

    <span class="nd">@auto_stack</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">auto_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_mode</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">set_mode</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_stack</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_auto_stack</span> <span class="o">=</span> <span class="n">set_mode</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">auto_unbind</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns boolean indicating whether ``auto_unbind`` mode is active. By</span>
<span class="sd">        default it is ``False``.</span>
<span class="sd">        </span>
<span class="sd">        This mode indicates whether the operation :func:`unbind` has to actually</span>
<span class="sd">        `unbind` the stacked tensor or just generate a collection of references.</span>
<span class="sd">        That is, if ``auto_unbind`` is set to ``False``, :func:`unbind` creates</span>
<span class="sd">        a collection of nodes, each of them storing the corresponding slice of</span>
<span class="sd">        the stacked tensor. If ``auto_unbind`` is set to ``True``, :func:`unbind`</span>
<span class="sd">        just creates the nodes and gives each of them an index to reference the</span>
<span class="sd">        stacked tensor, so that each node&#39;s tensor would be retrieved by indexing</span>
<span class="sd">        the stack. This avoids performing the operation, since these indices</span>
<span class="sd">        will be the same in subsequent iterations.</span>
<span class="sd">      </span>
<span class="sd">        Setting ``auto_unbind`` to ``True`` will be faster for **inference**, but</span>
<span class="sd">        slower for **training**.</span>
<span class="sd">        </span>
<span class="sd">        Be aware that changing ``auto_unbind`` mode entails :meth:`resetting</span>
<span class="sd">        &lt;reset&gt;` the network, which will modify its nodes. Thus, this mode has</span>
<span class="sd">        to be changed manually in order to avoid undesired behaviour.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_unbind</span>

    <span class="nd">@auto_unbind</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">auto_unbind</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_mode</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">set_mode</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_unbind</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_auto_unbind</span> <span class="o">=</span> <span class="n">set_mode</span>

    <span class="c1"># -------</span>
    <span class="c1"># Methods</span>
    <span class="c1"># -------</span>
    <span class="k">def</span> <span class="nf">_which_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the corresponding dict, depending on the type of the node.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_leaf</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leaf_nodes</span>
        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">_data</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span>
        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">_virtual</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span>

    <span class="k">def</span> <span class="nf">_add_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">edge</span><span class="p">:</span> <span class="n">Edge</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds an edge to the network. If it is a ``StackEdge``, it is not added,</span>
<span class="sd">        since those edges are a sort of `virtual` edges used for ``resultant``</span>
<span class="sd">        ``StackNodes``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        edge : Edge</span>
<span class="sd">            Edge to be added.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># StackEdges are ignores since these are edges for ``resultant`` nodes</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_batch</span><span class="p">()</span> <span class="ow">and</span> \
                    <span class="p">(</span><span class="n">edge</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_remove_edge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">edge</span><span class="p">:</span> <span class="n">Edge</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Removes an edge from the network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        edge : Edge</span>
<span class="sd">            Edge to be removed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="n">StackEdge</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_dangling</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_batch</span><span class="p">()</span> <span class="ow">and</span> \
                    <span class="p">(</span><span class="n">edge</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_add_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span> <span class="n">override</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds a node to the network. If it is a :class:`ParamNode`, its tensor</span>
<span class="sd">        (``torch.nn.Parameter``) becomes a parameter of the network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node : Node or ParamNode</span>
<span class="sd">            Node to be added to the network.</span>
<span class="sd">        override : bool</span>
<span class="sd">            Boolean indicating whether ``node`` should override an existing node</span>
<span class="sd">            of the network that has the same name (e.g. when parameterizing a </span>
<span class="sd">            node, the param-node overrides the original node).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">override</span><span class="p">:</span>
            <span class="n">prev_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_remove_node</span><span class="p">(</span><span class="n">prev_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assign_node_name</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_remove_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span> <span class="n">move_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Removes a node from the network. It just removes the reference to the</span>
<span class="sd">        node from the network, as well as the reference to the network that is</span>
<span class="sd">        kept by the node. To completely get rid of the node, it should be first</span>
<span class="sd">        disconnected from all its neighbours (that belong to the network) and</span>
<span class="sd">        then removed. This is what :meth:`delete_node` does.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node : AbstractNode</span>
<span class="sd">            Node to be removed.</span>
<span class="sd">        move_names : bool</span>
<span class="sd">            Boolean indicating whether names&#39; enumerations should be decreased</span>
<span class="sd">            when removing a node (``True``) or kept as they are (``False``).</span>
<span class="sd">            This is useful when several nodes are being modified at once, and</span>
<span class="sd">            each resultant node has the same enumeration as the corresponding</span>
<span class="sd">            original node.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">tensor</span>
        <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">node</span><span class="o">.</span><span class="n">_network</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_unassign_node_name</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">move_names</span><span class="p">)</span>

        <span class="n">nodes_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_which_dict</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span> <span class="ow">in</span> <span class="n">nodes_dict</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">nodes_dict</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span> <span class="o">==</span> <span class="n">node</span><span class="p">:</span>
                <span class="c1"># If we remove &quot;node_0&quot; from [&quot;node_0&quot;, &quot;node_1&quot;, &quot;node_2&quot;],</span>
                <span class="c1"># &quot;node_1&quot; will become &quot;node_0&quot; when unassigning &quot;node_0&quot; name.</span>
                <span class="c1"># Hence, it can happen that nodes_dict[node._name] != node</span>
                <span class="k">del</span> <span class="n">nodes_dict</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">:</span>
                    <span class="c1"># It can happen that name is not in memory_nodes, when the</span>
                    <span class="c1"># node is using the memory of another node</span>
                    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span>

<div class="viewcode-block" id="TensorNetwork.delete_node"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.delete_node">[docs]</a>    <span class="k">def</span> <span class="nf">delete_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span> <span class="n">move_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disconnects node from all its neighbours and removes it from the network.</span>
<span class="sd">        To completely get rid of the node, do not forget to delete it:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; del node</span>
<span class="sd">            </span>
<span class="sd">        or override it:</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; node = node.copy()  # .copy() calls to .delete_node()</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node : Node or ParamNode</span>
<span class="sd">            Node to be deleted.</span>
<span class="sd">        move_names : bool</span>
<span class="sd">            Boolean indicating whether names&#39; enumerations should be decreased</span>
<span class="sd">            when removing a node (``True``) or kept as they are (``False``).</span>
<span class="sd">            This is useful when several nodes are being modified at once, and</span>
<span class="sd">            each resultant node has the same enumeration as the corresponding</span>
<span class="sd">            original node.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.randn((3, 4))</span>
<span class="sd">        &gt;&gt;&gt; nodeA[1] ^ nodeB[0]</span>
<span class="sd">        &gt;&gt;&gt; print(nodeA.name, nodeB.name)</span>
<span class="sd">        node_0 node_1</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeB.network.delete_node(nodeB)</span>
<span class="sd">        &gt;&gt;&gt; nodeA.neighbours() == []</span>
<span class="sd">        True</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; print(nodeA.name)</span>
<span class="sd">        node</span>
<span class="sd">        </span>
<span class="sd">        If ``move_names`` is set to ``False``, enumeration is not removed.</span>
<span class="sd">        Useful to avoid managing enumeration of a list of nodes that are all</span>
<span class="sd">        going to be deleted.</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.randn((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.randn((3, 4))</span>
<span class="sd">        &gt;&gt;&gt; nodeA[1] ^ nodeB[0]</span>
<span class="sd">        &gt;&gt;&gt; nodeB.network.delete_node(nodeB, False)</span>
<span class="sd">        &gt;&gt;&gt; print(nodeA.name)</span>
<span class="sd">        node_0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">node</span><span class="o">.</span><span class="n">disconnect</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_remove_node</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">move_names</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">node</span></div>

    <span class="k">def</span> <span class="nf">_update_node_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span> <span class="n">new_name</span><span class="p">:</span> <span class="n">Text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates a single node&#39;s ``tensor_info`` &quot;address&quot; and its corresponding</span>
<span class="sd">        address in ``memory_nodes`` and ``inverse_memory``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prev_name</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span>
        <span class="n">nodes_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_which_dict</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="c1"># For instance, when changing the name of node &quot;node_0&quot;, its name</span>
        <span class="c1"># will be first unassigned, thus moving the node &quot;node_1&quot; to &quot;node_0&quot;</span>
        <span class="c1"># (which would be ``new_name``, and is already in ``nodes_dict``)</span>
        <span class="k">if</span> <span class="n">new_name</span> <span class="ow">in</span> <span class="n">nodes_dict</span><span class="p">:</span>
            <span class="n">aux_node</span> <span class="o">=</span> <span class="n">nodes_dict</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span>
            <span class="n">aux_node</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="n">aux_node</span><span class="o">.</span><span class="n">tensor</span>

        <span class="k">if</span> <span class="n">nodes_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prev_name</span><span class="p">)</span> <span class="o">==</span> <span class="n">node</span><span class="p">:</span>
            <span class="n">nodes_dict</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nodes_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">prev_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
                    <span class="n">prev_name</span><span class="p">)</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_name</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tracing</span> <span class="ow">and</span> <span class="p">(</span><span class="n">prev_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inverse_memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
                        <span class="n">prev_name</span><span class="p">)</span>

        <span class="c1"># This would be the case in the example above, where the original</span>
        <span class="c1"># &quot;node_0&quot; will be replaced by the original &quot;node_1&quot;, and hence the</span>
        <span class="c1"># &quot;node_0&quot; in ``nodes_dict`` will be the original &quot;node_1&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nodes_dict</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_name</span>

    <span class="k">def</span> <span class="nf">_update_node_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span> <span class="n">new_name</span><span class="p">:</span> <span class="n">Text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updates a single node&#39;s name, without taking care of the other names.&quot;&quot;&quot;</span>
        <span class="c1"># Node is ParamNode and tensor is not None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;param&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_attached_to</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_remove_edge</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_update_node_info</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">new_name</span><span class="p">)</span>
        <span class="n">node</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">new_name</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;param&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">])):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span>
                    <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;param&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Nodes names are never repeated, so it is likely that</span>
                <span class="c1"># this case will never occur</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Network already has attribute named </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_add_edge</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_assign_node_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span>
                          <span class="n">name</span><span class="p">:</span> <span class="n">Text</span><span class="p">,</span>
                          <span class="n">first_time</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Assigns a new name to a node in the network. If the node was not previously</span>
<span class="sd">        in the network, a new ``tensor_info`` dict is created and the node&#39;s tensor</span>
<span class="sd">        is stored in the network&#39;s memory (``memory_nodes``). If the node was</span>
<span class="sd">        in the network (case its name is being changed), the ``tensor_info`` dict</span>
<span class="sd">        gets updated.</span>

<span class="sd">        In case there are already nodes with the same name in the network, an</span>
<span class="sd">        enumeration is added to the new node&#39;s name (and to the network&#39;s node</span>
<span class="sd">        in case there was only one node with the same name).</span>

<span class="sd">        * **New node&#39;s name**: &quot;my_node&quot;</span>
<span class="sd">          **Network&#39;s nodes&#39; names**: [&quot;my_node_0&quot;, &quot;my_node_1&quot;]</span>
<span class="sd">          **Result**: [&quot;my_node_0&quot;, &quot;my_node_1&quot;, &quot;my_node_2&quot;(new node)]</span>

<span class="sd">        * **New node&#39;s name**: &quot;my_node&quot;</span>
<span class="sd">          **Network&#39;s nodes&#39; names**: [&quot;my_node&quot;]</span>
<span class="sd">          **Result**: [&quot;my_node_0&quot;, &quot;my_node_1&quot;(new node)]</span>

<span class="sd">        Also, if the new node&#39;s name already had an enumeration, it will be removed.</span>

<span class="sd">        * **New node&#39;s name**: &quot;my_node_0&quot; -&gt; &quot;my_node&quot; (then apply other rules)</span>
<span class="sd">          **Network&#39;s nodes&#39; names**: [&quot;my_node&quot;]</span>
<span class="sd">          **Result**: [&quot;my_node_0&quot;, &quot;my_node_1&quot;(new node)]</span>

<span class="sd">        To add a custom enumeration to keep track of the nodes of the network</span>
<span class="sd">        in a user-defined way, one may use brackets or parenthesis.</span>

<span class="sd">        * **New node&#39;s name**: &quot;my_node_(0)&quot;</span>
<span class="sd">          **Network&#39;s nodes&#39; names**: [&quot;my_node&quot;]</span>
<span class="sd">          **Result**: [&quot;my_node&quot;, &quot;my_node_(0)&quot;(new node)]</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node : Node or ParamNode</span>
<span class="sd">            Node whose name will be assigned in the network.</span>
<span class="sd">        name : str</span>
<span class="sd">            Node&#39;s name. If it coincides with a name that already exists in the</span>
<span class="sd">            network, the rules explained before will modify the name and assign</span>
<span class="sd">            this version as the node&#39;s name.</span>
<span class="sd">        first_time : bool</span>
<span class="sd">            Boolean indicating whether it is the first time a name is assigned</span>
<span class="sd">            to ``node``. In this case (``True``), a new ``tensor_info`` dict is</span>
<span class="sd">            created for the node.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">non_enum_prev_name</span> <span class="o">=</span> <span class="n">erase_enum</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">node</span><span class="o">.</span><span class="n">is_resultant</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">non_enum_prev_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">operations</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assign_node_name</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Node</span><span class="se">\&#39;</span><span class="s1">s name cannot be an operation name: &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">operations</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">non_enum_prev_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span><span class="p">[</span><span class="n">non_enum_prev_name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">aux_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">non_enum_prev_name</span><span class="p">]</span>
                <span class="n">aux_new_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">non_enum_prev_name</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_update_node_name</span><span class="p">(</span><span class="n">aux_node</span><span class="p">,</span> <span class="n">aux_new_name</span><span class="p">)</span>
            <span class="n">new_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">non_enum_prev_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">count</span><span class="p">)])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_name</span> <span class="o">=</span> <span class="n">non_enum_prev_name</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span><span class="p">[</span><span class="n">non_enum_prev_name</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span><span class="p">[</span><span class="n">non_enum_prev_name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Since node name might change, edges should be removed and</span>
        <span class="c1"># added later, so that their names as sub-modules are correct</span>
        <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_attached_to</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_remove_edge</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">first_time</span><span class="p">:</span>
            <span class="n">nodes_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_which_dict</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
            <span class="n">nodes_dict</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">new_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;address&#39;</span><span class="p">:</span> <span class="n">new_name</span><span class="p">,</span>
                                 <span class="s1">&#39;node_ref&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="s1">&#39;full&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                                 <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_network</span> <span class="o">=</span> <span class="bp">self</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">new_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_node_info</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">new_name</span><span class="p">)</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">new_name</span>

        <span class="c1"># Node is ParamNode and tensor is not None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;param&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">])):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span>
                    <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;param&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Nodes names are never repeated, so it is likely that</span>
                <span class="c1"># this case will never occur</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Network already has attribute named </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_add_edge</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_unassign_node_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span> <span class="n">move_names</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Unassigns node&#39;s name from the network, thus deleting its tensor as</span>
<span class="sd">        a parameter of the network (if node is a ``ParamNode``), and removing</span>
<span class="sd">        its edges. Also, if ``move_names`` is set to ``True``, the enumeration</span>
<span class="sd">        of all the nodes in the network that have the same name as the given</span>
<span class="sd">        node will be decreased by one. If only one node remains, the enumeration</span>
<span class="sd">        will be removed.</span>

<span class="sd">        * **Node&#39;s name**: &quot;my_node_1&quot;</span>
<span class="sd">          **Network&#39;s nodes&#39; names**: [&quot;my_node_0&quot;, &quot;my_node_1&quot;(node), &quot;my_node_2&quot;]</span>
<span class="sd">          **Result**: [&quot;my_node_0&quot;, &quot;my_node_1&quot;]</span>

<span class="sd">        * **Node&#39;s name**: &quot;my_node_1&quot;</span>
<span class="sd">          **Network&#39;s nodes&#39; names**: [&quot;my_node_0&quot;, &quot;my_node_1&quot;(node)]</span>
<span class="sd">          **Result**: [&quot;my_node&quot;]</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node : Node or ParamNode</span>
<span class="sd">            Node whose name will be unassigned in the network.</span>
<span class="sd">        move_names : bool</span>
<span class="sd">            Boolean indicating whether names&#39; enumerations should be decreased</span>
<span class="sd">            when removing a node (``True``) or kept as they are (``False``).</span>
<span class="sd">            This is useful when several nodes are being modified at once, and</span>
<span class="sd">            each resultant node has the same enumeration as the corresponding</span>
<span class="sd">            original node.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Node is ParamNode and tensor is not None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;param&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">edge</span><span class="o">.</span><span class="n">is_attached_to</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_remove_edge</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

        <span class="n">non_enum_prev_name</span> <span class="o">=</span> <span class="n">erase_enum</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span><span class="p">[</span><span class="n">non_enum_prev_name</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">move_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">enum</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">enum</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
                    <span class="n">aux_prev_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">non_enum_prev_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                    <span class="n">aux_new_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">non_enum_prev_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>
                    <span class="n">aux_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">aux_prev_name</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_update_node_name</span><span class="p">(</span><span class="n">aux_node</span><span class="p">,</span> <span class="n">aux_new_name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span><span class="p">[</span><span class="n">non_enum_prev_name</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="n">count</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeated_nodes_names</span><span class="p">[</span><span class="n">non_enum_prev_name</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">move_names</span><span class="p">:</span>
                <span class="n">aux_prev_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">non_enum_prev_name</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">])</span>
                <span class="n">aux_new_name</span> <span class="o">=</span> <span class="n">non_enum_prev_name</span>
                <span class="n">aux_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">aux_prev_name</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_update_node_name</span><span class="p">(</span><span class="n">aux_node</span><span class="p">,</span> <span class="n">aux_new_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_change_node_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">AbstractNode</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">Text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Changes the name of a node in the network. To take care of all the other</span>
<span class="sd">        names this change might affect, it calls :meth:`TensorNetwork._unassign_node_name`</span>
<span class="sd">        and :meth:`TensorNetwork._assign_node_name`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_network</span> <span class="o">!=</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Cannot change the name of a node that does &#39;</span>
                             <span class="s1">&#39;not belong to the network&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">erase_enum</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">!=</span> <span class="n">erase_enum</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_unassign_node_name</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assign_node_name</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

<div class="viewcode-block" id="TensorNetwork.copy"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.copy">[docs]</a>    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;TensorNetwork&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Copies the tensor network (via ``copy.deepcopy``).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorNetwork.parameterize"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.parameterize">[docs]</a>    <span class="k">def</span> <span class="nf">parameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">set_param</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="n">override</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;TensorNetwork&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameterizes all ``leaf`` nodes of the network. If there are</span>
<span class="sd">        ``resultant`` nodes in the :class:`TensorNetwork`, it will be first</span>
<span class="sd">        :meth:`reset`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        set_param : bool</span>
<span class="sd">            Boolean indicating whether the tensor network has to be parameterized</span>
<span class="sd">            (``True``) or de-parameterized (``False``).</span>
<span class="sd">        override : bool</span>
<span class="sd">            Boolean indicating whether the tensor network should be parameterized</span>
<span class="sd">            in-place (``True``) or copied and then parameterized (``False``).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Resultant nodes will be removed before parameterizing&#39;</span>
                          <span class="s1">&#39; the TN&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            
        <span class="k">if</span> <span class="n">override</span><span class="p">:</span>
            <span class="n">net</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">_leaf_nodes</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="n">node</span><span class="o">.</span><span class="n">parameterize</span><span class="p">(</span><span class="n">set_param</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">net</span></div>

<div class="viewcode-block" id="TensorNetwork.set_data_nodes"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.set_data_nodes">[docs]</a>    <span class="k">def</span> <span class="nf">set_data_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">input_edges</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Edge</span><span class="p">]],</span>
                       <span class="n">num_batch_edges</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates ``data`` nodes with as many batch edges as ``num_batch_edges``</span>
<span class="sd">        and one feature edge. Then it connects each of these nodes&#39; feature</span>
<span class="sd">        edges to an edge from the list ``input_edges`` (following the provided</span>
<span class="sd">        order). Thus, edges in ``input_edges`` need to be dangling. Also, if</span>
<span class="sd">        there are already ``data`` nodes (or the ``&quot;stack_data_memory&quot;``) in</span>
<span class="sd">        the network, they should be :meth:`unset` first.</span>

<span class="sd">        If all the ``data`` nodes have the same shape, a ``virtual`` node will</span>
<span class="sd">        contain all the tensors stacked in one, what will save some memory</span>
<span class="sd">        and time in computations. This node is ``&quot;stack_data_memory&quot;``. See</span>
<span class="sd">        :class:`AbstractNode` to learn more about this node.</span>
<span class="sd">        </span>
<span class="sd">        If this method is overriden in subclasses, it can be done in two</span>
<span class="sd">        flavours:</span>
<span class="sd">        </span>
<span class="sd">        ::</span>
<span class="sd">        </span>
<span class="sd">            def set_data_nodes(self):</span>
<span class="sd">                # Collect input edges</span>
<span class="sd">                input_edges = [node_1[i], ..., node_n[j]]</span>
<span class="sd">                </span>
<span class="sd">                # Define number of batches</span>
<span class="sd">                num_batch_edges = m</span>
<span class="sd">                </span>
<span class="sd">                # Call parent method</span>
<span class="sd">                super().set_data_nodes(input, edges, num_batch_edges)</span>
<span class="sd">                </span>
<span class="sd">        ::</span>
<span class="sd">        </span>
<span class="sd">            def set_data_nodes(self):</span>
<span class="sd">                # Create data nodes directly</span>
<span class="sd">                data_nodes = [</span>
<span class="sd">                    tk.Node(shape=(batch_1, ..., batch_m, feature_dim),</span>
<span class="sd">                                   axes_names=(&#39;batch_1&#39;, ..., &#39;batch_m&#39;, &#39;feature&#39;)</span>
<span class="sd">                                   network=self,</span>
<span class="sd">                                   data=True)</span>
<span class="sd">                    for _ in range(n)]</span>
<span class="sd">                    </span>
<span class="sd">                # Connect them with the leaf nodes</span>
<span class="sd">                for i, data_node in enumerate(data_nodes):</span>
<span class="sd">                    data_node[&#39;feature&#39;] ^ self.my_nodes[i][&#39;input&#39;]</span>
<span class="sd">                    </span>
<span class="sd">        If this method is overriden, there is no need to call it explicitly</span>
<span class="sd">        during training, since it will be done in the :meth:`forward` call.</span>
<span class="sd">        </span>
<span class="sd">        On the other hand, if one does not override ``set_data_nodes``, it</span>
<span class="sd">        should be called before starting training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_edges : list[int] or list[Edge]</span>
<span class="sd">            List of edges (or indices of :meth:`edges` if given as ``int``) to</span>
<span class="sd">            which the ``data`` nodes&#39; feature edges will be connected.</span>
<span class="sd">        num_batch_edges : int</span>
<span class="sd">            Number of batch edges in the ``data`` nodes.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.Node(shape=(2, 5, 2),</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                 name=&#39;nodeA&#39;,</span>
<span class="sd">        ...                 init_method=&#39;randn&#39;)</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.Node(shape=(2, 5, 2),</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                 name=&#39;nodeB&#39;,</span>
<span class="sd">        ...                 init_method=&#39;randn&#39;)</span>
<span class="sd">        &gt;&gt;&gt; nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = nodeA.network</span>
<span class="sd">        &gt;&gt;&gt; input_edges = [nodeA[&#39;input&#39;], nodeB[&#39;input&#39;]]</span>
<span class="sd">        &gt;&gt;&gt; net.set_data_nodes(input_edges, 1)</span>
<span class="sd">        &gt;&gt;&gt; list(net.data_nodes.keys())</span>
<span class="sd">        [&#39;data_0&#39;, &#39;data_1&#39;]</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; net[&#39;data_0&#39;]</span>
<span class="sd">        Node(</span>
<span class="sd">            name: data_0</span>
<span class="sd">            tensor:</span>
<span class="sd">                    None</span>
<span class="sd">            axes:</span>
<span class="sd">                    [batch</span>
<span class="sd">                     feature]</span>
<span class="sd">            edges:</span>
<span class="sd">                    [data_0[batch] &lt;-&gt; None</span>
<span class="sd">                     data_0[feature] &lt;-&gt; nodeA[input]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">input_edges</span> <span class="o">==</span> <span class="p">[]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;`input_edges` is empty. &#39;</span>
                <span class="s1">&#39;Cannot set data nodes if no edges are provided&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;Tensor network already has data nodes. These should be unset &#39;</span>
                <span class="s1">&#39;in order to set new ones&#39;</span><span class="p">)</span>

        <span class="c1"># &quot;stack_data_memory&quot; is only created if all the input edges</span>
        <span class="c1"># have the same dimension</span>
        <span class="n">same_dim</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_edges</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">input_edges</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">input_edges</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
                <span class="n">same_dim</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">same_dim</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;stack_data_memory&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="p">:</span>
                <span class="c1"># If `same_dim`, all input_edges have the same feature dimension</span>
                
                <span class="c1"># &#39;n_features&#39; is still in the first position because it is</span>
                <span class="c1"># easier to index just the first position of the tensor,</span>
                <span class="c1"># provided there might be several batch edges</span>
                <span class="n">stack_node</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_edges</span><span class="p">),</span>
                                         <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">num_batch_edges</span><span class="p">),</span>
                                         <span class="n">input_edges</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                                  <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;n_features&#39;</span><span class="p">,</span>
                                              <span class="o">*</span><span class="p">([</span><span class="s1">&#39;batch&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">num_batch_edges</span><span class="p">),</span>
                                              <span class="s1">&#39;feature&#39;</span><span class="p">),</span>
                                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;stack_data_memory&#39;</span><span class="p">,</span>
                                  <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                                  <span class="n">virtual</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Tensor network already has &quot;stack_data_memory&quot; node. &#39;</span>
                    <span class="s1">&#39;Data nodes should be unset in order to set new ones&#39;</span><span class="p">)</span>

        <span class="n">data_nodes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_edges</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">edge</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">edge</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="n">Edge</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">edge</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;Edge </span><span class="si">{</span><span class="n">edge</span><span class="si">!r}</span><span class="s1"> should be a dangling edge of the &#39;</span>
                        <span class="s1">&#39;Tensor Network&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s1">&#39;`input_edges` should be list[int] or list[Edge] type&#39;</span><span class="p">)</span>

            <span class="n">node</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">num_batch_edges</span><span class="p">),</span> <span class="n">edge</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                        <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="o">*</span><span class="p">([</span><span class="s1">&#39;batch&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">num_batch_edges</span><span class="p">),</span> <span class="s1">&#39;feature&#39;</span><span class="p">),</span>
                        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span>
                        <span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">node</span><span class="p">[</span><span class="s1">&#39;feature&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">edge</span>
            <span class="n">data_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">same_dim</span><span class="p">:</span>
            <span class="c1"># Use &quot;stack_data_memory&quot; tensor for all data nodes</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_nodes</span><span class="p">):</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]]</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stack_node</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span></div>

<div class="viewcode-block" id="TensorNetwork.unset_data_nodes"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.unset_data_nodes">[docs]</a>    <span class="k">def</span> <span class="nf">unset_data_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deletes all ``data`` nodes (including the ``&quot;stack_data_memory&quot;`` when</span>
<span class="sd">        this node exists).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">delete_node</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

            <span class="k">if</span> <span class="s1">&#39;stack_data_memory&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">delete_node</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="p">[</span><span class="s1">&#39;stack_data_memory&#39;</span><span class="p">])</span></div>

<div class="viewcode-block" id="TensorNetwork.add_data"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.add_data">[docs]</a>    <span class="k">def</span> <span class="nf">add_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds data tensor(s) to ``data`` nodes, that is, changes their tensors</span>
<span class="sd">        by new data tensors when a new batch is provided.</span>
<span class="sd">        </span>
<span class="sd">        If all data nodes have the same shape, thus having its tensor stored in</span>
<span class="sd">        ``&quot;stack_data_memory&quot;``, the whole data tensor will be stored by this</span>
<span class="sd">        node. The ``data`` nodes will just store a reference to a slice of that</span>
<span class="sd">        tensor.</span>
<span class="sd">        </span>
<span class="sd">        Otherwise, each tensor in the list (``data``) will be stored by each</span>
<span class="sd">        ``data`` node in the network, in the order they appear in</span>
<span class="sd">        :meth:`data_nodes`.</span>
<span class="sd">        </span>
<span class="sd">        If one wants to customize how data is set into the ``data`` nodes, this</span>
<span class="sd">        method can be overriden.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        data : torch.Tensor or list[torch.Tensor]</span>
<span class="sd">            If all data nodes have the same shape, thus having its tensor stored</span>
<span class="sd">            in ``&quot;stack_data_memory&quot;``, ``data`` should be a tensor of shape</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">                batch\_size_{0} \times ... \times batch\_size_{n} \times</span>
<span class="sd">                n_{features} \times feature\_dim</span>
<span class="sd">                </span>
<span class="sd">            Otherwise, it should be a list with :math:`n_{features}` elements,</span>
<span class="sd">            each of them being a tensor with shape</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">                batch\_size_{0} \times ... \times batch\_size_{n} \times</span>
<span class="sd">                feature\_dim</span>
<span class="sd">                </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; nodeA = tk.Node(shape=(3, 5, 3),</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                 name=&#39;nodeA&#39;,</span>
<span class="sd">        ...                 init_method=&#39;randn&#39;)</span>
<span class="sd">        &gt;&gt;&gt; nodeB = tk.Node(shape=(3, 5, 3),</span>
<span class="sd">        ...                 axes_names=(&#39;left&#39;, &#39;input&#39;, &#39;right&#39;),</span>
<span class="sd">        ...                 name=&#39;nodeB&#39;,</span>
<span class="sd">        ...                 init_method=&#39;randn&#39;)</span>
<span class="sd">        &gt;&gt;&gt; nodeA[&#39;right&#39;] ^ nodeB[&#39;left&#39;]</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = nodeA.network</span>
<span class="sd">        &gt;&gt;&gt; input_edges = [nodeA[&#39;input&#39;], nodeB[&#39;input&#39;]]</span>
<span class="sd">        &gt;&gt;&gt; net.set_data_nodes(input_edges, 1)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net.add_data(torch.randn(100, 2, 5))</span>
<span class="sd">        &gt;&gt;&gt; net[&#39;data_0&#39;].shape</span>
<span class="sd">        torch.Size([100, 5])</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">stack_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stack_data_memory&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">stack_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">stack_node</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">data</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data_node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="o">.</span><span class="n">values</span><span class="p">())):</span>
                <span class="n">data_node</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data_node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="o">.</span><span class="n">values</span><span class="p">())):</span>
                <span class="n">data_node</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Cannot add data if no data nodes are set&#39;</span><span class="p">)</span></div>
        
<div class="viewcode-block" id="TensorNetwork.reset"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resets the :class:`TensorNetwork` to its initial state, before computing</span>
<span class="sd">        any non-in-place :class:`Operation`. Different actions apply to different</span>
<span class="sd">        types of nodes:</span>
<span class="sd">        </span>
<span class="sd">        * ``leaf``: These nodes retrieve their tensors in case they were just</span>
<span class="sd">          referencing a slice of the tensor in the :class:`ParamStackNode` that</span>
<span class="sd">          is created when :func:`stacking &lt;stack&gt;` :class:`ParamNodes &lt;ParamNode&gt;`</span>
<span class="sd">          (if ``auto_stack`` mode is active). If there is a ``&quot;virtual_uniform&quot;``</span>
<span class="sd">          node in the network from which all ``leaf`` nodes take their tensor,</span>
<span class="sd">          this is not modified.</span>
<span class="sd">          </span>
<span class="sd">        * ``virtual``: Only virtual nodes created in :class:`operations</span>
<span class="sd">          &lt;Operation&gt;` are :meth:`deleted &lt;delete_node&gt;`. This only includes</span>
<span class="sd">          nodes using the reserved name ``&quot;virtual_stack&quot;``.</span>
<span class="sd">          </span>
<span class="sd">        * ``resultant``: These nodes are :meth:`deleted &lt;delete_node&gt;` from the</span>
<span class="sd">          network.</span>
<span class="sd">        </span>
<span class="sd">        Also, the lists of :class:`Successors &lt;Successor&gt;` of all ``leaf`` and</span>
<span class="sd">        ``data`` nodes are emptied.</span>
<span class="sd">        </span>
<span class="sd">        The :class:`TensorNetwork` is automatically ``reset`` when</span>
<span class="sd">        :meth:`parameterizing &lt;parameterize&gt;` it, changing :meth:`auto_stack`</span>
<span class="sd">        or :meth:`auto_unbind` modes, or :meth:`tracing &lt;trace&gt;`.</span>
<span class="sd">        </span>
<span class="sd">        See :class:`AbstractNode` to learn more about the **4 types** of nodes</span>
<span class="sd">        and the **reserved names**.</span>
<span class="sd">        </span>
<span class="sd">        # TODO: include example from other file</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_seq_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inverse_memory</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="p">:</span>
            <span class="n">aux_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="n">aux_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_leaf_nodes</span><span class="p">)</span>
            <span class="n">aux_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span><span class="p">)</span>
            <span class="n">aux_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">aux_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_virtual</span> <span class="ow">and</span> <span class="p">(</span><span class="s1">&#39;virtual_stack&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>
                    <span class="c1"># Virtual nodes named &quot;virtual_stack&quot; are ParamStackNodes</span>
                    <span class="c1"># that result from stacking a collection of ParamNodes</span>
                    <span class="c1"># This condition is satisfied by the rest of virtual nodes</span>
                    <span class="k">continue</span>

                <span class="n">node</span><span class="o">.</span><span class="n">_successors</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

                <span class="n">node_ref</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">node_ref</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">node_ref</span><span class="o">.</span><span class="n">_virtual</span> <span class="ow">and</span> <span class="p">(</span><span class="s1">&#39;virtual_uniform&#39;</span> <span class="ow">in</span> <span class="n">node_ref</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>
                        <span class="c1"># Virtual nodes named &quot;virtual_uniform&quot; are ParamNodes</span>
                        <span class="c1"># whose tensor is shared accross all the nodes in a</span>
                        <span class="c1"># uniform tensor network</span>
                        <span class="k">continue</span>

                <span class="c1"># Store tensor as temporary</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">tensor</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;node_ref&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_tensor_info</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;param_&#39;</span> <span class="o">+</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>
                        <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;param_&#39;</span> <span class="o">+</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_memory_nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="c1"># Set tensor and save it in ``memory_nodes``</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">_unrestricted_set_tensor</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span><span class="p">)</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">_temp_tensor</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
                <span class="n">node</span><span class="o">.</span><span class="n">_successors</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

            <span class="n">aux_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="n">aux_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span><span class="p">)</span>
            <span class="n">aux_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_virtual_nodes</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">aux_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">_virtual</span> <span class="ow">and</span> <span class="p">(</span><span class="s1">&#39;virtual_stack&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>
                    <span class="c1"># This condition is satisfied by the rest of virtual nodes</span>
                    <span class="c1"># (e.g. &quot;virtual_feature&quot;, &quot;virtual_n_features&quot;)</span>
                    <span class="k">continue</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">delete_node</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span></div>
                
<div class="viewcode-block" id="TensorNetwork.trace"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.trace">[docs]</a>    <span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">example</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Traces the tensor network contraction algorithm with two purposes:</span>

<span class="sd">        * Create all the intermediate ``resultant`` nodes that result from</span>
<span class="sd">          :class:`Operations &lt;Operation&gt;` so that in the next contractions only</span>
<span class="sd">          the :ref:`tensor-like-ops` have to be computed, thus saving a lot of</span>
<span class="sd">          time.</span>

<span class="sd">        * Keep track of the tensors that are used to compute operations, so that</span>
<span class="sd">          intermediate results that are not useful any more can be deleted, thus</span>
<span class="sd">          saving a lot of memory. This is achieved by constructing an</span>
<span class="sd">          ``inverse_memory`` that, given a memory address, stores the nodes that</span>
<span class="sd">          use the tensor located in that address of the network&#39;s memory.</span>

<span class="sd">        To trace a tensor network, it is necessary to provide the same arguments</span>
<span class="sd">        that would be required in the forward call. In case the tensor network</span>
<span class="sd">        is contracted with some input data, an example tensor with batch dimension</span>
<span class="sd">        1 and filled with zeros would be enough to trace the contraction.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        example : torch.Tensor, optional</span>
<span class="sd">            Example tensor used to trace the contraction of the tensor network.</span>
<span class="sd">            In case the tensor network is contracted with some input data, an</span>
<span class="sd">            example tensor with batch dimension 1 and filled with zeros would</span>
<span class="sd">            be enough to trace the contraction.</span>
<span class="sd">        args :</span>
<span class="sd">            Arguments that might be used in :meth:`contract`.</span>
<span class="sd">        kwargs :</span>
<span class="sd">            Keyword arguments that might be used in :meth:`contract`.</span>
<span class="sd">            </span>
<span class="sd">        # TODO: include example from other file</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tracing</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tracing</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorNetwork.contract"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.contract">[docs]</a>    <span class="k">def</span> <span class="nf">contract</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Node</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Contracts the whole tensor network returning a single :class:`Node`.</span>
<span class="sd">        This method is not implemented and subclasses of :class:`TensorNetwork`</span>
<span class="sd">        should override it to define the contraction algorithm of the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Custom, optimized contraction methods should be defined for each new</span>
        <span class="c1"># subclass of TensorNetwork</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s1">&#39;Contraction methods not implemented for generic TensorNetwork class&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorNetwork.forward"><a class="viewcode-back" href="../../components.html#tensorkrowch.TensorNetwork.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Contracts :class:`TensorNetwork` with input data. It can be called using</span>
<span class="sd">        the ``__call__`` operator ``()``.</span>

<span class="sd">        Overrides the ``forward`` method of **PyTorch**&#39;s ``torch.nn.Module``.</span>
<span class="sd">        Sets data nodes automatically whenever :meth:`set_data_nodes` is</span>
<span class="sd">        overriden, :meth:`adds data &lt;add_data&gt;` tensor(s) to these nodes, and</span>
<span class="sd">        contracts the whole network according to :meth:`contract`, returning a</span>
<span class="sd">        single ``torch.Tensor``.</span>
<span class="sd">        </span>
<span class="sd">        Furthermore, to optimize the contraction algorithm during training,</span>
<span class="sd">        once the :class:`TensorNetwork` is :meth:`traced &lt;trace&gt;`, all that</span>
<span class="sd">        ``forward`` does is calling the different :class:`Operations &lt;Operation&gt;`</span>
<span class="sd">        used in :meth:`contract` in the same order they appeared in the code.</span>
<span class="sd">        Hence, the **last operation** in :meth:`contract` should be the one that</span>
<span class="sd">        **returns the single output** :class:`Node`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        data : torch.Tensor or list[torch.Tensor], optional</span>
<span class="sd">            If all data nodes have the same shape, thus having its tensor stored</span>
<span class="sd">            in ``&quot;stack_data_memory&quot;``, ``data`` should be a tensor of shape</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">                batch\_size_{0} \times ... \times batch\_size_{n} \times</span>
<span class="sd">                n_{features} \times feature\_dim</span>
<span class="sd">                </span>
<span class="sd">            Otherwise, it should be a list with :math:`n_{features}` elements,</span>
<span class="sd">            each of them being a tensor with shape</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">                batch\_size_{0} \times ... \times batch\_size_{n} \times</span>
<span class="sd">                feature\_dim</span>

<span class="sd">            Also, it is not necessary that the network has ``data`` nodes, thus</span>
<span class="sd">            ``None`` is also valid.</span>
<span class="sd">        args :</span>
<span class="sd">            Arguments that might be used in :meth:`contract`.</span>
<span class="sd">        kwargs :</span>
<span class="sd">            Keyword arguments that might be used in :meth:`contract`.</span>
<span class="sd">            </span>
<span class="sd">        # TODO: include example from other file</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_nodes</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_data_nodes</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_data</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resultant_nodes</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">contract</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">tensor</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seq_ops</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">operations</span><span class="p">[</span><span class="n">op</span><span class="p">[</span><span class="mi">0</span><span class="p">]](</span><span class="o">**</span><span class="n">op</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">Node</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">op</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;unbind&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The last operation should be the one &#39;</span>
                                     <span class="s1">&#39;returning a single resultant node&#39;</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">tensor</span></div>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Text</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Edge</span><span class="p">,</span> <span class="n">AbstractNode</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">Text</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Tensor network </span><span class="si">{</span><span class="bp">self</span><span class="si">!s}</span><span class="s1"> does not have any node with &#39;</span>
                    <span class="sa">f</span><span class="s1">&#39;name </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;`key` should be int or str type&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Text</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">(</span><span class="se">\n</span><span class="s1"> &#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">name: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">nodes: </span><span class="se">\n</span><span class="si">{</span><span class="n">tab_string</span><span class="p">(</span><span class="n">print_list</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">())),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span> \
               <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">edges:</span><span class="se">\n</span><span class="si">{</span><span class="n">tab_string</span><span class="p">(</span><span class="n">print_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_edges</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">)&#39;</span></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By José Ramón Pareja Monturiol<br/>
  
      &copy; Copyright 2023, José Ramón Pareja Monturiol.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>