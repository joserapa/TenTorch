
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Contracting and Differentiating the Tensor Network &#8212; TensorKrowch 1.0.1 documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <link rel="shortcut icon" href="../_static/tensorkrowch_favicon_light.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to save Memory and Time with TensorKrowch (ADVANCED)" href="3_memory_management.html" />
    <link rel="prev" title="Creating a Tensor Network in TensorKrowch" href="1_creating_tensor_network.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tensorkrowch_logo_light.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contents:
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../tutorials.html">
   Tutorials
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="0_first_steps.html">
     First Steps with TensorKrowch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="1_creating_tensor_network.html">
     Creating a Tensor Network in TensorKrowch
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Contracting and Differentiating the Tensor Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_memory_management.html">
     How to save Memory and Time with TensorKrowch (ADVANCED)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_types_of_nodes.html">
     The different Types of Nodes (ADVANCED)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_subclass_tensor_network.html">
     How to subclass TensorNetwork to build Custom Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6_mix_with_pytorch.html">
     Creating a Hybrid Neural-Tensor Network Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../api.html">
   API Reference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../components.html">
     Components
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../operations.html">
     Operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models.html">
     Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../initializers.html">
     Initializers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings.html">
     Embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../decompositions.html">
     Decompositions
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/joserapa98/tensorkrowch"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/tutorials/2_contracting_tensor_network.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps">
   Steps
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distinguish-between-nodes-and-paramnodes">
     1. Distinguish between Nodes and ParamNodes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#operations-between-nodes">
     2. Operations between Nodes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#contracting-a-matrix-product-state">
     3. Contracting a Matrix Product State
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Contracting and Differentiating the Tensor Network</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps">
   Steps
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distinguish-between-nodes-and-paramnodes">
     1. Distinguish between Nodes and ParamNodes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#operations-between-nodes">
     2. Operations between Nodes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#contracting-a-matrix-product-state">
     3. Contracting a Matrix Product State
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="contracting-and-differentiating-the-tensor-network">
<span id="tutorial-2"></span><h1>Contracting and Differentiating the Tensor Network<a class="headerlink" href="#contracting-and-differentiating-the-tensor-network" title="Permalink to this headline">#</a></h1>
<p>In the previous <a class="reference internal" href="1_creating_tensor_network.html#tutorial-1"><span class="std std-ref">tutorial</span></a> you learned how to build a fixed
tensor network. However, <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> is built on top of <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> in
order to be able to train these models as easily as any other <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.
Hence, the next step should be to learn about the components of <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code>
that make it possible to compute <cite>learnable</cite> functions.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>In this tutorial you will learn about the two main classes of nodes in
<code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> and how to operate with them.</p>
</section>
<section id="steps">
<h2>Steps<a class="headerlink" href="#steps" title="Permalink to this headline">#</a></h2>
<ol class="arabic simple">
<li><p>Distinguish between Nodes and ParamNodes.</p></li>
<li><p>Operations between nodes.</p></li>
<li><p>Contracting a Matrix Product State.</p></li>
</ol>
<section id="distinguish-between-nodes-and-paramnodes">
<h3>1. Distinguish between Nodes and ParamNodes<a class="headerlink" href="#distinguish-between-nodes-and-paramnodes" title="Permalink to this headline">#</a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> there are 2 main classes of nodes: the ones that are fixed
(<a class="reference internal" href="../components.html#tensorkrowch.Node" title="tensorkrowch.Node"><code class="xref py py-class docutils literal notranslate"><span class="pre">Nodes</span></code></a>) and the ones you can train (<a class="reference internal" href="../components.html#tensorkrowch.ParamNode" title="tensorkrowch.ParamNode"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParamNodes</span></code></a>).
The main (and almost only difference) is that <code class="docutils literal notranslate"><span class="pre">Nodes</span></code> contain a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>,
while <code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code> contain a <code class="docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code>, the <cite>tensors</cite> of PyTorch
with respect to which gradients are computed.</p>
<p><code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code> are initalized in the same fashion as <code class="docutils literal notranslate"><span class="pre">Nodes</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">tensorkrowch</span> <span class="k">as</span> <span class="nn">tk</span>

<span class="n">paramnode1</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">ParamNode</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>     <span class="c1"># Empty paramnode</span>
<span class="n">paramnode2</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">ParamNode</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                          <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;randn&#39;</span><span class="p">)</span>
<span class="n">paramnode3</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                      <span class="n">param_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Indicates if node is ParamNode</span>
</pre></div>
</div>
<p>Also, if we try to initialize a <code class="docutils literal notranslate"><span class="pre">ParamNode</span></code> with an existing <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>,
this will be first transformed into a <code class="docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">paramnode</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">ParamNode</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paramnode</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">)</span>
</pre></div>
</div>
<p>Another important and useful feature of <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> is that you can
<cite>parameterize</cite> <code class="docutils literal notranslate"><span class="pre">Nodes</span></code> or <cite>de-parameterize</cite> <code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code> at any time:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node</span> <span class="o">=</span> <span class="n">paramnode</span><span class="o">.</span><span class="n">parameterize</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">)</span>

<span class="n">paramnode</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">parameterize</span><span class="p">()</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paramnode</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">)</span>
</pre></div>
</div>
<p>Be aware that when parameterizing or de-parameterizing, the previous <code class="docutils literal notranslate"><span class="pre">Node</span></code>
or <code class="docutils literal notranslate"><span class="pre">ParamNode</span></code> will be overriden in the network by the new <code class="docutils literal notranslate"><span class="pre">ParamNode</span></code> or
<code class="docutils literal notranslate"><span class="pre">Node</span></code>, respectively.</p>
<p>Finally, to check that, effectively, these <code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code> can be trained, let’s
compute a simple function and differentiate:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">sum</span> <span class="o">=</span> <span class="n">paramnode</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># Sums over all axes of the node</span>
<span class="nb">sum</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>         <span class="c1"># Differentiates sum with respect to paramnode</span>
</pre></div>
</div>
<p>Now with <code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code> we can access directly the gradient of their tensors via:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">paramnode</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
<p>Although this is insightful to learn the basics of <code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code>, we want
tools to work with tensor networks. In the next section you will learn
about an important part of <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code>: <a class="reference internal" href="../operations.html#tensorkrowch.Operation" title="tensorkrowch.Operation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Operations</span></code></a>.</p>
</section>
<section id="operations-between-nodes">
<h3>2. Operations between Nodes<a class="headerlink" href="#operations-between-nodes" title="Permalink to this headline">#</a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> there are some <a class="reference internal" href="../operations.html#tensorkrowch.Operation" title="tensorkrowch.Operation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Operations</span></code></a> you can
compute between nodes. We can distinguish between two types of operations:</p>
<ol class="loweralpha simple">
<li><p><strong>Tensor-like</strong>: We refer to the operations one can compute using tensors in
vanilla <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> like <a class="reference internal" href="../operations.html#tensorkrowch.permute" title="tensorkrowch.permute"><code class="xref py py-func docutils literal notranslate"><span class="pre">permute()</span></code></a> (and the in-place variant
<a class="reference internal" href="../operations.html#tensorkrowch.permute_" title="tensorkrowch.permute_"><code class="xref py py-func docutils literal notranslate"><span class="pre">permute_()</span></code></a>), <a class="reference internal" href="../operations.html#tensorkrowch.tprod" title="tensorkrowch.tprod"><code class="xref py py-func docutils literal notranslate"><span class="pre">tprod()</span></code></a> (tensor product), <a class="reference internal" href="../operations.html#tensorkrowch.mul" title="tensorkrowch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">mul()</span></code></a>, <a class="reference internal" href="../operations.html#tensorkrowch.add" title="tensorkrowch.add"><code class="xref py py-func docutils literal notranslate"><span class="pre">add()</span></code></a>
and <a class="reference internal" href="../operations.html#tensorkrowch.sub" title="tensorkrowch.sub"><code class="xref py py-func docutils literal notranslate"><span class="pre">sub()</span></code></a>.</p></li>
<li><p><strong>Node-like</strong>: We refer to the operations one will need to contract a tensor
network. These we will explain in more detail in this section.</p></li>
</ol>
<p>For both types of operations, the result will always be a <code class="docutils literal notranslate"><span class="pre">Node</span></code>. That is,
<code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code> can only be used as the <cite>initial</cite> nodes that define a tensor
network, and with respect to which we will differentiate. But all intermediate
nodes that result from an operation will be non-parametric <code class="docutils literal notranslate"><span class="pre">Nodes</span></code>.</p>
<p>Regarding the <strong>node-like</strong> operations, these are:</p>
<ol class="arabic">
<li><p><a class="reference internal" href="../operations.html#tensorkrowch.contract_between" title="tensorkrowch.contract_between"><code class="xref py py-func docutils literal notranslate"><span class="pre">contract_between()</span></code></a>: Contracts all connected edges between two nodes.
The operand <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> can be used to perform the contraction:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node1</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                 <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">))</span>
<span class="n">node2</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                 <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">))</span>

<span class="n">node1</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">node2</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span>
<span class="n">node1</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">node2</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">node1</span> <span class="o">@</span> <span class="n">node2</span>

<span class="k">assert</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">()</span>
</pre></div>
</div>
<p>There also variants of this operations. You can contract nodes in-place with
<a class="reference internal" href="../operations.html#tensorkrowch.contract_between_" title="tensorkrowch.contract_between_"><code class="xref py py-func docutils literal notranslate"><span class="pre">contract_between_()</span></code></a>), that is, modifying the initial network you defined.
You can also contract only selected edges with <a class="reference internal" href="../operations.html#tensorkrowch.contract_edges" title="tensorkrowch.contract_edges"><code class="xref py py-func docutils literal notranslate"><span class="pre">contract_edges()</span></code></a>.</p>
</li>
<li><p><a class="reference internal" href="../operations.html#tensorkrowch.split" title="tensorkrowch.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">split()</span></code></a>: Splits a node in two via Singular Value or QR decompositions.
The edges that go with each resultant node should be specified:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left1&#39;</span><span class="p">,</span> <span class="s1">&#39;left2&#39;</span><span class="p">,</span> <span class="s1">&#39;right1&#39;</span><span class="p">,</span> <span class="s1">&#39;right2&#39;</span><span class="p">))</span>
<span class="n">res1</span><span class="p">,</span> <span class="n">res2</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">split</span><span class="p">([</span><span class="s1">&#39;left1&#39;</span><span class="p">,</span> <span class="s1">&#39;right1&#39;</span><span class="p">],</span>
                        <span class="p">[</span><span class="s1">&#39;left2&#39;</span><span class="p">,</span> <span class="s1">&#39;right2&#39;</span><span class="p">],</span>
                        <span class="n">rank</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">res1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">res2</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>As can be noted, there is also a new edge connecting the resultant nodes.
Similar to <code class="docutils literal notranslate"><span class="pre">contract_between</span></code>, there is also an in-place variant <a class="reference internal" href="../operations.html#tensorkrowch.split_" title="tensorkrowch.split_"><code class="xref py py-func docutils literal notranslate"><span class="pre">split_()</span></code></a>.</p>
</li>
<li><p><a class="reference internal" href="../operations.html#tensorkrowch.stack" title="tensorkrowch.stack"><code class="xref py py-func docutils literal notranslate"><span class="pre">stack()</span></code></a>: Stacks a list of nodes of the same <cite>type</cite>. That is, only nodes
with the same number of edges, same axes names and belonging to the same
network. The sizes of each edge, however, can be different for different nodes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">TensorNetwork</span><span class="p">()</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">),</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

<span class="n">stack_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">stack_node</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The resultant <code class="docutils literal notranslate"><span class="pre">stack_node</span></code> is actually a different class of node, a
<a class="reference internal" href="../components.html#tensorkrowch.StackNode" title="tensorkrowch.StackNode"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackNode</span></code></a>. These only result from stacking other nodes, and have
as first edge a special <strong>batch</strong> edge called <code class="docutils literal notranslate"><span class="pre">&quot;stack&quot;</span></code>. The rest of edges
are of class <a class="reference internal" href="../components.html#tensorkrowch.StackEdge" title="tensorkrowch.StackEdge"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackEdge</span></code></a>, a new type of edge that collect information
from all the edges from the nodes that are being stacked. This information
enables to automatically reconnect nodes to their previous neighbours when
<code class="docutils literal notranslate"><span class="pre">unbinding</span></code> the stack.</p>
<p>Be aware that stacks <strong>cannot recognize neighbours</strong>. That is, if we create
two stacks of nodes that were all connected one-to-one, we have to reconnect
the stacks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">TensorNetwork</span><span class="p">()</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">data_nodes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">),</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

    <span class="n">data_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                    <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;feature&#39;</span><span class="p">),</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;data_node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">data_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_node</span><span class="p">)</span>

    <span class="n">node</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">data_node</span><span class="p">[</span><span class="s1">&#39;feature&#39;</span><span class="p">]</span>

<span class="n">stack_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
<span class="n">stack_data_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">data_nodes</span><span class="p">)</span>

<span class="c1"># reconnect stacks</span>
<span class="n">stack_node</span> <span class="o">^</span> <span class="n">stack_data_node</span>
</pre></div>
</div>
</li>
<li><p><a class="reference internal" href="../operations.html#tensorkrowch.unbind" title="tensorkrowch.unbind"><code class="xref py py-func docutils literal notranslate"><span class="pre">unbind()</span></code></a>: Unbinds a <a class="reference internal" href="../components.html#tensorkrowch.StackNode" title="tensorkrowch.StackNode"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackNode</span></code></a> and returns a list of nodes that
are already connected to the corresponding neighbours:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">TensorNetwork</span><span class="p">()</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">),</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

<span class="n">stack_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
<span class="n">unbinded_nodes</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">stack_node</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">unbinded_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><a class="reference internal" href="../operations.html#tensorkrowch.einsum" title="tensorkrowch.einsum"><code class="xref py py-func docutils literal notranslate"><span class="pre">einsum()</span></code></a>: Evaluates the Einstein summation convention on the nodes.
It is based on <a class="reference external" href="https://optimized-einsum.readthedocs.io/en/stable/autosummary/opt_einsum.contract.html">opt_einsum</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node1</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                 <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="s1">&#39;batch&#39;</span><span class="p">))</span>
<span class="n">node2</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                 <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="s1">&#39;batch&#39;</span><span class="p">))</span>
<span class="n">node3</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                 <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="s1">&#39;batch&#39;</span><span class="p">))</span>

<span class="n">node1</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">node2</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span>
<span class="n">node2</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">node3</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span>
<span class="n">node3</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">node1</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ijb,jkb,kib-&gt;b&#39;</span><span class="p">,</span> <span class="n">node1</span><span class="p">,</span> <span class="n">node2</span><span class="p">,</span> <span class="n">node3</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">100</span><span class="p">,)</span>
</pre></div>
</div>
<p>There is another variant of <code class="docutils literal notranslate"><span class="pre">einsum</span></code> that accepts a sequence of lists of
nodes and previously stacks each list of nodes in a <code class="docutils literal notranslate"><span class="pre">StackNode</span></code> and then
evaluates a batched version of <code class="docutils literal notranslate"><span class="pre">einsum</span></code>. This operation is
<a class="reference internal" href="../operations.html#tensorkrowch.stacked_einsum" title="tensorkrowch.stacked_einsum"><code class="xref py py-func docutils literal notranslate"><span class="pre">stacked_einsum()</span></code></a>.</p>
</li>
</ol>
<p>Some of this operations can also be called from the nodes’ edges, like
<a class="reference internal" href="../operations.html#tensorkrowch.contract_" title="tensorkrowch.contract_"><code class="xref py py-func docutils literal notranslate"><span class="pre">contract_()</span></code></a> or <a class="reference internal" href="../operations.html#tensorkrowch.svd_" title="tensorkrowch.svd_"><code class="xref py py-func docutils literal notranslate"><span class="pre">svd_()</span></code></a>.</p>
</section>
<section id="contracting-a-matrix-product-state">
<h3>3. Contracting a Matrix Product State<a class="headerlink" href="#contracting-a-matrix-product-state" title="Permalink to this headline">#</a></h3>
<p>Now that you know how to construct a <a class="reference internal" href="../components.html#tensorkrowch.TensorNetwork" title="tensorkrowch.TensorNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorNetwork</span></code></a> with <code class="docutils literal notranslate"><span class="pre">ParamNodes</span></code>,
and use <code class="docutils literal notranslate"><span class="pre">Operations</span></code> between them, let’s apply all of this to contract a
Matrix Product State (MPS) with some input data, and compute gradients of the
result with respect to the MPS nodes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mps</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">TensorNetwork</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;mps&#39;</span><span class="p">)</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">data_nodes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">),</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">mps</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span>
                    <span class="n">param_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

    <span class="n">data_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span>
                         <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;feature&#39;</span><span class="p">,),</span>
                         <span class="n">network</span><span class="o">=</span><span class="n">mps</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;data_node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">data_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_node</span><span class="p">)</span>

    <span class="n">node</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">data_node</span><span class="p">[</span><span class="s1">&#39;feature&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">mps</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">][</span><span class="s1">&#39;right&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">mps</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">100</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">][</span><span class="s1">&#39;left&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>With this, we have already created our MPS where nodes can be trained. We have
also added some data nodes that will hold our data (though in this example they
will be filled with random tensors).</p>
<p>To contract all the data nodes with their respective neighbours we can use
<code class="docutils literal notranslate"><span class="pre">stack</span></code> to perform a single big contraction, instead of a hundread of small
contractions, which will save us some time:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">stack_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
<span class="n">stack_data_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">data_nodes</span><span class="p">)</span>

<span class="n">stack_node</span> <span class="o">^</span> <span class="n">stack_data_node</span>

<span class="n">stack_result</span> <span class="o">=</span> <span class="n">stack_node</span> <span class="o">@</span> <span class="n">stack_data_node</span>
<span class="n">unbind_result</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">stack_result</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we have a list with a bunch of matrices that are all connected to the
previous and next ones, forming a ring. Let’s contract all of them with
simple contractions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">unbind_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">unbind_result</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="n">result</span> <span class="o">@=</span> <span class="n">node</span>

<span class="k">assert</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">()</span>
</pre></div>
</div>
<p>Since we have contracted the whole network, and no edge is still dangling, the
result is a single number. We can then compute gradients:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">node</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Here we have our desired gradient! Now you can use it to learn a function using
gradient descent methods.</p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="1_creating_tensor_network.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Creating a Tensor Network in TensorKrowch</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_memory_management.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to save Memory and Time with TensorKrowch (ADVANCED)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By José Ramón Pareja Monturiol<br/>
  
      &copy; Copyright 2023, José Ramón Pareja Monturiol.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>