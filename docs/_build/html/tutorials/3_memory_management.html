
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>How to save Memory and Time with TensorKrowch (ADVANCED) &#8212; TensorKrowch 1.0.0 documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/tensorkrowch_favicon_light.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The different Types of Nodes (ADVANCED)" href="4_types_of_nodes.html" />
    <link rel="prev" title="Contracting and Differentiating the Tensor Network" href="2_contracting_tensor_network.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tensorkrowch_logo_light.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contents:
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../tutorials.html">
   Tutorials
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="0_first_steps.html">
     First Steps with TensorKrowch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="1_creating_tensor_network.html">
     Creating a Tensor Network in TensorKrowch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_contracting_tensor_network.html">
     Contracting and Differentiating the Tensor Network
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     How to save Memory and Time with TensorKrowch (ADVANCED)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_types_of_nodes.html">
     The different Types of Nodes (ADVANCED)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_subclass_tensor_network.html">
     How to subclass TensorNetwork to build Custom Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6_mix_with_pytorch.html">
     Creating a Hybrid Neural-Tensor Network Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../api.html">
   API Reference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../components.html">
     Components
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../operations.html">
     Operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models.html">
     Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../initializers.html">
     Initializers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings.html">
     Embeddings
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/joserapa98/tensorkrowch"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/tutorials/3_memory_management.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps">
   Steps
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-tensors-are-stored-in-the-tensornetwork">
     1. How Tensors are stored in the TensorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-tensorkrowch-skipps-operations-to-run-faster">
     2. How TensorKrowch skipps Operations to run faster
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#memory-management-modes">
     3. Memory Management modes
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>How to save Memory and Time with TensorKrowch (ADVANCED)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps">
   Steps
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-tensors-are-stored-in-the-tensornetwork">
     1. How Tensors are stored in the TensorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-tensorkrowch-skipps-operations-to-run-faster">
     2. How TensorKrowch skipps Operations to run faster
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#memory-management-modes">
     3. Memory Management modes
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="how-to-save-memory-and-time-with-tensorkrowch-advanced">
<span id="tutorial-3"></span><h1>How to save Memory and Time with TensorKrowch (ADVANCED)<a class="headerlink" href="#how-to-save-memory-and-time-with-tensorkrowch-advanced" title="Permalink to this headline">#</a></h1>
<p>Since <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> is devoted to construct tensor network models, many of
the operations one can compute between nodes have to keep track of information
of the underlying graph. However, this could be very costly if we had to compute
all these ancillary steps every time during training. That is why <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code>
uses different tricks like managing memory and skipping operations automatically
in order to save some memory and time.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>In this tutorial you will learn how memory is stored in the tensor network and
what are the tricks <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> uses to take advantage of that.</p>
<p>Also, you will learn how some steps are skipped in <a class="reference internal" href="../operations.html#tensorkrowch.Operation" title="tensorkrowch.Operation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Operations</span></code></a>
in order to save time. Learn more about how to use operations in the previous
<a class="reference internal" href="2_contracting_tensor_network.html#tutorial-2"><span class="std std-ref">tutorial</span></a>.</p>
</section>
<section id="steps">
<h2>Steps<a class="headerlink" href="#steps" title="Permalink to this headline">#</a></h2>
<ol class="arabic simple">
<li><p>How Tensors are stored in the TensorNetwork.</p></li>
<li><p>How TensorKrowch skipps Operations to run faster.</p></li>
<li><p>Memory Management modes.</p></li>
</ol>
<section id="how-tensors-are-stored-in-the-tensornetwork">
<h3>1. How Tensors are stored in the TensorNetwork<a class="headerlink" href="#how-tensors-are-stored-in-the-tensornetwork" title="Permalink to this headline">#</a></h3>
<p>As explained in the first <a class="reference internal" href="1_creating_tensor_network.html#tutorial-1"><span class="std std-ref">tutorial</span></a>, although nodes act as a
sort of <cite>containers</cite> for <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>’s, this is not what happens under the
hood.</p>
<p>Actually, each <code class="docutils literal notranslate"><span class="pre">TensorNetwork</span></code> has a unique memory where all tensors are stored.
This memory can be accessed by all nodes to retrieve their respective tensors.
Hence, all that nodes <cite>contain</cite> is just a <strong>memory address</strong> together with some
other information that helps to access the correct tensor.</p>
<p>When a node is instantiated a memory address is created with the name of the
node. That is where its tensor will be stored. Even if the node is empty, that
place is reserved in case we set a tensor in the empty node.</p>
<p>We can check the memory address of our nodes via:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">tensorkrowch</span> <span class="k">as</span> <span class="nn">tk</span>

<span class="n">node1</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;my_node&#39;</span><span class="p">)</span>
<span class="n">node1</span><span class="o">.</span><span class="n">tensor_address</span><span class="p">()</span>
</pre></div>
</div>
<p>For now, there is no tensor stored in that memory address:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node1</span><span class="o">.</span><span class="n">tensor</span>
</pre></div>
</div>
<p>But we can set a new tensor into the node:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">new_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">node1</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">new_tensor</span>  <span class="c1"># Same as node1.set_tensor(new_tensor)</span>
</pre></div>
</div>
<p>Now <code class="docutils literal notranslate"><span class="pre">node</span></code> is not empty, there is a tensor stored in its corresponding memory
address:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node1</span><span class="o">.</span><span class="n">tensor</span>
</pre></div>
</div>
<p>Since nodes only contain memory addresses, we can create a second node that
instead of storing its own memory address, it uses the memory of the first node:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node2</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;your_node&#39;</span><span class="p">,</span>
                <span class="n">network</span><span class="o">=</span><span class="n">node1</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>
<span class="n">node2</span><span class="o">.</span><span class="n">set_tensor_from</span><span class="p">(</span><span class="n">node1</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">node2</span><span class="o">.</span><span class="n">tensor_address</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;my_node&#39;</span>
</pre></div>
</div>
<p>Of course, to share memory, both nodes need to be in the same network and have
the same shape.</p>
<p>Now, if we change the tensor in <code class="docutils literal notranslate"><span class="pre">node1</span></code>, <code class="docutils literal notranslate"><span class="pre">node2</span></code> will reproduce the same
change:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node1</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">node1</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
<span class="n">node2</span><span class="o">.</span><span class="n">tensor</span>
</pre></div>
</div>
<p>Furthermore, we can have even more nodes sharing the same memory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node3</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;other_node&#39;</span><span class="p">,</span>
                <span class="n">network</span><span class="o">=</span><span class="n">node1</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>
<span class="n">node3</span><span class="o">.</span><span class="n">set_tensor_from</span><span class="p">(</span><span class="n">node2</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">node3</span><span class="o">.</span><span class="n">tensor_address</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;my_node&#39;</span>
</pre></div>
</div>
<p>This feature of <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> can be very useful to create uniform or
translationally invariant tensor networks by simply using a node whose memory
is shared by all the nodes in the network. Let’s create a Uniform Matrix
Product State:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mps</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">TensorNetwork</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;mps&#39;</span><span class="p">)</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">uniform_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                        <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">),</span>
                        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span>
                        <span class="n">network</span><span class="o">=</span><span class="n">mps</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">axes_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">),</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">mps</span><span class="p">)</span>
    <span class="n">node</span><span class="o">.</span><span class="n">set_tensor_from</span><span class="p">(</span><span class="n">uniform_node</span><span class="p">)</span>

    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">mps</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">][</span><span class="s1">&#39;right&#39;</span><span class="p">]</span> <span class="o">^</span> <span class="n">mps</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;node_(</span><span class="si">{</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">100</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">][</span><span class="s1">&#39;left&#39;</span><span class="p">]</span>

<span class="c1"># Check that all nodes share tensor with uniform_node</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">node</span><span class="o">.</span><span class="n">tensor_address</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span>
</pre></div>
</div>
</section>
<section id="how-tensorkrowch-skipps-operations-to-run-faster">
<h3>2. How TensorKrowch skipps Operations to run faster<a class="headerlink" href="#how-tensorkrowch-skipps-operations-to-run-faster" title="Permalink to this headline">#</a></h3>
<p>The main purpose of <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> is enabling you to experiment
creating and training different tensor networks, only having to worry about
instantiating nodes, making connections and performing contractions.</p>
<p>Because of that, there is much going on under the hood. For instance, say you
want to contract these two nodes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">node1</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">node2</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">node1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">^</span> <span class="n">node2</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">node1</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">^</span> <span class="n">node2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">node1</span> <span class="o">@</span> <span class="n">node2</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> returns directly the resultant node with its edges in the
correct order. To perform that contraction in vanilla <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>, you would
have to permute and reshape both nodes to compute a matrix multiplication, and
then reshape and permute again the result. And for every different node you
would have to think how to do the permutes and reshapes to leave the resultant
edges in the desired order.</p>
<p><code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> does all of that for you, but it is costly. To avoid having
such overhead compared to just performing a matrix multiplication,
<code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> calculates how the permutes and reshapes should be performed
only during the first time a contraction occurs. Then all the ancillary
information needed to perform the contraction is saved in a sort of cache memory.
In subsequent contractions, <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> will behave almost like vanilla
<code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>.</p>
</section>
<section id="memory-management-modes">
<h3>3. Memory Management modes<a class="headerlink" href="#memory-management-modes" title="Permalink to this headline">#</a></h3>
<p>Now that you know how <code class="docutils literal notranslate"><span class="pre">TensorKrowch</span></code> manages memory and skips some steps
when operating with nodes repeatedly, you can learn about <strong>two important modes</strong>
that can be turned on/off for training or inference.</p>
<ul>
<li><p><strong>auto_stack</strong> (<code class="docutils literal notranslate"><span class="pre">False</span></code> by default): This mode indicates whether the
operation <a class="reference internal" href="../operations.html#tensorkrowch.stack" title="tensorkrowch.stack"><code class="xref py py-func docutils literal notranslate"><span class="pre">stack()</span></code></a> can take control of the memory management of the
network to skip some steps in future computations. If <code class="docutils literal notranslate"><span class="pre">auto_stack</span></code> is set
to <code class="docutils literal notranslate"><span class="pre">True</span></code> and a collection of <a class="reference internal" href="../components.html#tensorkrowch.ParamNode" title="tensorkrowch.ParamNode"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParamNodes</span></code></a> are
<a class="reference internal" href="../operations.html#tensorkrowch.stack" title="tensorkrowch.stack"><code class="xref py py-func docutils literal notranslate"><span class="pre">stacked</span></code></a> (as the first operation in which these nodes are
involved), then those nodes will no longer store their own tensors, but
rather a <code class="docutils literal notranslate"><span class="pre">virtual</span></code> <a class="reference internal" href="../components.html#tensorkrowch.ParamStackNode" title="tensorkrowch.ParamStackNode"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParamStackNode</span></code></a> will store the stacked tensor,
avoiding the computation of that first <a class="reference internal" href="../operations.html#tensorkrowch.stack" title="tensorkrowch.stack"><code class="xref py py-func docutils literal notranslate"><span class="pre">stack()</span></code></a> in every contraction.
This behaviour is not possible if <code class="docutils literal notranslate"><span class="pre">auto_stack</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, in
which case all nodes will always store their own tensors.</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">auto_stack</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> will be faster for both <strong>inference</strong> and
<strong>training</strong>. However, while experimenting with <code class="docutils literal notranslate"><span class="pre">TensorNetworks</span></code> one might
want that all nodes store their own tensors to avoid problems.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">TensorNetwork</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">auto_stack</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">)</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

<span class="c1"># First operation is computed</span>
<span class="n">stack_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>

<span class="c1"># All ParamNodes use a slice of the tensor in stack_node</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">node</span><span class="o">.</span><span class="n">tensor_address</span><span class="p">()</span> <span class="o">==</span> <span class="n">stack_node</span><span class="o">.</span><span class="n">name</span>

<span class="c1"># Second operation does nothing</span>
<span class="n">stack_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>auto_unbind</strong> (<code class="docutils literal notranslate"><span class="pre">False</span></code> by default): This mode indicates whether the
operation <a class="reference internal" href="../operations.html#tensorkrowch.unbind" title="tensorkrowch.unbind"><code class="xref py py-func docutils literal notranslate"><span class="pre">unbind()</span></code></a> has to actually <cite>unbind</cite> the stacked tensor or just
generate a collection of references. That is, if <code class="docutils literal notranslate"><span class="pre">auto_unbind</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">False</span></code>, <a class="reference internal" href="../operations.html#tensorkrowch.unbind" title="tensorkrowch.unbind"><code class="xref py py-func docutils literal notranslate"><span class="pre">unbind()</span></code></a> creates a collection of nodes, each of them storing
the corresponding slice of the stacked tensor. If <code class="docutils literal notranslate"><span class="pre">auto_unbind</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>, <a class="reference internal" href="../operations.html#tensorkrowch.unbind" title="tensorkrowch.unbind"><code class="xref py py-func docutils literal notranslate"><span class="pre">unbind()</span></code></a> just creates the nodes and gives each of them an
index to reference the stacked tensor, so that each node’s tensor would be
retrieved by indexing the stack. This avoids performing the operation, since
these indices will be the same in subsequent iterations.</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">auto_unbind</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> will be faster for <strong>inference</strong>, but
slower for <strong>training</strong>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">TensorNetwork</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">auto_unbind</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">)</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

<span class="n">stack_node</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>

<span class="c1"># First operation is computed</span>
<span class="n">unbinded_nodes</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">stack_node</span><span class="p">)</span>

<span class="c1"># All unbinded nodes use a slice of the tensor in stack_node</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">unbinded_nodes</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">node</span><span class="o">.</span><span class="n">tensor_address</span><span class="p">()</span> <span class="o">==</span> <span class="n">stack_node</span><span class="o">.</span><span class="n">name</span>

<span class="c1"># Second operation does nothing</span>
<span class="n">unbinded_nodes</span> <span class="o">=</span> <span class="n">tk</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">stack_node</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>Once the training algorithm starts, these modes should not be changed (very
often at least), since changing them entails first <code class="xref py py-meth docutils literal notranslate"><span class="pre">resetting</span></code>
the whole network, which is a costly method.</p>
<p>To learn more about what <code class="docutils literal notranslate"><span class="pre">virtual</span></code> and other types of nodes are, check the
next <a class="reference internal" href="4_types_of_nodes.html#tutorial-4"><span class="std std-ref">tutorial</span></a>.</p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="2_contracting_tensor_network.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Contracting and Differentiating the Tensor Network</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4_types_of_nodes.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The different Types of Nodes (ADVANCED)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By José Ramón Pareja Monturiol<br/>
  
      &copy; Copyright 2023, José Ramón Pareja Monturiol.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>