{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the [documentation](https://joserapa98.github.io/tensorkrowch/_build/html/index.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this locally, use a conda/venv environment:\n",
    "\n",
    "- Create environment: conda create -n tutorial_tk python=3.12\n",
    "- Activate environment: conda activate tutorial_tk\n",
    "- Install packages:\n",
    "    - pip install torch torchvision\n",
    "    - pip install ipykernel\n",
    "    - pip install numpy (optional)\n",
    "    - pip install matplotlib (optional)\n",
    "    - pip install tensorkrowch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorkrowch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir data\n",
    "%mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "\n",
    "import tensorkrowch as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu121'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 11:22:55 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...    Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P5              27W /  80W |    450MiB /  8192MiB |     27%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1668      G   /usr/lib/xorg/Xorg                           45MiB |\n",
      "|    0   N/A  N/A      2427      G   /usr/lib/xorg/Xorg                          172MiB |\n",
      "|    0   N/A  N/A      2602      G   /usr/bin/gnome-shell                         34MiB |\n",
      "|    0   N/A  N/A      4121      G   ...seed-version=20240430-180109.710000       52MiB |\n",
      "|    0   N/A  N/A      5319      G   ...erProcess --variations-seed-version      118MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]],\n",
    "                         dtype=torch.float32,\n",
    "                         device=device)\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(2, 2)\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(3, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(4, 5)\n",
    "print(x)\n",
    "\n",
    "x = torch.eye(2, 4)\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x.device)\n",
    "print(x.dtype)\n",
    "print()\n",
    "\n",
    "x = x.to('cuda')       # x.cuda()\n",
    "x = x.to(torch.int32)  # x.int()\n",
    "print(x.device)\n",
    "print(x.dtype)\n",
    "\n",
    "# x.bool()\n",
    "# x.short()\n",
    "# x.long()\n",
    "# x.half()\n",
    "# x.float()\n",
    "# x.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.zeros((3, 3))\n",
    "print(np_array)\n",
    "\n",
    "tensor = torch.from_numpy(np_array)\n",
    "print(tensor)\n",
    "\n",
    "np_array_back = tensor.numpy()\n",
    "print(np_array_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change view of tensors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.view(6, 4).shape)\n",
    "print(x.view(2, 1, 3, 1, 4).shape)\n",
    "print(x.view(-1).shape)\n",
    "print(x.view(-1, 4).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.permute(1, 0, 2)\n",
    "print(y.shape)\n",
    "\n",
    "print(x.is_contiguous())\n",
    "print(y.is_contiguous())\n",
    "\n",
    "try:\n",
    "    y = y.view(6, 4)\n",
    "except:\n",
    "    print('It failed')\n",
    "    y = y.contiguous().view(6, 4)   \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.reshape(2, 3, 4).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor indexing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "features = 25\n",
    "x = torch.rand(batch_size, features)\n",
    "\n",
    "print(x[0].shape)   # x[0, :]\n",
    "print(x[:, 0].shape)\n",
    "\n",
    "print(x[2, 0:10])\n",
    "\n",
    "x[0, 0] = 100\n",
    "x[0, 1:3] = 20\n",
    "print(x[0, 0:10])\n",
    "\n",
    "x[0, :] = torch.arange(0, 25)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10)\n",
    "indices = [2, 5, 8]\n",
    "print(x[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 5)\n",
    "rows = torch.tensor([1, 0])\n",
    "cols = torch.tensor([4, 0])\n",
    "print(x, x[rows, cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10)\n",
    "print(x[(x < 2) | (x > 8)])\n",
    "print(x[(x < 2) & (x > 8)])\n",
    "print(x[x.remainder(2) == 0])\n",
    "\n",
    "print(torch.where(x > 5, x, x * 2))\n",
    "print(torch.tensor([0, 0, 1, 2, 2, 3, 4]).unique())\n",
    "print(x.ndimension()) # 5x5x5 -> 3\n",
    "print(x.numel()) # number of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat / stack\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(3, 3)\n",
    "x2 = torch.randn(3, 3)\n",
    "print(x1)\n",
    "print(x2)\n",
    "\n",
    "x3 = torch.cat([x1, x2], dim=0)\n",
    "print(x3, x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = torch.stack([x1, x2], dim=0)\n",
    "print(x3, x3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([9, 8, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition\n",
    "z1 = torch.empty(3)\n",
    "print(z1)\n",
    "torch.add(x, y, out=z1)\n",
    "print(z1)\n",
    "\n",
    "z2 = torch.add(x, y)\n",
    "print(z2)\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# Subtraction\n",
    "z = x - y\n",
    "print(z)\n",
    "\n",
    "# Division\n",
    "z = torch.true_divide(x, y) # element-wise division\n",
    "print(z)\n",
    "\n",
    "# In-place operations\n",
    "# Any operation followed by underscore means inplace operation\n",
    "# (does not create new tensor, just replaces t)\n",
    "t = torch.zeros(3)\n",
    "print(t)\n",
    "t.add_(x)\n",
    "print(t)\n",
    "\n",
    "t = t + x # Not in-place\n",
    "t += x # In-place\n",
    "\n",
    "# Exponentiation\n",
    "z = x.pow(2)\n",
    "print(z)\n",
    "\n",
    "z = x ** 2\n",
    "print(z)\n",
    "\n",
    "# Comparison\n",
    "z = x > 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise multiplication\n",
    "z = x * y\n",
    "print(z)\n",
    "\n",
    "# Dot product\n",
    "z = torch.dot(x, y)\n",
    "print(z)\n",
    "print(x.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "x1 = torch.rand(2, 5)\n",
    "x2 = torch.rand(5, 3)\n",
    "\n",
    "x3 = torch.mm(x1, x2)\n",
    "print(x3)\n",
    "x3 = x1.mm(x2)\n",
    "print(x3)\n",
    "\n",
    "\n",
    "# Batch matrix multiplication\n",
    "batch = 32\n",
    "n = 10\n",
    "m = 20\n",
    "p = 30\n",
    "\n",
    "tensor1 = torch.rand(batch, n, m)\n",
    "tensor2 = torch.rand(batch, m, p)\n",
    "out = torch.bmm(tensor1, tensor2) # (batch, n, p)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einsum\n",
    "x = torch.randn(2, 3, 4)\n",
    "y = torch.randn(2, 7, 1, 4)\n",
    "z = torch.einsum('bmn,bijn->bmi', x, y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Trainable* tensors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.sum())\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Parameter(torch.randn(2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bars and stripes dataset\n",
    "dataset_name = 'bars_stripes'\n",
    "batch_size = 64\n",
    "sq_root_size = 5\n",
    "input_size = sq_root_size ** 2\n",
    "dataset_size = int(1e4)\n",
    "num_classes = 2\n",
    "\n",
    "# Create dataset\n",
    "def create_dataset(size, sq_root_size):\n",
    "    tensors = []\n",
    "    labels = []\n",
    "    for _ in range(size):\n",
    "        while True:\n",
    "            t = torch.randint(low=0, high=2, size=(1, sq_root_size))  # stripes\n",
    "            t = t.expand(sq_root_size, -1)\n",
    "            if not torch.equal(t, torch.zeros_like(t)) \\\n",
    "                and not torch.equal(t, torch.ones_like(t)):\n",
    "                    break\n",
    "        \n",
    "        if torch.rand(1) < 0.5:\n",
    "            # stripes\n",
    "            tensors.append(t)\n",
    "            labels.append(1)\n",
    "            \n",
    "        else:\n",
    "            # bars\n",
    "            tensors.append(t.t())\n",
    "            labels.append(0)\n",
    "    \n",
    "    tensors = torch.stack(tensors, dim=0).view(size,\n",
    "                                               1,\n",
    "                                               sq_root_size,\n",
    "                                               sq_root_size)\n",
    "    labels = torch.Tensor(labels).long()\n",
    "    \n",
    "    return tensors.float(), labels\n",
    "\n",
    "dataset = create_dataset(dataset_size, sq_root_size)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = TensorDataset(dataset[0][:int(0.8 * dataset_size)],\n",
    "                              dataset[1][:int(0.8 * dataset_size)])\n",
    "test_dataset = TensorDataset(dataset[0][int(0.8 * dataset_size):],\n",
    "                             dataset[1][int(0.8 * dataset_size):])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset\n",
    "dataset_name = 'mnist'\n",
    "batch_size = 64\n",
    "input_size = 28 * 28  # 784\n",
    "num_classes = 10\n",
    "\n",
    "# Load data\n",
    "train_dataset = datasets.MNIST(root='data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "test_dataset = datasets.MNIST(root='data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FashionMNIST Dataset\n",
    "dataset_name = 'fashion_mnist'\n",
    "batch_size = 64\n",
    "input_size = 28 * 28  # 784\n",
    "num_classes = 10\n",
    "\n",
    "# Load data\n",
    "train_dataset = datasets.FashionMNIST(root='data/',\n",
    "                                      train=True,\n",
    "                                      transform=transforms.ToTensor(),\n",
    "                                      download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='data/',\n",
    "                                     train=False,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "random_sample = torch.randint(low=0, high=len(train_dataset), size=(1,)).item()\n",
    "\n",
    "plt.imshow(train_dataset[random_sample][0].squeeze(0), cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "print(train_dataset[random_sample][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components of Neural Networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "linear = nn.Linear(10, 30)\n",
    "print(linear.weight.shape, linear.bias.shape)\n",
    "\n",
    "x = torch.randn(100, 10)\n",
    "result1 = x @ linear.weight.t() + linear.bias\n",
    "result2 = linear(x)\n",
    "\n",
    "print(result1.shape, result2.shape)\n",
    "print((result1 - result2).norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer\n",
    "conv = nn.Conv2d(5, 10, kernel_size=3, stride=3, padding=0)\n",
    "print(conv.weight.shape, conv.bias.shape)\n",
    "\n",
    "x = torch.randn(100, 5, 9, 9)\n",
    "result1 = conv(x)\n",
    "\n",
    "aux_x = torch.randn(100, 9, 5, 3, 3)\n",
    "result2 = torch.einsum('bncwh,ocwh->bon', aux_x, conv.weight)\n",
    "result2 = result2.reshape(100, 10, 3, 3)\n",
    "result2 = result2 + conv.bias.view(1, -1, 1, 1)\n",
    "\n",
    "print(result1.shape, result2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(sigmoid(torch.tensor([1e-5, 1e-1, 0.5, 1e1, 1e5])))\n",
    "\n",
    "relu = nn.ReLU()\n",
    "print(relu(torch.tensor([1., 4., -3, 5., -1])))\n",
    "\n",
    "softmax = nn.Softmax(dim=0)\n",
    "print(softmax(torch.tensor([1e-5, -2e1, 4e-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFFC(nn.Module):  # Feed forward fully connected\n",
    "    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # Here we define the elements and structure of the Neural Network\n",
    "        \n",
    "        super().__init__() # super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # We could add other custom parameters\n",
    "        # self.my_param = nn.Parameter(torch.randn(10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Here we specify how the components of the NN are combined to\n",
    "        # return a certain output\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "model = FFFC(input_size=input_size, num_classes=num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 5\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, model)\n",
    "    test_acc = check_accuracy(test_loader, model)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "torch.save(model.state_dict(), f'models/fffc_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model):\n",
    "    n = 0\n",
    "    for p in model.parameters():\n",
    "        n += p.numel()\n",
    "    return n\n",
    "\n",
    "n = n_params(model)\n",
    "test_acc = check_accuracy(test_loader, model)\n",
    "test_acc, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of TensorKrowch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = tk.Node(shape=(2, 5, 2))\n",
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititalize with tensor\n",
    "tensor = torch.randn(2, 5, 2)\n",
    "node = tk.Node(tensor=tensor)\n",
    "\n",
    "# Inititalize with init_method\n",
    "node = tk.Node(shape=(2, 5, 2),\n",
    "               init_method='randn')\n",
    "\n",
    "node = tk.randn(shape=(2, 5, 2),\n",
    "                axes_names=('left', 'input', 'right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(node.shape)   # Returns node's shape\n",
    "print(node.tensor)  # Returns node's tensor\n",
    "print(node.axes)    # Returns node's axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tk.TensorNetwork(name='my_network')\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(net, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = tk.Node(shape=(100, 2, 5, 2),\n",
    "                name='my_node',\n",
    "                axes_names=('batch', 'left', 'input', 'right'),\n",
    "                network=net)\n",
    "node1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2 = tk.Node(shape=(100, 2, 5, 2),\n",
    "                name='other_node',\n",
    "                axes_names=('batch', 'left', 'input', 'right'),\n",
    "                network=net)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(node1['right'].is_dangling())  # Indicates if the edge is not connected\n",
    "print(node1['batch'].is_batch())     # Indicates if the edge is a batch edge\n",
    "\n",
    "print(node1['input'].size())         # Returns the shape of the node in that axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1['right'] ^ node2['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1['right'] | node2['left']  # node1['right'].disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = tk.TensorNetwork(name='mps')\n",
    "\n",
    "for i in range(100):\n",
    "    _ = tk.randn(shape=(2, 5, 2),\n",
    "                 axes_names=('left', 'input', 'right'),\n",
    "                 name=f'node_({i})',\n",
    "                 network=mps)\n",
    "\n",
    "for i in range(100):\n",
    "    mps[f'node_({i})']['right'] ^ mps[f'node_({(i + 1) % 100})']['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps.nodes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Trainable* nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramnode1 = tk.ParamNode(shape=(2, 5, 2))     # Empty paramnode\n",
    "paramnode2 = tk.ParamNode(shape=(2, 5, 2),\n",
    "                          init_method='randn')\n",
    "paramnode3 = tk.randn(shape=(2, 5, 2),\n",
    "                      param_node=True)  # Indicates if node is ParamNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(2, 5, 2)\n",
    "paramnode = tk.ParamNode(tensor=tensor,\n",
    "                         name='my_paramnode')\n",
    "\n",
    "isinstance(paramnode.tensor, nn.Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in paramnode.network.named_parameters():\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = paramnode.parameterize(False)\n",
    "assert isinstance(node.tensor, torch.Tensor)\n",
    "assert not isinstance(node.tensor, nn.Parameter)\n",
    "\n",
    "print('Node in parameters?')\n",
    "for n, p in node.network.named_parameters():\n",
    "    print(n, p)\n",
    "\n",
    "paramnode = node.parameterize()\n",
    "assert isinstance(paramnode.tensor, nn.Parameter)\n",
    "\n",
    "print('Node in parameters?')\n",
    "for n, p in paramnode.network.named_parameters():\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = paramnode.sum()  # Sums over all axes of the node\n",
    "sum.backward()         # Differentiates sum with respect to paramnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramnode.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor-like operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = tk.randn(shape=(2, 3),\n",
    "                 axes_names=('left', 'right'))\n",
    "node2 = tk.randn(shape=(2, 3),\n",
    "                 axes_names=('left', 'right'))\n",
    "node2.move_to_network(node1.network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication\n",
    "node3 = node1 * node2\n",
    "\n",
    "# Division\n",
    "node4 = node1 / node2\n",
    "\n",
    "# Addition\n",
    "node5 = node1 + node2\n",
    "\n",
    "# Subtraction\n",
    "node6 = node1 - node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renormalize\n",
    "node7 = node1.renormalize()\n",
    "node1.norm(), node7.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1.network.resultant_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node-like operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contract_between\n",
    "node1 = tk.randn(shape=(100, 2, 3),\n",
    "                 axes_names=('batch', 'left', 'right'))\n",
    "node2 = tk.randn(shape=(100, 3, 4),\n",
    "                 axes_names=('batch', 'left', 'right'))\n",
    "\n",
    "assert node1.network != node2.network\n",
    "# node2.move_to_network(node1.network)\n",
    "\n",
    "node1['right'] ^ node2['left']\n",
    "\n",
    "assert node1.network == node2.network\n",
    "\n",
    "result = node1 @ node2\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "node = tk.randn(shape=(2, 3, 4, 5),\n",
    "                axes_names=('left1', 'left2', 'right1', 'right2'))\n",
    "res1, res2 = node.split(['left1', 'right1'],\n",
    "                        ['left2', 'right2'],\n",
    "                        rank=2)\n",
    "\n",
    "print(res1.shape, res2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack\n",
    "net = tk.TensorNetwork()\n",
    "nodes = []\n",
    "\n",
    "for i in range(100):\n",
    "    node = tk.randn(shape=(2, 5, 2),\n",
    "                    axes_names=('left', 'input', 'right'),\n",
    "                    network=net,\n",
    "                    name=f'node_({i})')\n",
    "    nodes.append(node)\n",
    "\n",
    "stack_node = tk.stack(nodes)\n",
    "print(stack_node.shape)\n",
    "\n",
    "stack_node['stack'].is_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacks need to be reconnected\n",
    "net = tk.TensorNetwork()\n",
    "nodes = []\n",
    "data_nodes = []\n",
    "\n",
    "for i in range(100):\n",
    "    node = tk.randn(shape=(2, 5, 2),\n",
    "                    axes_names=('left', 'input', 'right'),\n",
    "                    network=net,\n",
    "                    name=f'node_({i})')\n",
    "    nodes.append(node)\n",
    "\n",
    "    data_node = tk.randn(shape=(100, 5),\n",
    "                    axes_names=('batch', 'feature'),\n",
    "                    network=net,\n",
    "                    name=f'data_node_({i})')\n",
    "    data_nodes.append(data_node)\n",
    "\n",
    "    node['input'] ^ data_node['feature']\n",
    "\n",
    "stack_node = tk.stack(nodes)\n",
    "stack_data_node = tk.stack(data_nodes)\n",
    "\n",
    "print(stack_node.is_connected_to(stack_data_node))\n",
    "\n",
    "# reconnect stacks\n",
    "stack_node ^ stack_data_node\n",
    "\n",
    "print(stack_node.is_connected_to(stack_data_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbind\n",
    "net = tk.TensorNetwork()\n",
    "nodes = []\n",
    "\n",
    "for i in range(100):\n",
    "    node = tk.randn(shape=(2, 5, 2),\n",
    "                    axes_names=('left', 'input', 'right'),\n",
    "                    network=net,\n",
    "                    name=f'node_({i})')\n",
    "    nodes.append(node)\n",
    "\n",
    "stack_node = tk.stack(nodes)\n",
    "unbound_nodes = tk.unbind(stack_node)  # stack_node.unbind()\n",
    "\n",
    "len(unbound_nodes), unbound_nodes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einsum\n",
    "node1 = tk.randn(shape=(10, 15, 100),\n",
    "                 axes_names=('left', 'right', 'batch'))\n",
    "node2 = tk.randn(shape=(15, 7, 100),\n",
    "                 axes_names=('left', 'right', 'batch'))\n",
    "node3 = tk.randn(shape=(7, 10, 100),\n",
    "                 axes_names=('left', 'right', 'batch'))\n",
    "\n",
    "node1['right'] ^ node2['left']\n",
    "node2['right'] ^ node3['left']\n",
    "node3['right'] ^ node1['left']\n",
    "\n",
    "result = tk.einsum('ijb,jkb,kib->b', node1, node2, node3)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these operations, we can contract the whole tensor network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = tk.TensorNetwork(name='mps')\n",
    "nodes = []\n",
    "data_nodes = []\n",
    "\n",
    "for i in range(100):\n",
    "    node = tk.randn(shape=(2, 5, 2),\n",
    "                    axes_names=('left', 'input', 'right'),\n",
    "                    network=mps,\n",
    "                    name=f'node_({i})',\n",
    "                    param_node=True)\n",
    "    nodes.append(node)\n",
    "\n",
    "    data_node = tk.randn(shape=(5,),\n",
    "                         axes_names=('feature',),\n",
    "                         network=mps,\n",
    "                         data=True,\n",
    "                         name=f'data_node_({i})')\n",
    "    data_nodes.append(data_node)\n",
    "\n",
    "    node['input'] ^ data_node['feature']\n",
    "\n",
    "for i in range(100):\n",
    "    mps[f'node_({i})']['right'] ^ mps[f'node_({(i + 1) % 100})']['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps.data_nodes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_node = tk.stack(nodes)\n",
    "stack_data_node = tk.stack(data_nodes)\n",
    "\n",
    "stack_node ^ stack_data_node\n",
    "\n",
    "stack_result = stack_node @ stack_data_node\n",
    "unbind_result = tk.unbind(stack_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = unbind_result[0]\n",
    "for node in unbind_result[1:]:\n",
    "    result @= node\n",
    "\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tensor.backward()\n",
    "\n",
    "for node in nodes:\n",
    "    assert node.grad is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps.resultant_nodes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps.unset_data_nodes()\n",
    "mps.data_nodes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps.reset()\n",
    "mps.resultant_nodes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the first TensorNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPS(tk.TensorNetwork):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 bond_dim,\n",
    "                 uniform=False):\n",
    "        \"\"\"\n",
    "        In the __init__ method we define the tensor network\n",
    "        structure and initialize all the nodes\n",
    "        \"\"\"\n",
    "        super().__init__(name='mps')\n",
    "\n",
    "        #############\n",
    "        # Create TN #\n",
    "        #############\n",
    "        input_nodes = []\n",
    "\n",
    "        # Number of input nodes equal to number of features\n",
    "        for _ in range(input_size):\n",
    "            node = tk.ParamNode(shape=(bond_dim, input_dim, bond_dim),\n",
    "                                axes_names=('left', 'input', 'right'),\n",
    "                                name='input_node',\n",
    "                                network=self)\n",
    "            input_nodes.append(node)\n",
    "\n",
    "        for i in range(len(input_nodes) - 1):\n",
    "            input_nodes[i]['right'] ^ input_nodes[i + 1]['left']\n",
    "\n",
    "        # Output node is in the last position,\n",
    "        # but that could be changed\n",
    "        output_node = tk.ParamNode(shape=(bond_dim, output_dim, bond_dim),\n",
    "                                   axes_names=(\n",
    "                                       'left', 'output', 'right'),\n",
    "                                   name='output_node',\n",
    "                                   network=self)\n",
    "        output_node['right'] ^ input_nodes[0]['left']\n",
    "        output_node['left'] ^ input_nodes[-1]['right']\n",
    "\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_node = output_node\n",
    "\n",
    "        # If desired, the MPS can be uniform\n",
    "        if uniform:\n",
    "            uniform_memory = tk.ParamNode(shape=(bond_dim, input_dim, bond_dim),\n",
    "                                          axes_names=(\n",
    "                                              'left', 'input', 'right'),\n",
    "                                          name='virtual_uniform',\n",
    "                                          network=self,\n",
    "                                          virtual=True)\n",
    "            self.uniform_memory = uniform_memory\n",
    "\n",
    "        ####################\n",
    "        # Initialize Nodes #\n",
    "        ####################\n",
    "\n",
    "        # Input nodes\n",
    "        if uniform:\n",
    "            std = 1e-9\n",
    "            tensor = torch.randn(uniform_memory.shape) * std\n",
    "            random_eye = torch.randn(\n",
    "                tensor.shape[0], tensor.shape[2]) * std\n",
    "            random_eye = random_eye + \\\n",
    "                torch.eye(tensor.shape[0], tensor.shape[2])\n",
    "            tensor[:, 0, :] = random_eye\n",
    "\n",
    "            uniform_memory.tensor = tensor\n",
    "\n",
    "            # Memory of each node is just a reference\n",
    "            # to the uniform_memory tensor\n",
    "            for node in input_nodes:\n",
    "                node.set_tensor_from(self.uniform_memory)\n",
    "\n",
    "        else:\n",
    "            std = 1e-9\n",
    "            for node in input_nodes:\n",
    "                tensor = torch.randn(node.shape) * std\n",
    "                random_eye = torch.randn(\n",
    "                    tensor.shape[0], tensor.shape[2]) * std\n",
    "                random_eye = random_eye + \\\n",
    "                    torch.eye(tensor.shape[0], tensor.shape[2])\n",
    "                tensor[:, 0, :] = random_eye\n",
    "\n",
    "                node.tensor = tensor\n",
    "\n",
    "        # Output node\n",
    "        eye_tensor = torch.eye(output_node.shape[0], output_node.shape[2])\\\n",
    "            .view([output_node.shape[0], 1, output_node.shape[2]])\n",
    "        eye_tensor = eye_tensor.expand(output_node.shape)\n",
    "        tensor = eye_tensor + std * torch.randn(output_node.shape)\n",
    "\n",
    "        output_node.tensor = tensor\n",
    "\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_node = output_node\n",
    "\n",
    "    def set_data_nodes(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is optional. If overriden, it should not have\n",
    "        arguments other than ``self``. Furthermore, we won't have to\n",
    "        call it explicitly, since it will be called from ``forward``.\n",
    "\n",
    "        If not overriden, it should be explicitly called before\n",
    "        training.\n",
    "        \"\"\"\n",
    "        # Select input edges where to put data nodes\n",
    "        input_edges = []\n",
    "        for node in self.input_nodes:\n",
    "            input_edges.append(node['input'])\n",
    "\n",
    "        # num_batch_edges inicates number of batch edges. Usually\n",
    "        # it will be 1, for the batch of input data. But for\n",
    "        # convolutional or sequential models it could be 2,\n",
    "        # one edge for the batch of input data, and one for the\n",
    "        # patches of sequence\n",
    "        super().set_data_nodes(input_edges,\n",
    "                               num_batch_edges=1)\n",
    "    \n",
    "    # Optionally, the method ``add_data`` can also be overriden\n",
    "    # def add_data(self, data) -> None:\n",
    "    #     pass\n",
    "\n",
    "    def contract(self):\n",
    "        \"\"\"\n",
    "        In this method we define the contraction algorithm\n",
    "        for the tensor network.\n",
    "\n",
    "        The last operation computed must be the one returning\n",
    "        the final node.\n",
    "        \"\"\"\n",
    "        stack_input = tk.stack(self.input_nodes)\n",
    "        stack_data = tk.stack(list(self.data_nodes.values()))\n",
    "\n",
    "        stack_input ^ stack_data\n",
    "        stack_result = stack_input @ stack_data\n",
    "\n",
    "        stack_result = tk.unbind(stack_result)\n",
    "\n",
    "        result = stack_result[0]\n",
    "        for node in stack_result[1:]:\n",
    "            result @= node\n",
    "        result @= self.output_node\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "output_dim = num_classes\n",
    "bond_dim = 10\n",
    "\n",
    "mps = MPS(input_size=input_size,\n",
    "          input_dim=input_dim,\n",
    "          output_dim=output_dim,\n",
    "          bond_dim=bond_dim)\n",
    "mps = mps.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tk.embeddings.poly(train_dataset[0][0].view(1, -1),\n",
    "                             degree=input_dim - 1)\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = mps(example.to(device))\n",
    "\n",
    "# What happens under the hood\n",
    "mps.add_data(example.to(device))\n",
    "result2 = mps.contract().tensor\n",
    "\n",
    "assert result1.allclose(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model to accelerate training\n",
    "mps.trace(example.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 5\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mps.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(tk.embeddings.poly(x, degree=input_dim - 1))\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = mps(tk.embeddings.poly(data, degree=input_dim - 1))\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, mps)\n",
    "    test_acc = check_accuracy(test_loader, mps)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "# Reset before saving the model\n",
    "mps.reset()\n",
    "torch.save(mps.state_dict(), f'models/custom_mps_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "mps = MPS(input_size=input_size,\n",
    "          input_dim=input_dim,\n",
    "          output_dim=output_dim,\n",
    "          bond_dim=bond_dim)\n",
    "mps = mps.to(device)\n",
    "mps.load_state_dict(torch.load(f'models/custom_mps_{dataset_name}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Stoudenmire, Edwin, and David J. Schwab. \"Supervised learning with tensor networks.\" Advances in neural information processing systems 29 (2016).\n",
    "- Novikov, Alexander, Mikhail Trofimov, and Ivan Oseledets. \"Exponential machines.\" arXiv preprint arXiv:1605.03795 (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embedding_dim = 3\n",
    "output_dim = num_classes\n",
    "bond_dim = 10\n",
    "init_method = 'randn_eye' # rand, randn, randn_eye, canonical, unit\n",
    "\n",
    "# Contraction options\n",
    "inline_input = False\n",
    "inline_mats = False\n",
    "renormalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "mps = tk.models.MPSLayer(n_features=input_size + 1,\n",
    "                         in_dim=embedding_dim,\n",
    "                         out_dim=num_classes,\n",
    "                         bond_dim=bond_dim,\n",
    "                         boundary='obc',\n",
    "                         init_method=init_method,\n",
    "                         std=1e-9,\n",
    "                         device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one embedding, and adjust the `init_method` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.poly(x, degree=embedding_dim - 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.unit(x, dim=embedding_dim)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.discretize(x, base=embedding_dim, level=1).squeeze(-1).int()\n",
    "    x = tk.embeddings.basis(x, dim=embedding_dim).float() # batch x n_features x dim\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model to accelerate training\n",
    "mps.trace(torch.zeros(1, input_size, embedding_dim, device=device),\n",
    "          inline_input=inline_input,\n",
    "          inline_mats=inline_mats,\n",
    "          renormalize=renormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 5\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mps.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(embedding(x),\n",
    "                           inline_input=inline_input,\n",
    "                           inline_mats=inline_mats,\n",
    "                           renormalize=renormalize)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = mps(embedding(data),\n",
    "                     inline_input=inline_input,\n",
    "                     inline_mats=inline_mats,\n",
    "                     renormalize=renormalize)\n",
    "        # print(scores[0])\n",
    "        # break\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, mps)\n",
    "    test_acc = check_accuracy(test_loader, mps)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "# Reset before saving the model\n",
    "mps.reset()\n",
    "torch.save(mps.state_dict(), f'models/mps_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model):\n",
    "    n = 0\n",
    "    for p in model.parameters():\n",
    "        n += p.numel()\n",
    "    return n\n",
    "\n",
    "n = n_params(mps)\n",
    "test_acc = check_accuracy(test_loader, mps)\n",
    "test_acc, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network\n",
    "mps = tk.models.MPSLayer(n_features=input_size + 1,\n",
    "                         in_dim=embedding_dim,\n",
    "                         out_dim=num_classes,\n",
    "                         bond_dim=bond_dim,\n",
    "                         boundary='obc',\n",
    "                         init_method=init_method,\n",
    "                         std=1e-1,\n",
    "                         device=device)\n",
    "mps.load_state_dict(torch.load(f'models/mps_{dataset_name}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mps.canonicalize(renormalize=True, mode='qr')\n",
    "mps.canonicalize(cum_percentage=0.9, renormalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_params(mps)\n",
    "test_acc = check_accuracy(test_loader, mps)\n",
    "test_acc, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model to accelerate training\n",
    "mps.trace(torch.zeros(1, input_size, embedding_dim, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 5\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mps.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = mps(embedding(data), renormalize=True)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, mps)\n",
    "    test_acc = check_accuracy(test_loader, mps)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "mps.reset()\n",
    "# torch.save(mps.state_dict(), f'models/mps_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability with MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Ran, Shi-Ju, and Gang Su. \"Tensor networks for interpretable and efficient quantum-inspired machine learning.\" Intelligent Computing 2 (2023): 0061.\n",
    "- Aizpurua, Borja, and Roman Orus. \"Tensor Networks for Explainable Machine Learning in Cybersecurity.\" arXiv preprint arXiv:2401.00867 (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BornMachine(nn.Module):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mps = tk.models.MPSLayer(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, data, *args, **kwargs):\n",
    "        result = self.mps.forward(data, *args, **kwargs)\n",
    "        result = result.pow(2)\n",
    "        result = result / result.norm(dim=1, keepdim=True) # Probabilities\n",
    "        result = result.log() # Log-Probabilities\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embedding_dim = 2\n",
    "output_dim = num_classes\n",
    "bond_dim = 10\n",
    "init_method = 'unit' # rand, randn, randn_eye, canonical, unit\n",
    "\n",
    "# Contraction options\n",
    "inline_input = False\n",
    "inline_mats = False\n",
    "renormalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "born_machine = BornMachine(n_features=input_size + 1,\n",
    "                           in_dim=embedding_dim,\n",
    "                           out_dim=num_classes,\n",
    "                           bond_dim=bond_dim,\n",
    "                           boundary='obc',\n",
    "                           init_method=init_method,\n",
    "                           std=1e-9,\n",
    "                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.basis(x.int(), dim=embedding_dim).float() # batch x n_features x dim\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model to accelerate training\n",
    "born_machine.mps.trace(torch.zeros(1, input_size, embedding_dim, device=device),\n",
    "                       inline_input=inline_input,\n",
    "                       inline_mats=inline_mats,\n",
    "                       renormalize=renormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-6\n",
    "num_epochs = 5\n",
    "print_epochs = len(train_loader) // 5\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(born_machine.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(embedding(x),\n",
    "                           inline_input=inline_input,\n",
    "                           inline_mats=inline_mats,\n",
    "                           renormalize=renormalize)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = born_machine(embedding(data),\n",
    "                              inline_input=inline_input,\n",
    "                              inline_mats=inline_mats,\n",
    "                              renormalize=renormalize)\n",
    "        # print(scores[0])\n",
    "        # break\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, born_machine)\n",
    "    test_acc = check_accuracy(test_loader, born_machine)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "born_machine.mps.reset()\n",
    "torch.save(born_machine.state_dict(), f'models/born_machine_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Von Neumann Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network\n",
    "born_machine = BornMachine(n_features=input_size + 1,\n",
    "                           in_dim=embedding_dim,\n",
    "                           out_dim=num_classes,\n",
    "                           bond_dim=bond_dim,\n",
    "                           boundary='obc',\n",
    "                           init_method=init_method,\n",
    "                           std=1e-9,\n",
    "                           device=device)\n",
    "born_machine.load_state_dict(torch.load(f'models/born_machine_{dataset_name}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy, log_norm = born_machine.mps.entropy(middle_site=10, renormalize=True)\n",
    "entropy, log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "for i in range(born_machine.mps.n_features - 1):\n",
    "    entropy, _ = born_machine.mps.entropy(middle_site=i, renormalize=True)\n",
    "    entropies.append(entropy)\n",
    "\n",
    "plt.bar(list(range(born_machine.mps.n_features - 1)),\n",
    "        torch.stack(entropies, dim=0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "born_machine.mps.out_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(range(born_machine.mps.n_features - 1)),\n",
    "        born_machine.mps.bond_dim)\n",
    "\n",
    "born_machine.mps.canonicalize(cum_percentage=0.8, renormalize=True)\n",
    "plt.bar(list(range(born_machine.mps.n_features - 1)),\n",
    "        born_machine.mps.bond_dim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the Mutual Information of the data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensors = []\n",
    "for sample in train_dataset:\n",
    "    data_tensors.append(sample[0])\n",
    "\n",
    "data_tensors = torch.stack(data_tensors, dim=0).view(-1, input_size)\n",
    "data_tensors = data_tensors.unique(dim=0)\n",
    "data_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantum Mutual Information: $S(A:B) = S(\\rho_A) + S(\\rho_B) - S(\\rho_{AB})$,\n",
    "where $S$ is the von Neumann entropy.\n",
    "\n",
    "Classical Mutual Information: $I(A:B) = H(p(a)) + H(p(b)) - H(p(a, b))$,\n",
    "where $H$ is the Shannon entropy.\n",
    "\n",
    "$I(A:B) \\le S(A:B)$, and $S(A:B) = 2S(\\rho_A)$ for pure states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_mi(i):\n",
    "    N = data_tensors.shape[0]\n",
    "    left = data_tensors[:, :i].unique(dim=0)\n",
    "    right = data_tensors[:, i:].unique(dim=0)\n",
    "    \n",
    "    left = left.unsqueeze(1).expand(-1, right.shape[0], -1)\n",
    "    right = right.unsqueeze(0).expand(left.shape[0], -1, -1)\n",
    "    cross_data = torch.cat([left, right], dim=2).view(-1, input_size)\n",
    "    M = cross_data.shape[0]\n",
    "    \n",
    "    mi = 0\n",
    "    for sample in cross_data:\n",
    "        if sample in data_tensors:\n",
    "            mi += (1 / N) * log(M / N)\n",
    "    \n",
    "    return mi   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_mis = []\n",
    "for i in range(1, data_tensors.shape[1] - 1):\n",
    "    mi = classical_mi(i)\n",
    "    lst_mis.append(mi)\n",
    "\n",
    "plt.bar(list(range(1, data_tensors.shape[1] - 1)),\n",
    "        torch.tensor(lst_mis))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial density (and MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "trace_sites = list(set(range(born_machine.mps.n_features)).difference({i}))\n",
    "\n",
    "born_machine.mps.unset_data_nodes()\n",
    "born_machine.mps.reset()\n",
    "rho = born_machine.mps.reduced_density(trace_sites=trace_sites,\n",
    "                                       renormalize=True)\n",
    "rho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# von Neumann entropy between the region we traced out and the single free site\n",
    "u, s, vh = torch.linalg.svd(rho)\n",
    "s = s / s.norm(p=1)\n",
    "entropy = -(s * s.log()).sum()\n",
    "s, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_site_entropy(i):\n",
    "    trace_sites = list(set(range(born_machine.mps.n_features)).difference({i}))\n",
    "    \n",
    "    born_machine.mps.unset_data_nodes()\n",
    "    born_machine.mps.reset()\n",
    "    rho = born_machine.mps.reduced_density(trace_sites=trace_sites,\n",
    "                                           renormalize=True)\n",
    "    \n",
    "    _, s, _ = torch.linalg.svd(rho)\n",
    "    s = s[s > 0]\n",
    "    s = s / s.norm(p=1)\n",
    "    entropy = -(s * s.log()).sum()\n",
    "    return entropy.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "for i in range(born_machine.mps.n_features):\n",
    "    entropy = single_site_entropy(i)\n",
    "    entropies.append(entropy)\n",
    "\n",
    "plt.bar(list(range(born_machine.mps.n_features)),\n",
    "        torch.stack(entropies, dim=0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max entropy\n",
    "torch.tensor(embedding_dim).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the Mutual Information of the data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensors = []\n",
    "for sample in train_dataset:\n",
    "    data_tensors.append(sample[0])\n",
    "\n",
    "data_tensors = torch.stack(data_tensors, dim=0).view(-1, input_size)\n",
    "data_tensors = data_tensors.unique(dim=0)\n",
    "data_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensors[:, 10].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantum Mutual Information: $S(A:B) = S(\\rho_A) + S(\\rho_B) - S(\\rho_{AB})$,\n",
    "where $S$ is the von Neumann entropy.\n",
    "\n",
    "Classical Mutual Information: $I(A:B) = H(p(a)) + H(p(b)) - H(p(a, b))$,\n",
    "where $H$ is the Shannon entropy.\n",
    "\n",
    "$I(A:B) \\le S(A:B)$, and $S(A:B) = 2S(\\rho_A)$ for pure states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_site_classical_mi(i):\n",
    "    N = data_tensors.shape[0]\n",
    "    site = data_tensors[:, i:(i + 1)].unique(dim=0)\n",
    "    rest = torch.cat([data_tensors[:, :i],\n",
    "                      data_tensors[:, (i + 1):]], dim=1).unique(dim=0)\n",
    "    \n",
    "    site = site.unsqueeze(1).expand(-1, rest.shape[0], -1)\n",
    "    rest = rest.unsqueeze(0).expand(site.shape[0], -1, -1)\n",
    "    cross_data = torch.cat([site, rest], dim=2).view(-1, input_size)\n",
    "    M = cross_data.shape[0]\n",
    "    \n",
    "    mi = 0\n",
    "    for sample in cross_data:\n",
    "        if sample in data_tensors:\n",
    "            mi += (1 / N) * log(M / N)\n",
    "    \n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_mis = []\n",
    "for i in range(data_tensors.shape[1]):\n",
    "    mi = single_site_classical_mi(i)\n",
    "    lst_mis.append(mi)\n",
    "\n",
    "plt.bar(list(range(data_tensors.shape[1])),\n",
    "        torch.tensor(lst_mis))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Han, Zhao-Yu, et al. \"Unsupervised generative modeling using matrix product states.\" Physical Review X 8.3 (2018): 031012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, embedding, label):\n",
    "    sampled_features = []\n",
    "    for i in range(model.n_features):\n",
    "        \n",
    "        if i == model.out_position:\n",
    "            continue\n",
    "        \n",
    "        model.reset()\n",
    "        model.unset_data_nodes()\n",
    "        model.in_features = list(range(i + 1)) + [model.out_position]\n",
    "        \n",
    "        # Create auxiliary data with the already generated samples and\n",
    "        # the possible values at the new site\n",
    "        prev_features = torch.tensor(\n",
    "            sampled_features,\n",
    "            device=device).expand(model.phys_dim[i], -1)\n",
    "        \n",
    "        new_data = torch.arange(model.phys_dim[i],\n",
    "                                device=device).view(-1, 1)\n",
    "        \n",
    "        tensor_label = torch.tensor(\n",
    "            label,\n",
    "            device=device).view(1, 1).expand(model.phys_dim[i], -1)\n",
    "        \n",
    "        if i < model.out_position:\n",
    "            aux_data = torch.cat([prev_features,\n",
    "                                  new_data,\n",
    "                                  tensor_label], dim=1)\n",
    "        else:\n",
    "            aux_data = torch.cat([prev_features[:, :model.out_position],\n",
    "                                  tensor_label,\n",
    "                                  prev_features[:, model.out_position:],\n",
    "                                  new_data], dim=1)\n",
    "        \n",
    "        model.set_data_nodes()\n",
    "        model.add_data(embedding(aux_data))\n",
    "        result = model.contract(marginalize_output=True,\n",
    "                                renormalize=True).tensor\n",
    "        \n",
    "        if i < model.n_features - 1:\n",
    "            probs = torch.diagonal(result)\n",
    "        else:\n",
    "            probs = probs.pow(2)\n",
    "        probs = probs / probs.norm(p=1)\n",
    "        \n",
    "        # Sample\n",
    "        cum_probs = probs.cumsum(dim=0)\n",
    "        p = torch.rand(1).item()\n",
    "        for j in range(len(probs)):\n",
    "            if p <= cum_probs[j]:\n",
    "                sampled_features.append(j)\n",
    "                break\n",
    "        \n",
    "    return torch.tensor(sampled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = sample(born_machine.mps, embedding, 0)\n",
    "\n",
    "plt.imshow(image.view(sq_root_size, sq_root_size), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorizing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Novikov, Alexander, et al. \"Tensorizing neural networks.\" Advances in neural information processing systems 28 (2015).\n",
    "- Ma, Xindian, et al. \"A tensorized transformer for language modeling.\" Advances in neural information processing systems 32 (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network\n",
    "model = FFFC(input_size=input_size, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(f'models/fffc_{dataset_name}.pt'))\n",
    "\n",
    "def n_params(model):\n",
    "    n = 0\n",
    "    for p in model.parameters():\n",
    "        n += p.numel()\n",
    "    return n\n",
    "\n",
    "n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tensorized linear layer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TN_Linear(tk.models.MPO):\n",
    "    \n",
    "    def __init__(self, model, cum_percentage):\n",
    "        \n",
    "        # Get matrix from model and reshape it\n",
    "        matrix = model.fc1.weight.detach()\n",
    "        # matrix = matrix.reshape(1, 5, 5, 2,\n",
    "        #                         4, 7, 7, 4).permute(4, 0, 5, 1, 6, 2, 7, 3)\n",
    "        # matrix = matrix.reshape(1, 1, 5, 5, 2, 1,\n",
    "        #                         2, 2, 7, 7, 2, 2).permute(6, 0, 7, 1, 8, 2,\n",
    "        #                                                   9, 3, 10, 4, 11, 5)\n",
    "        matrix = matrix.reshape(2, 5, 5, 1, 5, 5).permute(3, 0, 4, 1, 5, 2)\n",
    "        \n",
    "        mpo_tensors = tk.decompositions.mat_to_mpo(matrix,\n",
    "                                                   cum_percentage=cum_percentage,\n",
    "                                                   renormalize=True)\n",
    "        super().__init__(tensors=mpo_tensors)\n",
    "\n",
    "        self.mps_data = tk.models.MPSData(n_features=3,\n",
    "                                          phys_dim=[1, 5, 5],\n",
    "                                          bond_dim=10,\n",
    "                                          boundary='obc')\n",
    "    \n",
    "    def set_data_nodes(self):\n",
    "        pass\n",
    "    \n",
    "    def add_data(self, data):\n",
    "        mps_tensors = tk.decompositions.vec_to_mps(data.reshape(-1, 1, 5, 5),\n",
    "                                                   n_batches=1,\n",
    "                                                   cum_percentage=0.95,\n",
    "                                                   renormalize=True)\n",
    "        self.mps_data.add_data(mps_tensors)\n",
    "    \n",
    "    def contract(self):\n",
    "        return super().contract(inline_input=True,\n",
    "                                inline_mats=True,\n",
    "                                mps=self.mps_data)\n",
    "        \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        result = super().forward(x, *args, **kwargs)\n",
    "        result = result.reshape(-1, 50)\n",
    "        result += model.fc1.bias\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TN_NN(nn.Module):\n",
    "    def __init__(self, model, cum_percentage):\n",
    "        super().__init__() # super(NN, self).__init__()\n",
    "        self.tn1 = TN_Linear(model, cum_percentage)\n",
    "        self.fc2 = model.fc2\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.tn1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_model = TN_NN(model, 0.3)\n",
    "\n",
    "test_acc = check_accuracy(test_loader, tn_model)\n",
    "print(test_acc, n_params(tn_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model to accelerate training\n",
    "tn_model.tn1.trace(torch.zeros(1, input_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 5\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(tn_model.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = tn_model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, tn_model)\n",
    "    test_acc = check_accuracy(test_loader, tn_model)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "tn_model.tn1.reset()\n",
    "torch.save(tn_model.state_dict(), f'models/tn_fffc_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_model.tn1.canonicalize(cum_percentage=0.5)\n",
    "\n",
    "test_acc = check_accuracy(test_loader, tn_model)\n",
    "print(test_acc, n_params(tn_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining several MPS models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Glasser, Ivan, Nicola Pancotti, and J. Ignacio Cirac. \"From probabilistic graphical models to generalized tensor networks for supervised learning.\" IEEE Access 8 (2020): 68169-68182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST / FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "dataset_name = 'mnist'\n",
    "batch_size = 64\n",
    "image_size = 28\n",
    "input_size = image_size ** 2  # 784\n",
    "num_classes = 10\n",
    "\n",
    "# Load data\n",
    "train_dataset = datasets.MNIST(root='data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "test_dataset = datasets.MNIST(root='data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeSBS(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 bond_dim,\n",
    "                 image_size,\n",
    "                 num_classes,\n",
    "                 init_method,\n",
    "                 inline_input,\n",
    "                 inline_mats,\n",
    "                 renormalize,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # image = batch_size x in_channels x 28 x 28\n",
    "        self.mps_layers = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            mps = tk.models.ConvMPSLayer(in_channels=in_channels,\n",
    "                                         bond_dim=bond_dim,\n",
    "                                         out_channels=num_classes,\n",
    "                                         kernel_size=image_size,\n",
    "                                         init_method=init_method,\n",
    "                                         *args,\n",
    "                                         **kwargs)\n",
    "            self.mps_layers.append(mps)\n",
    "        \n",
    "        self.inline_input = inline_input\n",
    "        self.inline_mats = inline_mats\n",
    "        self.renormalize = renormalize\n",
    "        \n",
    "    def forward(self, x):\n",
    "        flips_x = [x, x.transpose(2, 3), x.flip(2), x.transpose(2, 3).flip(2)]\n",
    "        lst_ys = []\n",
    "        for i in range(4):\n",
    "            y = self.mps_layers[i](flips_x[i],\n",
    "                                   mode='snake',\n",
    "                                   inline_input=self.inline_input,\n",
    "                                   inline_mats=self.inline_mats,\n",
    "                                   renormalize=self.renormalize)\n",
    "            lst_ys.append(y)\n",
    "        \n",
    "        y = torch.stack(lst_ys, dim=0)\n",
    "        y = y.prod(dim=0).view(-1, 10)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embedding_dim = 2\n",
    "output_dim = num_classes\n",
    "bond_dim = 5\n",
    "init_method = 'randn_eye' # rand, randn, randn_eye, canonical, unit\n",
    "\n",
    "# Contraction options\n",
    "inline_input = False\n",
    "inline_mats = False\n",
    "renormalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakes = SnakeSBS(in_channels=embedding_dim,\n",
    "                  bond_dim=bond_dim,\n",
    "                  image_size=image_size,\n",
    "                  num_classes=num_classes,\n",
    "                  init_method=init_method,\n",
    "                  inline_input=inline_input,\n",
    "                  inline_mats=inline_mats,\n",
    "                  renormalize=renormalize,\n",
    "                  std=1e-9)\n",
    "snakes = snakes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.poly(x, degree=embedding_dim - 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.unit(x, dim=embedding_dim)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.discretize(x, base=embedding_dim, level=1).squeeze(-1).int()\n",
    "    x = tk.embeddings.basis(x, dim=embedding_dim).float() # batch x n_features x dim\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace MPSs in model\n",
    "for mps in snakes.mps_layers:\n",
    "    mps.trace(torch.zeros(1, embedding_dim, image_size, image_size).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 1\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(snakes.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            x = embedding(x).permute(0, 2, 1)\n",
    "            x = x.reshape(-1, embedding_dim, image_size, image_size)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        data = embedding(data).permute(0, 2, 1)\n",
    "        data = data.reshape(-1, embedding_dim, image_size, image_size)\n",
    "        \n",
    "        # Forward\n",
    "        scores = snakes(data)\n",
    "        # print(scores[0])\n",
    "        # break\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, snakes)\n",
    "    test_acc = check_accuracy(test_loader, snakes)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "for mps in snakes.mps_layers:\n",
    "    mps.reset()\n",
    "torch.save(snakes.state_dict(), f'models/snakes_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid TN - NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Glasser, Ivan, Nicola Pancotti, and J. Ignacio Cirac. \"From probabilistic graphical models to generalized tensor networks for supervised learning.\" IEEE Access 8 (2020): 68169-68182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST / FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "dataset_name = 'mnist'\n",
    "batch_size = 64\n",
    "image_size = 28\n",
    "input_size = image_size ** 2  # 784\n",
    "num_classes = 10\n",
    "\n",
    "# Load data\n",
    "train_dataset = datasets.MNIST(root='data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "test_dataset = datasets.MNIST(root='data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_SnakeSBS(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 bond_dim,\n",
    "                 image_size,\n",
    "                 num_classes,\n",
    "                 init_method,\n",
    "                 inline_input,\n",
    "                 inline_mats,\n",
    "                 renormalize,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # image = batch_size x in_channels x 28 x 28\n",
    "        self.cnn = nn.Conv2d(in_channels=in_channels,\n",
    "                             out_channels=6,\n",
    "                             kernel_size=5,\n",
    "                             stride=1,\n",
    "                             padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)  # 6 X 14 x 14\n",
    "        \n",
    "        self.mps_layers = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            mps = tk.models.ConvMPSLayer(in_channels=7,\n",
    "                                         bond_dim=bond_dim,\n",
    "                                         out_channels=num_classes,\n",
    "                                         kernel_size=image_size // 2,\n",
    "                                         init_method=init_method,\n",
    "                                         *args,\n",
    "                                         **kwargs)\n",
    "            self.mps_layers.append(mps)\n",
    "        \n",
    "        self.inline_input = inline_input\n",
    "        self.inline_mats = inline_mats\n",
    "        self.renormalize = renormalize\n",
    "    \n",
    "    @staticmethod\n",
    "    def embedding(x):\n",
    "        ones = torch.ones_like(x[:, :1])\n",
    "        return torch.cat([ones, x], dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.cnn(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        flips_x = [x, x.transpose(2, 3), x.flip(2), x.transpose(2, 3).flip(2)]\n",
    "        lst_ys = []\n",
    "        for i in range(4):\n",
    "            y = self.mps_layers[i](flips_x[i],\n",
    "                                   mode='snake',\n",
    "                                   inline_input=self.inline_input,\n",
    "                                   inline_mats=self.inline_mats,\n",
    "                                   renormalize=self.renormalize)\n",
    "            lst_ys.append(y)\n",
    "        \n",
    "        y = torch.stack(lst_ys, dim=0)\n",
    "        y = y.prod(dim=0).view(-1, 10)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "output_dim = num_classes\n",
    "bond_dim = 5\n",
    "init_method = 'randn_eye' # rand, randn, randn_eye, canonical, unit\n",
    "\n",
    "# Contraction options\n",
    "inline_input = False\n",
    "inline_mats = False\n",
    "renormalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_snakes = CNN_SnakeSBS(in_channels=1,\n",
    "                          bond_dim=bond_dim,\n",
    "                          image_size=image_size,\n",
    "                          num_classes=num_classes,\n",
    "                          init_method=init_method,\n",
    "                          inline_input=inline_input,\n",
    "                          inline_mats=inline_mats,\n",
    "                          renormalize=renormalize,\n",
    "                          std=1e-9)\n",
    "cnn_snakes = cnn_snakes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace MPSs in model\n",
    "for mps in cnn_snakes.mps_layers:\n",
    "    mps.trace(torch.zeros(1, 7, image_size // 2, image_size // 2).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 1\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_snakes.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        scores = cnn_snakes(data)\n",
    "        # print(scores[0])\n",
    "        # break\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, cnn_snakes)\n",
    "    test_acc = check_accuracy(test_loader, cnn_snakes)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "for mps in cnn_snakes.mps_layers:\n",
    "    mps.reset()\n",
    "torch.save(cnn_snakes.state_dict(), f'models/cnn_snakes_{dataset_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Tree-MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MNIST dataset, whose samples have shape `(batch, 1, 28, 28)` and can be\n",
    "reshaped as `(batch, 784)`, build and train a Tree tensor network model, that\n",
    "ends up connected to a MPS.\n",
    "\n",
    "That is, construct 4 layers of a binary tree, to reduce the 784 input features\n",
    "to only 49. Then, connect a MPS to these 49 legs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "dataset_name = 'mnist'\n",
    "batch_size = 64\n",
    "image_size = 28\n",
    "input_size = image_size ** 2  # 784\n",
    "num_classes = 10\n",
    "\n",
    "# Load data\n",
    "train_dataset = datasets.MNIST(root='data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "test_dataset = datasets.MNIST(root='data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeMPS(tk.TensorNetwork):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 inner_dims,\n",
    "                 bond_dim,\n",
    "                 out_dim,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # image = batch_size x 784 x embedding_dim\n",
    "        # 784 = 2**4 * 49 -> 4 layers of binary tree + mps with 50 (49 + 1) sites\n",
    "        \n",
    "        # Tree layers\n",
    "        self.tree_layers = []\n",
    "        for i in range(4):\n",
    "            layer = []\n",
    "            for j in range(784 // 2**(i + 1)):\n",
    "                node = tk.ParamNode(shape=(inner_dims[i],\n",
    "                                           inner_dims[i],\n",
    "                                           inner_dims[i + 1]),\n",
    "                                    axes_names=('left',\n",
    "                                                'right',\n",
    "                                                'output'),\n",
    "                                    name=f'node_({j})_layer_({i})',\n",
    "                                    init_method='randn',\n",
    "                                    network=self)\n",
    "                \n",
    "                if i > 0:\n",
    "                    self.tree_layers[-1][2*j]['output'] ^ node['left']\n",
    "                    self.tree_layers[-1][2*j + 1]['output'] ^ node['right']\n",
    "                \n",
    "                layer.append(node)\n",
    "            self.tree_layers.append(layer)\n",
    "        \n",
    "        # MPS\n",
    "        self.mps = tk.models.MPSLayer(n_features=50,\n",
    "                                      in_dim=inner_dims[-1],\n",
    "                                      out_dim=out_dim,\n",
    "                                      bond_dim=bond_dim,\n",
    "                                      boundary='obc',\n",
    "                                      out_position=50 // 2,\n",
    "                                      init_method='randn',\n",
    "                                      *args,\n",
    "                                      **kwargs)\n",
    "        self.mps.mats_env[0].move_to_network(self)\n",
    "        \n",
    "        # Connect last Tree layer to MPS\n",
    "        for mps_node, tree_node in zip(self.mps.in_env, self.tree_layers[-1]):\n",
    "            mps_node['input'] ^ tree_node['output']\n",
    "        \n",
    "    def set_data_nodes(self):\n",
    "        input_edges = []\n",
    "        for node in self.tree_layers[0]:\n",
    "            input_edges += [node['left'], node['right']]\n",
    "        \n",
    "        super().set_data_nodes(input_edges=input_edges,\n",
    "                               num_batch_edges=1)\n",
    "    \n",
    "    def contract(self):\n",
    "        # Contract tree layers\n",
    "        layers = [list(self.data_nodes.values())] + self.tree_layers[:]\n",
    "        for i in range(1, 5):\n",
    "            for j in range(784 // 2**i):\n",
    "                node = layers[i][j]\n",
    "                node = node @ layers[i - 1][2*j]\n",
    "                node = node @ layers[i - 1][2*j + 1]\n",
    "                node = node.renormalize(axis='output')\n",
    "                layers[i][j] = node\n",
    "        \n",
    "        # Contract last tree layer with MPS\n",
    "        stack_mps = tk.stack(self.mps.in_env)\n",
    "        stack_tree = tk.stack(layers[-1])\n",
    "        \n",
    "        stack_mps ^ stack_tree\n",
    "        \n",
    "        stack_result = stack_mps @ stack_tree\n",
    "        mats_in_env = stack_result.unbind()\n",
    "        \n",
    "        # Left environment\n",
    "        left_env = mats_in_env[:self.mps.out_position]\n",
    "        left_result = self.mps.left_node\n",
    "        for node in left_env:\n",
    "            left_result @= node\n",
    "            left_result = left_result.renormalize(axis='right')\n",
    "        \n",
    "        # Right environment\n",
    "        right_env = mats_in_env[self.mps.out_position:]\n",
    "        right_env.reverse()\n",
    "        right_result = self.mps.right_node\n",
    "        for node in right_env:\n",
    "            right_result = node @ right_result\n",
    "            right_result = right_result.renormalize(axis='left')\n",
    "        \n",
    "        result = left_result @ self.mps.out_node @ right_result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "inner_dims = [3, 3, 3, 3, 3]\n",
    "bond_dim = 20\n",
    "out_dim = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mps = TreeMPS(inner_dims=inner_dims,\n",
    "                   bond_dim=bond_dim,\n",
    "                    out_dim=out_dim)\n",
    "tree_mps = tree_mps.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.poly(x, degree=inner_dims[0] - 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.unit(x, dim=inner_dims[0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(x):\n",
    "    x = tk.embeddings.discretize(x, base=inner_dims[0], level=1).squeeze(-1).int()\n",
    "    x = tk.embeddings.basis(x, dim=inner_dims[0]).float() # batch x n_features x dim\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mps.trace(torch.zeros(1, 784, inner_dims[0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-8\n",
    "num_epochs = 20\n",
    "print_epochs = len(train_loader) // 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(tree_mps.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            x = embedding(x)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        data = embedding(data)\n",
    "        \n",
    "        # Forward\n",
    "        scores = tree_mps(data)\n",
    "        # print(scores[0])\n",
    "        # break\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_epochs == 0:\n",
    "            print(f'\\t+ Batch {batch_idx + 1:<3} => Train. Loss: {loss.item()}')\n",
    "    \n",
    "    train_acc = check_accuracy(train_loader, tree_mps)\n",
    "    test_acc = check_accuracy(test_loader, tree_mps)\n",
    "    \n",
    "    print(f'* Epoch {epoch + 1:<3} => Train. Acc.: {train_acc}, Test Acc.: {test_acc}')\n",
    "\n",
    "tree_mps.reset()\n",
    "torch.save(tree_mps.state_dict(), f'models/tree_mps_{dataset_name}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_tk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
